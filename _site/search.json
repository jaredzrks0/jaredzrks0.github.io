[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this section, we explore a number of ‘unsupervized learning’ techniques in order to both again build upon our understanding of the data at hand, but also to try and uncover new interesting or hidden relationships within the BBWAA voting data. For some background, unsupervised learning is a branch of machine learning where the algorithms we employ are not given the underlying truth about each point (in our case the voting result - elected, expired, etc.). Because the machine does not have the ground truths, it works to find general underlying trends or patterns in the data1. It is these general patterns and trends that are quite helpful to us as baseball fans, in learning about how BBWWAA voting functions.\nWithin the realm of unsupervised learning, there are two major tasks that we will utilize in this section. They are clustering and dimensionality reduction. Clustering is the process of grouping data points into distinct groupings based on the underlying features of each point2. By converting the data from a singular dataset into subsets of ‘similar’ groupings, again, based on underlying similarities of each datapoint, we can learn about the relationships between different datapoints, different groupings within our data, and most importantly what drives these similarities and differences. We remember that during this process our computer is not given the truthful group labels for each datapoint, so it relys on the features of the each datapoint for its grouping process.2.\nThe second process, dimensionality reduction, is used to reduce the number of dimensions our data exists in. While this sounds complicated, if we remember that each column, or feature represents a dimension of the data, this process boils down to reducing the number of columns for the data3. Ideally, we do this in ways that preserve the ‘essense’ of the original data, so that we don’t lose significant useful information that can be used later during the process. Dimensionality reduction offers a few important benefits. First, it makes data visualization of datasets with greater than 3 dimensions possible, by creating the ability to convert the data back into a 2-d or 3-d space. Beyond this however, it can also help make prediction models more accurate on unseen data by reducing correlation between features (multicolinearity) among other benefits3.\n\n\n\nIn the following Unsupervised Learning section, we will utilize a number of both clustering and dimensionality reduction techniques. We outline the chosen methods below, alongside a brief explanation of how they work:\n\n\n\n\nKMeans is one of the more popular ‘centroid based’ clustering techniques, meaning the model defines a number of ‘center points’ or centroids to act as the centers of each grouping, and assigns each datapoint to the group belonging to the centroid which it is closest to. As a result, this classifies similar points to the same centroid/group, as they tend to already exist close to one another2. The KMeans clustering algorithm works in the following way.\n\nA user defined number (k) of centroids are chosen, and the machine randomly places them in the feature space\nEach datapoint in the dataset is classified to the closest centroid, resulting in k groups of datapoints\nThe ‘center’ of each group is calculated, and the centroids are moved to these centers\n\nTo calculate the centers, the mean of each group is taken, hence the name KMeans\n\nSteps 1-3 are repeated until the centers no longer change\n\nThe biggest decision when using KMeans is the number of centroids to choose, as this drastically changes the final result of the algorithm. If we know the truthful number of groupings, we can use this number, otherwise it is common practice to run the algorithm with a multitude of different cluster numbers, and choose the final result that results in the most ‘distinct’ clusters. Distinct-ness can be calculated with a few metrics such as the inertia, but in essence we are looking for clusters that are tightly packed to one another, while remaining far away from other clusters4.\n\n\n\nDBSCAN (Density Based Spatial Clustering of Applications with Noise), unlike a centroid based method, is a Density based method meaning it created clusters of the data based on the density of the data at specific points. This is calculated by looking at the number of points which are ‘close’ to any given other point, with closeness being a distance again defined by the user5.\nThere are generally two important benefits of DBSCAN over K-Means. First, the user does not have to determine the number of clusters. DBSCAN is automatically create its ‘optimal’ number of clusters based on the relative densities of the dataset. The second benefit is that DBSCAN is much less sensetive to outliers. In K-Means, an outlier can drag the cluster out toward it during the means calculations, but DBSCAN is able to classify outliers as ‘noise’, and not assign them to specific clusters5.\n\n\n\n\n\n\nPrincipal Component Analysis, or PCA, is one of the most popular dimensionality reduction techniques, reducing the dimensions of a dataset by projecting the data onto the ‘principal components’ of the data. These principal components are created by using the eigenvectors of the eigenvalues of the data’s covariance matrix3. The user can chose the number of principal components to project onto, which directly determines the number of dimensions the resulting data will exist in after the projection.\nThe added benefit of PCA is that by projecting onto the principal components, we are minimizing information loss during the reduction. Thus, our resulting data is as informationally similar to our original data as possible, but also much more free from multicolinearity and noise3!\n\n\n\nt-distributed Stochastic Neighbor Embedding, or t-SNE, is another method of dimensionality reduction that differs from PCA in its effectiveness with non-linear data. Rather than preserving the maximum variance of the dataset, its primary goal is to map data onto a lower dimension in such a way that points that are close to one another in the original dimensions stay close to one another in the resulting lower dimensions6. Because of this, it is often a great visualization technique for reduced data. The DataCamp page on how t-SNE works offers a great explanation of the major steps within t-SNE, which I have included below:\n\nt-SNE models a point being selected as a neighbor of another point in both higher and lower dimensions. It starts by calculating a pairwise similarity between all data points in the high-dimensional space using a Gaussian kernel. The points far apart have a lower probability of being picked than the points close together6.\nThe algorithm then tries to map higher-dimensional data points onto lower-dimensional space while preserving the pairwise similarities6.\n\n\n\n\n\n\nNow that we are familiar with the underpinnings of supervised learning, including the major techniques and applications of Clustering and Dimensionality Reduction, we can now move forward by applying these techniques to our BBWWAA voting dataset to visualize and explore the underlying relationships of the data!\n\n\n\n\n# General Imports\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nimport plotly.io as pio\n\n# Use pio to ensure plotly plots render in Quarto\npio.renderers.default = 'notebook_connected'"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-is-unsupervised-learning",
    "href": "technical-details/unsupervised-learning/main.html#what-is-unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this section, we explore a number of ‘unsupervized learning’ techniques in order to both again build upon our understanding of the data at hand, but also to try and uncover new interesting or hidden relationships within the BBWAA voting data. For some background, unsupervised learning is a branch of machine learning where the algorithms we employ are not given the underlying truth about each point (in our case the voting result - elected, expired, etc.). Because the machine does not have the ground truths, it works to find general underlying trends or patterns in the data1. It is these general patterns and trends that are quite helpful to us as baseball fans, in learning about how BBWWAA voting functions.\nWithin the realm of unsupervised learning, there are two major tasks that we will utilize in this section. They are clustering and dimensionality reduction. Clustering is the process of grouping data points into distinct groupings based on the underlying features of each point2. By converting the data from a singular dataset into subsets of ‘similar’ groupings, again, based on underlying similarities of each datapoint, we can learn about the relationships between different datapoints, different groupings within our data, and most importantly what drives these similarities and differences. We remember that during this process our computer is not given the truthful group labels for each datapoint, so it relys on the features of the each datapoint for its grouping process.2.\nThe second process, dimensionality reduction, is used to reduce the number of dimensions our data exists in. While this sounds complicated, if we remember that each column, or feature represents a dimension of the data, this process boils down to reducing the number of columns for the data3. Ideally, we do this in ways that preserve the ‘essense’ of the original data, so that we don’t lose significant useful information that can be used later during the process. Dimensionality reduction offers a few important benefits. First, it makes data visualization of datasets with greater than 3 dimensions possible, by creating the ability to convert the data back into a 2-d or 3-d space. Beyond this however, it can also help make prediction models more accurate on unseen data by reducing correlation between features (multicolinearity) among other benefits3."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#unsupervised-methods-used",
    "href": "technical-details/unsupervised-learning/main.html#unsupervised-methods-used",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In the following Unsupervised Learning section, we will utilize a number of both clustering and dimensionality reduction techniques. We outline the chosen methods below, alongside a brief explanation of how they work:\n\n\n\n\nKMeans is one of the more popular ‘centroid based’ clustering techniques, meaning the model defines a number of ‘center points’ or centroids to act as the centers of each grouping, and assigns each datapoint to the group belonging to the centroid which it is closest to. As a result, this classifies similar points to the same centroid/group, as they tend to already exist close to one another2. The KMeans clustering algorithm works in the following way.\n\nA user defined number (k) of centroids are chosen, and the machine randomly places them in the feature space\nEach datapoint in the dataset is classified to the closest centroid, resulting in k groups of datapoints\nThe ‘center’ of each group is calculated, and the centroids are moved to these centers\n\nTo calculate the centers, the mean of each group is taken, hence the name KMeans\n\nSteps 1-3 are repeated until the centers no longer change\n\nThe biggest decision when using KMeans is the number of centroids to choose, as this drastically changes the final result of the algorithm. If we know the truthful number of groupings, we can use this number, otherwise it is common practice to run the algorithm with a multitude of different cluster numbers, and choose the final result that results in the most ‘distinct’ clusters. Distinct-ness can be calculated with a few metrics such as the inertia, but in essence we are looking for clusters that are tightly packed to one another, while remaining far away from other clusters4.\n\n\n\nDBSCAN (Density Based Spatial Clustering of Applications with Noise), unlike a centroid based method, is a Density based method meaning it created clusters of the data based on the density of the data at specific points. This is calculated by looking at the number of points which are ‘close’ to any given other point, with closeness being a distance again defined by the user5.\nThere are generally two important benefits of DBSCAN over K-Means. First, the user does not have to determine the number of clusters. DBSCAN is automatically create its ‘optimal’ number of clusters based on the relative densities of the dataset. The second benefit is that DBSCAN is much less sensetive to outliers. In K-Means, an outlier can drag the cluster out toward it during the means calculations, but DBSCAN is able to classify outliers as ‘noise’, and not assign them to specific clusters5.\n\n\n\n\n\n\nPrincipal Component Analysis, or PCA, is one of the most popular dimensionality reduction techniques, reducing the dimensions of a dataset by projecting the data onto the ‘principal components’ of the data. These principal components are created by using the eigenvectors of the eigenvalues of the data’s covariance matrix3. The user can chose the number of principal components to project onto, which directly determines the number of dimensions the resulting data will exist in after the projection.\nThe added benefit of PCA is that by projecting onto the principal components, we are minimizing information loss during the reduction. Thus, our resulting data is as informationally similar to our original data as possible, but also much more free from multicolinearity and noise3!\n\n\n\nt-distributed Stochastic Neighbor Embedding, or t-SNE, is another method of dimensionality reduction that differs from PCA in its effectiveness with non-linear data. Rather than preserving the maximum variance of the dataset, its primary goal is to map data onto a lower dimension in such a way that points that are close to one another in the original dimensions stay close to one another in the resulting lower dimensions6. Because of this, it is often a great visualization technique for reduced data. The DataCamp page on how t-SNE works offers a great explanation of the major steps within t-SNE, which I have included below:\n\nt-SNE models a point being selected as a neighbor of another point in both higher and lower dimensions. It starts by calculating a pairwise similarity between all data points in the high-dimensional space using a Gaussian kernel. The points far apart have a lower probability of being picked than the points close together6.\nThe algorithm then tries to map higher-dimensional data points onto lower-dimensional space while preserving the pairwise similarities6."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#summary",
    "href": "technical-details/unsupervised-learning/main.html#summary",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Now that we are familiar with the underpinnings of supervised learning, including the major techniques and applications of Clustering and Dimensionality Reduction, we can now move forward by applying these techniques to our BBWWAA voting dataset to visualize and explore the underlying relationships of the data!"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#code",
    "href": "technical-details/unsupervised-learning/main.html#code",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "# General Imports\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nimport plotly.io as pio\n\n# Use pio to ensure plotly plots render in Quarto\npio.renderers.default = 'notebook_connected'"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#principal-component-analysis",
    "href": "technical-details/unsupervised-learning/main.html#principal-component-analysis",
    "title": "Unsupervised Learning",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nAll Voting Data Visualizations\n\n2-D Visualization\nBefore we dive into the clustering methods outlined above, we begin by leveraging PCA for dimensionality reduction to explore our voting data in lower dimensions. We do this because we are fortunate enough to have access to the ground truths for each voting outcome, and can plot with outcome labels to see if clusters are naturally present in the data.\n\n# Load in the data\nbatter_df = pd.read_csv('../../data/processed-data/batter_df_for_prediction.csv')\n\n# Define the targets, which we will use for coloring plots\nstr_targets = batter_df.outcome\nnum_targets = pd.Categorical(batter_df['outcome']).codes\n\n# Filter to only numeric columns, as these are the only plottable ones\nnumeric_df = batter_df.select_dtypes(include=np.number).drop(columns='votes_pct')\n\nWhen reducing dimensions, it is often a helpful step to standardize and/or scale data beforehand. We do this with scikit-learn’s StandardScaler, which scales all features to a more centered distribution with a standard deviation = 1.\nThis is done by subtracting the mean of the data from each point and dividing by the original standard deviation\n\n# Scale Data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(numeric_df)\n\nAfter scaling the data, we use PCA to reduce the dataset to 2 dimensions.\n\n# Convert to only 2 Dimensions via PCA\npca = PCA(n_components=2)\npca_df = pca.fit_transform(scaled_data)\n\nWith this two dimensional dataset, we now plot each data point, with each axis representing an individual principal component.\n\nsimple = batter_df[['b_war', 'year_on_ballot']]\npca_df = pd.DataFrame(pca_df, columns=['PC1', 'PC2'])\n\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue=str_targets, palette=\"husl\")\nplt.title(\"2D PCA Plot of Ballot Outcomes\", fontsize=18)\nplt.xlabel('PCA Compnent 1', fontsize=11)\nplt.ylabel('PCA Component 2', fontsize=11)\n\nText(0, 0.5, 'PCA Component 2')\n\n\n\n\n\n\n\n\n\nImmediately we see some interesting results! While there is still some overlap between the different outcomes, we still do see a clear pattern where as either PCA Components increase, we see that the outcomes send from elimination, to expiration/limbo to elected. This is a fascinating result because this order can generally be thought of as being increasingly good at baseball. Thus, even within the 2-d PCA data, there are clear visual trends separating HOF players from non-HOF players! Additionally, the fact that we see the different groups start to separate tells us that we have a decent shot at being able to predict whether a player will be elected or not based on their underlying stats.\n\n\n3D Visualization\nAs a next step, we undertake the exact same procedure as before, but this time reduce our data to 3 dimensions, to see if the added dimension is able to help separate different clusters.\n\n# Scale Data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(numeric_df)\n\n# Convert to only 2 Dimensions\npca = PCA(n_components=3)\npca_df = pca.fit_transform(scaled_data)\n\n\n# Create a 3D scatter plot\npca_df = pd.DataFrame(pca_df, columns=['PC1', 'PC2', 'PC3'])\n\n# Scatter plot\n# Plotly 3D Scatter plot\nfig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color=str_targets, hover_name = batter_df.player_id, \n                    title='Scatterplot of BBWAA Voting Data along 3 Principal Components', \n                    labels={'X': 'Principal Component 1', 'Y': 'Principal Component 2', 'Z': 'Principal Component 3'})\n\nfig.show()\n\n                                                \n\n\nAbove, we see that the third dimension does indeed help separate our outcome differences! There still remains some significant overlap between the limbo/expired and eliminated outcomes, but the election outcomes have clearly started to separate themselves, giving us even more hope that predicting outcomes is a reasonable task, especially if we limit to predicting a binary election/non-election outcome.\nThe other interesting result is we see that there are lots of mini clusters of points each belonging to the same player. You can see this for yourself by hovering the curser over individual points. This occurs becuase the points for all these players are the same other than an added year, plus an update to the last years voting percentage. As a next step, we look to only plot one point per player, plotting them as elected if they were, or else non elected for any other set of outcomes. While this is not feasable as a prediction method given in real time we never know if it is a player’s last year on the ballot, but it is a great exercise in curiosity to see if elected vs non-elected players can be separated visually on the career level.\n\n\n\nCareer Voting Visualization\n\n# Subset to only one point per player, ensuring we keep the election outcome if it exists.\nbatter_df = batter_df.sort_values(by='outcome').drop_duplicates(subset=['name'], keep='first')\n\n# Create a new targets column that is binary for if the player was elected\nbinary_targets = batter_df.outcome == \"elected\"\n\n# Define the targets, which we will use for coloring plots\nstr_targets = batter_df.outcome\nnum_targets = pd.Categorical(batter_df['outcome']).codes\n\n# Filter to only numeric columns, as these are the only plottable ones\nnumeric_df = batter_df.select_dtypes(include=np.number).drop(columns='votes_pct')\n\n# Print the number of players in the filtered df\nprint(f\"There are {len(numeric_df)} individual players who have been placed on a ballot\")\n\n# Scale Data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(numeric_df)\n\n# Convert to only 2 Dimensions\npca = PCA(n_components=3)\npca_df = pca.fit_transform(scaled_data)\n\n# Create a 3D scatter plot\npca_df = pd.DataFrame(pca_df, columns=['PC1', 'PC2', 'PC3'])\n\n# Scatter plot\n# Plotly 3D Scatter plot\nfig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color=binary_targets, hover_name=batter_df.name,\n                    title='Scatterplot of Succesful BBWAA Ballot Elections', \n                    labels={'X': 'Principal Component 1', 'Y': 'Principal Component 2', 'Z': 'Principal Component 3'})\n\nfig.show()\n\nThere are 781 individual players who have been placed on a ballot\n\n\n                                                \n\n\nConverting the targets to binary values and limiting the data to only one outcome products two pretty stark groupings of points. What is interesting about the data however is that they are not necessarily two ‘distinct’ clusters. There is only one cluster of data from a density standpoint, but it contains a somewhat clean barrier between the two classes. As a result, we are unsure if more tradidional clustering methods will be able to succesfully cluster the data in accordance with the ground truths, but we explore the possibilities below."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#k-means",
    "href": "technical-details/unsupervised-learning/main.html#k-means",
    "title": "Unsupervised Learning",
    "section": "K-Means",
    "text": "K-Means\n\nOptimal Known Clusters\nMoving into our clustering analysis, we begin by fitting the K-Means algorithm on our dataset with one point per player. Given we know that there are 4 ground truth outcomes, we start by fitting K-Means with 4 clusters.\n\nkmeans = KMeans(n_clusters=4, random_state=5000)\nkmeans.fit(scaled_data)\n\n\nlabels = kmeans.labels_\n\ncluster_names = {0: 'Cluster 0', 1: 'Cluster 1', 2: 'Cluster 2', 3: 'Cluster 3'}\nlabels = pd.Series(labels).map(cluster_names)\n\nfig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color=labels, \n                    title='K-Means Results with 4 Clusters', \n                    labels={'X': 'Principal Component 1', 'Y': 'Principal Component 2', 'Z': 'Principal Component 3'})\n\nfig.show()\n\n                                                \n\n\nUnsuprisingly, the K-Means algorithm does not fit our data very well. We can see that it pretty equally divides our data into 4 quadrants rather than matching the separations we saw earlier with the ground truths. This is becuase while we saw above the data had some boundries between the outcomes, there were more just boundries than distinct clusters. Without the distinct clustering in the data, K-Means is unable to produce the clusters we are looking for.\n\n\nCalculating ‘Optimal’ Clusters\nOne way to choose an optimal number of clusters for K-Means is to plot a metric of choice, in this case intertia, for a varying number of clusters. Then the ‘optimal’ number of clusters becomes the point where the decrease intertia sharply flattens out. This is known as the elbow method. Below, we utilize the elbow method to determine the ‘optimal’ number of clusters as according to K-Means.\n\nscores = []\nfor n in range(2,10):\n    kmeans = KMeans(n_clusters=n, random_state=5000)\n    kmeans.fit(scaled_data)\n    labels = kmeans.labels_\n\n    score = kmeans.inertia_\n    scores.append(score)\n\nscore_df = pd.DataFrame({'n_clusters':range(2,10), 'scores':scores})\n\n\nsns.lineplot(data=score_df, x='n_clusters', y='scores', marker='o')\nplt.ylabel('Inertia', fontsize=14)\nplt.xlabel('Number of Clusters', fontsize=14)\nplt.title('KMeans Silhouette Score Elbow Plot', fontsize=18)\n\nText(0.5, 1.0, 'KMeans Silhouette Score Elbow Plot')\n\n\n\n\n\n\n\n\n\nIn the elbow chart above, there is not a clear number of clusters to choose as the optimal number. There is some drop off in the decrease in inertia at 5 or 6 clusters, but it is nothing drastic. As such, especially knowing that the true number of outcome classes is 4, we conclude that K-Means does not produce a good fit for out data."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan-1",
    "href": "technical-details/unsupervised-learning/main.html#dbscan-1",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nAs a final clustering method to explore, we fit the DBSCAN algorithm to the same dataset as in K-Means, again in 3 dimensions. The results can be seen below:\n\ndbscan = DBSCAN()\ndbscan.fit(scaled_data)\n\nlabels = dbscan.labels_\n\nfig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color=labels, \n                    title='Scatterplot of BBWAA Voting Data, Clustered with DBSCAN', \n                    labels={'X': 'Principal Component 1', 'Y': 'Principal Component 2', 'Z': 'Principal Component 3'})\n\nfig.show()\n\n                                                \n\n\nWe see above that the DBSCAN method produces even worse results than the K-Means. This makes intuitive sense however, as we see that our data has a fairly uniform density, meaning DBSCAN with converge onto one large cluster (given the lack of varying densities). As we did with K-Means, we once again conclude that DBSCAN does not prodive a good fit to our dataset."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Now that we have all of our final data, we can begin to explore and analyze the trends within! As a quick reminder, we have two datasets: one for pitchers and one for batters. We explore each below:"
  },
  {
    "objectID": "technical-details/eda/main.html#general-overview",
    "href": "technical-details/eda/main.html#general-overview",
    "title": "Exploratory Data Analysis",
    "section": "General Overview",
    "text": "General Overview\n\nBatter DataFrame\n\nprint(f'The Batter DataFrame contains {batter_df.shape[0]} entries and {batter_df.shape[1]} features.')\n\nThe Batter DataFrame contains 3108 entries and 38 features.\n\n\n\nbatter_df.head()\n\n\n\n\n\n\n\n\nname\nplayer_id\nvoting_year\nyear_on_ballot\nvotes_pct\nly_votes_pct\noutcome\nposition\nscandal\nb_war\n...\nG_p_app\nG_c\nG_1b\nG_2b\nG_3b\nG_ss\nG_lf_app\nG_cf_app\nG_rf_app\nG_dh\n\n\n\n\n0\nTy Cobb\ncobbty01\n1936\n1\n98.2\n12.975208\nelected\nbatter\n0\n151.4\n...\n0.001013\n0.0\n0.004728\n0.001013\n0.000338\n0.000000\n0.012158\n0.741304\n0.239446\n0.0\n\n\n1\nBabe Ruth\nruthba01\n1936\n1\n95.1\n12.975208\nelected\nbatter\n0\n162.2\n...\n0.066449\n0.0\n0.013453\n0.000000\n0.000000\n0.000000\n0.428047\n0.030575\n0.461476\n0.0\n\n\n2\nHonus Wagner\nwagneho01\n1936\n1\n95.1\n12.975208\nelected\nbatter\n0\n131.0\n...\n0.000718\n0.0\n0.090452\n0.020818\n0.075377\n0.677674\n0.012922\n0.024408\n0.097631\n0.0\n\n\n3\nNap Lajoie\nlajoina01\n1936\n1\n64.6\n12.975208\nlimbo\nbatter\n0\n106.9\n...\n0.000000\n0.0\n0.117647\n0.831699\n0.008987\n0.030229\n0.002042\n0.002042\n0.007353\n0.0\n\n\n4\nTris Speaker\nspeaktr01\n1936\n1\n58.8\n12.975208\nlimbo\nbatter\n0\n135.0\n...\n0.000368\n0.0\n0.006620\n0.000000\n0.000000\n0.000000\n0.000736\n0.989702\n0.002574\n0.0\n\n\n\n\n5 rows × 38 columns\n\n\n\n\nbatter_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3108 entries, 0 to 3107\nData columns (total 38 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   name                         3108 non-null   object \n 1   player_id                    3108 non-null   object \n 2   voting_year                  3108 non-null   int64  \n 3   year_on_ballot               3108 non-null   int64  \n 4   votes_pct                    3108 non-null   float64\n 5   ly_votes_pct                 3108 non-null   float64\n 6   outcome                      3108 non-null   object \n 7   position                     3108 non-null   object \n 8   scandal                      3108 non-null   int64  \n 9   b_war                        3108 non-null   float64\n 10  b_pa                         3108 non-null   int64  \n 11  b_h                          3108 non-null   int64  \n 12  b_hr                         3108 non-null   int64  \n 13  b_sb                         3108 non-null   float64\n 14  b_bb                         3108 non-null   int64  \n 15  b_so                         3108 non-null   float64\n 16  b_batting_avg                3108 non-null   float64\n 17  b_onbase_plus_slugging       3108 non-null   float64\n 18  b_onbase_plus_slugging_plus  3108 non-null   int64  \n 19  b_home_run_perc              3108 non-null   float64\n 20  b_strikeout_perc             3108 non-null   float64\n 21  b_base_on_balls_perc         3108 non-null   float64\n 22  b_cwpa_bat                   3108 non-null   float64\n 23  b_baseout_runs               3108 non-null   float64\n 24  mvps                         3108 non-null   int64  \n 25  gold_gloves                  3108 non-null   int64  \n 26  batting_titles               3108 non-null   int64  \n 27  all_stars                    3108 non-null   int64  \n 28  G_p_app                      3108 non-null   float64\n 29  G_c                          3108 non-null   float64\n 30  G_1b                         3108 non-null   float64\n 31  G_2b                         3108 non-null   float64\n 32  G_3b                         3108 non-null   float64\n 33  G_ss                         3108 non-null   float64\n 34  G_lf_app                     3108 non-null   float64\n 35  G_cf_app                     3108 non-null   float64\n 36  G_rf_app                     3108 non-null   float64\n 37  G_dh                         3108 non-null   float64\ndtypes: float64(22), int64(12), object(4)\nmemory usage: 922.8+ KB\n\n\n\n\nPitcher DataFrame\n\nprint(f'The Pitcher DataFrame contains {pitcher_df.shape[0]} entries and {pitcher_df.shape[1]} features.')\n\nThe Pitcher DataFrame contains 514 entries and 46 features.\n\n\n\npitcher_df.head()\n\n\n\n\n\n\n\n\nname\nplayer_id\nvoting_year\nyear_on_ballot\nvotes_pct\nly_votes_pct\noutcome\nposition\nscandal\np_war\n...\nG_p_app\nG_c\nG_1b\nG_2b\nG_3b\nG_ss\nG_lf_app\nG_cf_app\nG_rf_app\nG_dh\n\n\n\n\n0\nRube Waddell\nwadderu01\n1936\n1\n14.6\n12.975208\nlimbo\npitcher\n0\n60.9\n...\n0.997549\n0.0\n0.002451\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n1\nRube Waddell\nwadderu01\n1937\n2\n33.3\n14.600000\nlimbo\npitcher\n0\n60.9\n...\n0.997549\n0.0\n0.002451\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n2\nAddie Joss\njossad01\n1937\n1\n5.5\n12.975208\nlimbo\npitcher\n0\n47.7\n...\n0.969492\n0.0\n0.020339\n0.000000\n0.003390\n0.000000\n0.000000\n0.006780\n0.000000\n0.0\n\n\n3\nClark Griffith\ngriffcl01\n1937\n1\n2.0\n12.975208\neliminated\npitcher\n0\n59.9\n...\n0.945720\n0.0\n0.002088\n0.002088\n0.002088\n0.008351\n0.012526\n0.010438\n0.016701\n0.0\n\n\n4\nTheodore Breitenstein\nbreitte01\n1937\n1\n0.5\n12.975208\neliminated\npitcher\n0\n51.8\n...\n0.853933\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.044944\n0.035955\n0.065169\n0.0\n\n\n\n\n5 rows × 46 columns\n\n\n\n\npitcher_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 514 entries, 0 to 513\nData columns (total 46 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   name                            514 non-null    object \n 1   player_id                       514 non-null    object \n 2   voting_year                     514 non-null    int64  \n 3   year_on_ballot                  514 non-null    int64  \n 4   votes_pct                       514 non-null    float64\n 5   ly_votes_pct                    514 non-null    float64\n 6   outcome                         514 non-null    object \n 7   position                        514 non-null    object \n 8   scandal                         514 non-null    int64  \n 9   p_war                           514 non-null    float64\n 10  p_w                             514 non-null    int64  \n 11  p_win_loss_perc                 514 non-null    float64\n 12  p_earned_run_avg                514 non-null    float64\n 13  p_earned_run_avg_plus           514 non-null    int64  \n 14  p_g                             514 non-null    int64  \n 15  p_gs                            514 non-null    int64  \n 16  p_sho                           514 non-null    int64  \n 17  p_sv                            514 non-null    int64  \n 18  p_ip                            514 non-null    float64\n 19  p_so                            514 non-null    int64  \n 20  p_whip                          514 non-null    float64\n 21  p_fip                           514 non-null    float64\n 22  p_strikeouts_per_base_on_balls  514 non-null    float64\n 23  p_batting_avg                   514 non-null    float64\n 24  p_onbase_plus_slugging          514 non-null    float64\n 25  p_home_run_perc                 514 non-null    float64\n 26  p_strikeout_perc                514 non-null    float64\n 27  p_cwpa_def                      514 non-null    float64\n 28  p_baseout_runs                  514 non-null    float64\n 29  b_war                           514 non-null    float64\n 30  b_batting_avg                   514 non-null    float64\n 31  b_onbase_plus_slugging_plus     514 non-null    float64\n 32  cy_youngs                       514 non-null    int64  \n 33  gold_gloves                     514 non-null    int64  \n 34  mvps                            514 non-null    int64  \n 35  all_stars                       514 non-null    int64  \n 36  G_p_app                         514 non-null    float64\n 37  G_c                             514 non-null    float64\n 38  G_1b                            514 non-null    float64\n 39  G_2b                            514 non-null    float64\n 40  G_3b                            514 non-null    float64\n 41  G_ss                            514 non-null    float64\n 42  G_lf_app                        514 non-null    float64\n 43  G_cf_app                        514 non-null    float64\n 44  G_rf_app                        514 non-null    float64\n 45  G_dh                            514 non-null    float64\ndtypes: float64(28), int64(14), object(4)\nmemory usage: 184.8+ KB"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Like any project, cleaning our raw data collected in the prior step is critical to ensuring we have a dataset that can be explored and modeled to extract insights. In this section, we read in the raw scraped data, and process in the following steps:\n\nReading in all the scraped data\nLooking carefully through each dataset and:\n\nEnsuring data is read in properly\nChecking for and potentially removing missing values\nDropping unnecessary columns that do not relate to the research questions\nCreating new features for for analysis\nMerging datasets together into a singular final dataset each for batters and pitchers"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#reading-in-raw-data",
    "href": "technical-details/data-cleaning/main.html#reading-in-raw-data",
    "title": "Data Cleaning",
    "section": "Reading in Raw Data",
    "text": "Reading in Raw Data\n\n# Read in our datasets individually\nall_inductees_df = pd.read_csv('../../data/raw-data/all_hof_inductees_table.csv')\n\nyearly_hof_voting_df = pd.read_csv('../../data/raw-data/yearly_hof_voting_data.csv')\n\nall_player_info = pd.read_csv('../../data/raw-data/all_player_info.csv')\n\nwith open('../../data/raw-data/all_player_stats.pkl', 'rb') as fpath:\n    all_player_stats_dict = pkl.load(fpath)\n\nBecause we scraped our player stats into a dictionary with keys for each player, we next split this dictionary into DataFrames for each key type - Career Stats, Annual Stats, and Appearences by Position. While doing this, we also attatch the ID from the dictionary key into the underlying DataFrame so that we don’t lose the connection\n\n# Define the list of all player IDs scraped\nids = list(all_player_stats_dict.keys())\n\nfor id in ids:\n    if isinstance(all_player_stats_dict[id], dict):\n        all_player_stats_dict[id].get('career_stats')['player_id'] = id\n        all_player_stats_dict[id].get('annual_stats')['player_id'] = id\n        all_player_stats_dict[id].get('appearances')['player_id'] = id\n\n# Create a list of each DataFrame stored within each ID, accounting for the occurence of Non-dictionaries when scraping fails\nall_player_career_stats = [all_player_stats_dict[player_id].get('career_stats') if isinstance(all_player_stats_dict[player_id], dict) else None for player_id in ids]\nall_player_annual_stats = [all_player_stats_dict[player_id].get('annual_stats') if isinstance(all_player_stats_dict[player_id], dict) else None for player_id in ids]\nall_player_career_appearences = [all_player_stats_dict[player_id].get('appearances')if isinstance(all_player_stats_dict[player_id], dict) else None for player_id in ids]"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-data",
    "href": "technical-details/data-cleaning/main.html#cleaning-data",
    "title": "Data Cleaning",
    "section": "Cleaning Data",
    "text": "Cleaning Data\n\nAll Inducted Players DataFrame\nWorking through our DataFrames, we start with the DataFrame holding all succesfully inducted players. Start by taking a glance at the first few rows of the data.\n\nall_inductees_df.head()\n\n\n\n\n\n\n\n\nvoting_year\nplayer\nliving_status\nvoting_body\ninducted_as\nvotes\nvote_percentage\n\n\n\n\n0\n2024\nAdrian BeltrÃ©\n1979-Living\nBBWAA\nPlayer\n366.0\n95.1%\n\n\n1\n2024\nTodd Helton\n1973-Living\nBBWAA\nPlayer\n307.0\n79.7%\n\n\n2\n2024\nJim Leyland\n1944-Living\nContemporary Baseball Era Non-Players\nManager\nNaN\nNaN\n\n\n3\n2024\nJoe Mauer\n1983-Living\nBBWAA\nPlayer\n293.0\n76.1%\n\n\n4\n2023\nFred McGriff\n1963-Living\nContemporary Baseball Era\nPlayer\n16.0\n100.0%\n\n\n\n\n\n\n\nWe observe a few necessary fixes for the table, including:\n\nPlayer names not properly encoded\nVoting bodies other than BBWAA\nNon-Players included via non-BBWAA methods\nNaN values for vote tallies for non-BBWAA methods\n\nAdditionally, there are some cosmetic alterations we can make, including:\n\nRemoving the living status, which we will not use for further analysis\n\n\n### Clean the all_inductees_df DataFrame\n\n# Define a function for encoding the player names\ndef decode_player_name(name):\n    encoded_name = name.encode('latin-1')\n    corrected_name = encoded_name.decode('utf-8')\n    return corrected_name.strip()\n\n# Encode the player names\nall_inductees_df.player = all_inductees_df.player.apply(decode_player_name)\n\n# Filter to only BBWAA votes\nall_inductees_df = all_inductees_df[all_inductees_df.voting_body == 'BBWAA']\n\n# Drop the living status column\nall_inductees_df = all_inductees_df.drop(columns=['living_status'])\n\nLooking at the table again, all previous concerns are now resolved:\n\nall_inductees_df.head()\n\n\n\n\n\n\n\n\nvoting_year\nplayer\nvoting_body\ninducted_as\nvotes\nvote_percentage\n\n\n\n\n0\n2024\nAdrian Beltré\nBBWAA\nPlayer\n366.0\n95.1%\n\n\n1\n2024\nTodd Helton\nBBWAA\nPlayer\n307.0\n79.7%\n\n\n3\n2024\nJoe Mauer\nBBWAA\nPlayer\n293.0\n76.1%\n\n\n5\n2023\nScott Rolen\nBBWAA\nPlayer\n297.0\n76.3%\n\n\n12\n2022\nDavid Ortiz\nBBWAA\nPlayer\n307.0\n77.9%\n\n\n\n\n\n\n\nNext, we confirm the full table is clean, with no missing values, and all the columns we desire.\n\nall_inductees_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 134 entries, 0 to 345\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   voting_year      134 non-null    int64  \n 1   player           134 non-null    object \n 2   voting_body      134 non-null    object \n 3   inducted_as      134 non-null    object \n 4   votes            134 non-null    float64\n 5   vote_percentage  134 non-null    object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 7.3+ KB\n\n\nWe also observe that the player names are now properly encoded - resulting in a cleaned dataframe. Thus, we save the data back to the processed data folder.\n\nall_inductees_df.player.iloc[1:25]\n\n1           Todd Helton\n3             Joe Mauer\n5           Scott Rolen\n12          David Ortiz\n13          Derek Jeter\n16         Larry Walker\n18         Roy Halladay\n19       Edgar Martínez\n20         Mike Mussina\n21       Mariano Rivera\n23    Vladimir Guerrero\n24       Trevor Hoffman\n25        Chipper Jones\n27            Jim Thome\n29         Jeff Bagwell\n30           Tim Raines\n31       Iván Rodríguez\n34      Ken Griffey Jr.\n35          Mike Piazza\n36         Craig Biggio\n37        Randy Johnson\n38       Pedro Martinez\n39          John Smoltz\n41          Tom Glavine\nName: player, dtype: object\n\n\n\nall_inductees_df.to_csv('../../data/processed-data/all_hof_inductees.csv', index=False)\n\n\n\nYearly BBWAA HOF Ballot Voting DataFrame\nThe next DataFrame is that containing the yearly voting results of the BBWAA ballot. We again print out the first few rows to observe the table structure.\n\nyearly_hof_voting_df.head()\n\n\n\n\n\n\n\n\nrank\nname\nplayer_page_url\nyear_on_ballot\nvotes\nvotes_pct\nhof_monitor\nhof_standard\nexperience\nWAR_career\n...\nG_p\nGS\nSV\nIP\nH_p\nHR_p\nBB_p\nSO_p\npos_summary\nvoting_year\n\n\n\n\n0\n1\nTy Cobb\nhttps://www.baseball-reference.com/players/c/c...\n1st\n222\n98.2%\n445.0\n75.0\n24.0\n151.4\n...\n3.0\n0.0\n1.0\n5.0\n6.0\n0.0\n2.0\n0.0\n*8*9H7/3145\n1936\n\n\n1\n2\nBabe Ruth\nhttps://www.baseball-reference.com/players/r/r...\n1st\n215\n95.1%\n411.0\n79.0\n22.0\n162.2\n...\n163.0\n147.0\n4.0\n1221.1\n974.0\n10.0\n441.0\n488.0\n*9*71H83\n1936\n\n\n2\n3\nHonus Wagner\nhttps://www.baseball-reference.com/players/w/w...\n1st\n215\n95.1%\n313.0\n75.0\n21.0\n131.0\n...\n2.0\n0.0\n0.0\n8.1\n7.0\n0.0\n6.0\n6.0\n*6*O3594H7/81\n1936\n\n\n3\n4\nChristy Mathewson\nhttps://www.baseball-reference.com/players/m/m...\n1st\n205\n90.7%\n303.0\n84.0\n17.0\n106.7\n...\n636.0\n552.0\n30.0\n4788.2\n4219.0\n89.0\n848.0\n2507.0\n*1/H397\n1936\n\n\n4\n5\nWalter Johnson\nhttps://www.baseball-reference.com/players/j/j...\n1st\n189\n83.6%\n364.0\n82.0\n21.0\n166.9\n...\n802.0\n666.0\n34.0\n5914.1\n4913.0\n97.0\n1363.0\n3509.0\n*1H/897\n1936\n\n\n\n\n5 rows × 41 columns\n\n\n\nAgain, there are some immediate fixes we must complete. These include:\n\nCorrectly encoding player names\nConverting the year on ballot and vote percentage columns to numeric values\n\nAs well as some cosmetic edits:\n\nDropping all the stats columns, plus rank, url, and raw vote tallies\n\nWe already have all the stats we will use in the stats DataFrames\n\n\nFor this DataFrame however, there is also some Feature Engineering/Creation we complete:\n\nBuilding a column for player ID, taken from the scraped URL\nBuilding a column for the players vote percentage in the year prior\n\nAgain, this leverages a single-step markov assumption, that current vote % is impacted by (and only by) last years vote total.\n\nCreate an ‘outcome’ column, based on whether the player was elected, expired, or staying in limbo\n\n\n# Decode the player names\nyearly_hof_voting_df.name = yearly_hof_voting_df.name.apply(decode_player_name)\n\n# Convert the year on ballot and vote percent colulmns to numberic values\nyearly_hof_voting_df.year_on_ballot = yearly_hof_voting_df.year_on_ballot.apply(lambda x: int(x[0]))\nyearly_hof_voting_df.votes_pct = yearly_hof_voting_df.votes_pct.apply(lambda x: float(x[:-1]))\n\n# Build the ID column\nyearly_hof_voting_df['player_id'] = yearly_hof_voting_df['player_page_url'].apply(lambda x: x.split('/')[-1].split('.')[0])\n\n# Build the prior year vote percent column\n# For players who are on the ballot for the first year, we insert None, and will later impute a value\nyearly_hof_voting_df['ly_votes_pct'] = yearly_hof_voting_df.apply(lambda x: yearly_hof_voting_df[(yearly_hof_voting_df.player_id == x.player_id) & (yearly_hof_voting_df.year_on_ballot == x.year_on_ballot - 1)].votes_pct.iloc[0] if x.year_on_ballot &gt; 1 else None, axis=1)\n\n# Create outcome column\ndef return_ballot_outcome(year_on_ballot, current_vote):\n    '''Returns the outcome for a player in the BBWAA vote, depending on their vote share and numbher\n      of previous years on the ballot'''\n    \n    if current_vote &gt;= 75:\n        return 'elected'\n    elif current_vote &lt; 5:\n        return 'eliminated'\n    elif current_vote &lt; 75 and year_on_ballot == 9:\n        return 'expired'\n    else:\n        return 'limbo'\n\nyearly_hof_voting_df['outcome'] = yearly_hof_voting_df.apply(lambda x: return_ballot_outcome(x.year_on_ballot, x.votes_pct), axis=1)\n\n# Drop unnecessary columns\nyearly_hof_voting_df = yearly_hof_voting_df[['name', 'player_id', 'voting_year', 'year_on_ballot', 'votes_pct', 'ly_votes_pct', 'outcome']]\n\nBecause we’ve filled in the prior year’s vote percentage as None for players in their first year on the ballot, we now must impute, or fill in, those missing values. To do this, we will impute with the average vote percentage of first year players. We don’t impute with 0, as the values are numeric and relative, so 0 may undervalue these players in a model. We also don’t chose the overall mean value, as we already have the additional information that these are first year players, so we may introduce falsehoods by including ballot ‘veterans’ in the mean.\n\n# Calculate the mean vote percentage garnered by first year players\nmean_value = yearly_hof_voting_df[['year_on_ballot', 'votes_pct']].groupby(by='year_on_ballot').mean().loc[1].votes_pct\n\n# Impute the mean value\nyearly_hof_voting_df.ly_votes_pct = yearly_hof_voting_df.ly_votes_pct.fillna(mean_value)\n\nFinally, we once again check that the first few rows of the dataset to ensure our changes have taken effect, and print info about the dataset as a whole to ensure we have completed the cleaning process, before we write the dataset out to the processed data folder.\n\nyearly_hof_voting_df.head()\n\n\n\n\n\n\n\n\nname\nplayer_id\nvoting_year\nyear_on_ballot\nvotes_pct\nly_votes_pct\noutcome\n\n\n\n\n0\nTy Cobb\ncobbty01\n1936\n1\n98.2\n12.975208\nelected\n\n\n1\nBabe Ruth\nruthba01\n1936\n1\n95.1\n12.975208\nelected\n\n\n2\nHonus Wagner\nwagneho01\n1936\n1\n95.1\n12.975208\nelected\n\n\n3\nChristy Mathewson\nmathech01\n1936\n1\n90.7\n12.975208\nelected\n\n\n4\nWalter Johnson\njohnswa01\n1936\n1\n83.6\n12.975208\nelected\n\n\n\n\n\n\n\n\nyearly_hof_voting_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4900 entries, 0 to 4899\nData columns (total 7 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   name            4900 non-null   object \n 1   player_id       4900 non-null   object \n 2   voting_year     4900 non-null   int64  \n 3   year_on_ballot  4900 non-null   int64  \n 4   votes_pct       4900 non-null   float64\n 5   ly_votes_pct    4900 non-null   float64\n 6   outcome         4900 non-null   object \ndtypes: float64(2), int64(2), object(3)\nmemory usage: 268.1+ KB\n\n\n\n# Write DataFrame to csv in processed data folder\nyearly_hof_voting_df.to_csv('../../data/processed-data/all_bbwaa_voting.csv', index=False)\n\n\n\nPlayer Names and IDs DataFrame\nFor the next DataFrame, we inspect the DataFrame containing all the General Player Information\n\nall_player_info.head()\n\n\n\n\n\n\n\n\nname\nid\nurl_suffix\n\n\n\n\n0\nDavid Aardsma\naardsda01\n/players/a/aardsda01.shtml\n\n\n1\nHenry Aaron\naaronha01\n/players/a/aaronha01.shtml\n\n\n2\nTommie Aaron\naaronto01\n/players/a/aaronto01.shtml\n\n\n3\nDon Aase\naasedo01\n/players/a/aasedo01.shtml\n\n\n4\nAndy Abad\nabadan01\n/players/a/abadan01.shtml\n\n\n\n\n\n\n\n\nall_player_info.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23370 entries, 0 to 23369\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   name        23370 non-null  object\n 1   id          23370 non-null  object\n 2   url_suffix  23370 non-null  object\ndtypes: object(3)\nmemory usage: 547.9+ KB\n\n\nWe see that the DataFrame is already clean and tidy, but we can drop the url suffix, as we will not need it for further research. We then save the DataFrame to the processed data folder\n\n# Drop the URL column\nall_player_info = all_player_info.drop(columns=['url_suffix'])\n\n# Save the DataFrame\nall_player_info.to_csv('../../data/processed-data/all_player_info.csv', index=False)\n\n\n\nCareer Stats for All Players\nMoving onto the career stats for every player throughout history, we begin by dividing data between batters and pitchers. We accomplish this by filtering DataFrames based on whether the player meets thresholds for innings pitched and if not then plate appearences. During this process we also drop duplicated column names so that we can combine DataFrames in a future step.\n\nIf players do not meet either threshold, we simply ignore them as there is zero possibility they will be eligible for the HOF under BBWAA rules\n\n\nbatter_career_stats_l = []\npitcher_career_stats_l = []\n\nfor series in all_player_career_stats:\n    if isinstance(series, pd.DataFrame):\n        # Check if the player pitched more than 100 innings\n        if 'p_ip' in series.columns and float(series.p_ip.T.iloc[0,0]) &gt; 150 and series['player_id'].iloc[0] != 'ruthba01': # We also make Babe Ruth a Batter\n            duplicated_columns = series.columns.duplicated()\n            pitcher_career_stats_l.append(series.loc[:, ~duplicated_columns])\n        \n        # Check if the batter had at least 500 plate appearences\n        elif 'b_pa' in series.columns and float(series.b_pa.T.iloc[0,0]) &gt; 850:\n            duplicated_columns = series.columns.duplicated()\n            batter_career_stats_l.append(series.loc[:, ~duplicated_columns])\n\n# Finally, we concatonate all the individual DataFrames into one\npitcher_career_stats = pd.concat([df for df in pitcher_career_stats_l], ignore_index=True)\nbatter_career_stats = pd.concat([df for df in batter_career_stats_l], ignore_index=True)\n\nNow we inspect the first few rows of both the batting and pitching DataFrames\n\nbatter_career_stats.head()\n\n\n\n\n\n\n\n\nb_war\nb_games\nb_pa\nb_ab\nb_r\nb_h\nb_doubles\nb_triples\nb_hr\nb_rbi\n...\np_avg_exit_velo\np_hard_hit_perc\np_ld_perc\np_gb_perc\np_fb_perc\np_gb_fb_ratio\np_wpa_def\np_cwpa_def\np_baseout_runs\np_win_loss_perc\n\n\n\n\n0\n143.1\n3298\n13941\n12364\n2174\n3771\n624\n98\n755\n2297\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-2.8\n437\n1046\n944\n102\n216\n42\n6\n13\n94\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n8.7\n855\n3479\n3044\n355\n772\n99\n43\n11\n324\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1.8\n452\n1965\n1756\n307\n493\n67\n46\n19\n280\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.5\n702\n2227\n2044\n273\n523\n109\n23\n62\n242\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 99 columns\n\n\n\n\npitcher_career_stats.head()\n\n\n\n\n\n\n\n\nb_war\nb_games\nb_pa\nb_ab\nb_r\nb_h\nb_doubles\nb_triples\nb_hr\nb_rbi\n...\np_cwpa_def\np_baseout_runs\naccomplishments\nplayer_id\nb_gb_fb_ratio\nb_run_scoring_perc\nb_extra_bases_taken_perc\nb_stolen_base_perc\nb_avg_exit_velo\nb_hard_hit_perc\n\n\n\n\n0\n-0.1\n139\n5\n4\n0\n0\n0\n0\n0\n0\n...\n-0.1%\n11.21\n\naardsda01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-0.1\n81\n5\n5\n0\n0\n0\n0\n0\n0\n...\n8.1%\n36.38\n1x All-Star\naasedo01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-0.1\n159\n9\n9\n0\n1\n0\n0\n0\n0\n...\n-1.3%\n10.25\n\nabadfe01\n3.00\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-1.0\n79\n252\n225\n21\n38\n3\n3\n0\n17\n...\nNaN\nNaN\n\nabbeybe01\nNaN\n35.6\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.7%\n18.95\n\nabbotan01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 99 columns\n\n\n\nBoth DataFrames have a large number of features, so it is necessary to print out the full columnn list, where we will determine which are necessary\n\npitcher_career_stats.columns\n\nIndex(['b_war', 'b_games', 'b_pa', 'b_ab', 'b_r', 'b_h', 'b_doubles',\n       'b_triples', 'b_hr', 'b_rbi', 'b_sb', 'b_cs', 'b_bb', 'b_so',\n       'b_batting_avg', 'b_onbase_perc', 'b_slugging_perc',\n       'b_onbase_plus_slugging', 'b_onbase_plus_slugging_plus', 'b_roba',\n       'b_rbat_plus', 'b_tb', 'b_gidp', 'b_hbp', 'b_sh', 'b_sf', 'b_ibb',\n       'pos', 'awards', 'b_batting_avg_bip', 'b_iso_slugging',\n       'b_home_run_perc', 'b_strikeout_perc', 'b_base_on_balls_perc',\n       'b_ld_perc', 'b_gb_perc', 'b_fb_perc', 'b_pull_perc', 'b_center_perc',\n       'b_oppo_perc', 'b_wpa_bat', 'b_cwpa_bat', 'b_baseout_runs', 'p_war',\n       'p_w', 'p_l', 'p_win_loss_perc', 'p_earned_run_avg', 'p_g', 'p_gs',\n       'p_gf', 'p_cg', 'p_sho', 'p_sv', 'p_ip', 'p_h', 'p_r', 'p_er', 'p_hr',\n       'p_bb', 'p_ibb', 'p_so', 'p_hbp', 'p_bk', 'p_wp', 'p_bfp',\n       'p_earned_run_avg_plus', 'p_fip', 'p_whip', 'p_hits_per_nine',\n       'p_hr_per_nine', 'p_bb_per_nine', 'p_so_per_nine',\n       'p_strikeouts_per_base_on_balls', 'p_batting_avg', 'p_onbase_perc',\n       'p_slugging_perc', 'p_onbase_plus_slugging', 'p_batting_avg_bip',\n       'p_home_run_perc', 'p_strikeout_perc', 'p_base_on_balls_perc',\n       'p_avg_exit_velo', 'p_hard_hit_perc', 'p_ld_perc', 'p_gb_perc',\n       'p_fb_perc', 'p_gb_fb_ratio', 'p_wpa_def', 'p_cwpa_def',\n       'p_baseout_runs', 'accomplishments', 'player_id', 'b_gb_fb_ratio',\n       'b_run_scoring_perc', 'b_extra_bases_taken_perc', 'b_stolen_base_perc',\n       'b_avg_exit_velo', 'b_hard_hit_perc'],\n      dtype='object')\n\n\n\npitcher_career_stats.columns\n\nIndex(['b_war', 'b_games', 'b_pa', 'b_ab', 'b_r', 'b_h', 'b_doubles',\n       'b_triples', 'b_hr', 'b_rbi', 'b_sb', 'b_cs', 'b_bb', 'b_so',\n       'b_batting_avg', 'b_onbase_perc', 'b_slugging_perc',\n       'b_onbase_plus_slugging', 'b_onbase_plus_slugging_plus', 'b_roba',\n       'b_rbat_plus', 'b_tb', 'b_gidp', 'b_hbp', 'b_sh', 'b_sf', 'b_ibb',\n       'pos', 'awards', 'b_batting_avg_bip', 'b_iso_slugging',\n       'b_home_run_perc', 'b_strikeout_perc', 'b_base_on_balls_perc',\n       'b_ld_perc', 'b_gb_perc', 'b_fb_perc', 'b_pull_perc', 'b_center_perc',\n       'b_oppo_perc', 'b_wpa_bat', 'b_cwpa_bat', 'b_baseout_runs', 'p_war',\n       'p_w', 'p_l', 'p_win_loss_perc', 'p_earned_run_avg', 'p_g', 'p_gs',\n       'p_gf', 'p_cg', 'p_sho', 'p_sv', 'p_ip', 'p_h', 'p_r', 'p_er', 'p_hr',\n       'p_bb', 'p_ibb', 'p_so', 'p_hbp', 'p_bk', 'p_wp', 'p_bfp',\n       'p_earned_run_avg_plus', 'p_fip', 'p_whip', 'p_hits_per_nine',\n       'p_hr_per_nine', 'p_bb_per_nine', 'p_so_per_nine',\n       'p_strikeouts_per_base_on_balls', 'p_batting_avg', 'p_onbase_perc',\n       'p_slugging_perc', 'p_onbase_plus_slugging', 'p_batting_avg_bip',\n       'p_home_run_perc', 'p_strikeout_perc', 'p_base_on_balls_perc',\n       'p_avg_exit_velo', 'p_hard_hit_perc', 'p_ld_perc', 'p_gb_perc',\n       'p_fb_perc', 'p_gb_fb_ratio', 'p_wpa_def', 'p_cwpa_def',\n       'p_baseout_runs', 'accomplishments', 'player_id', 'b_gb_fb_ratio',\n       'b_run_scoring_perc', 'b_extra_bases_taken_perc', 'b_stolen_base_perc',\n       'b_avg_exit_velo', 'b_hard_hit_perc'],\n      dtype='object')\n\n\nWe will not need this many features in our final datasets. Thus, for pitchers, we will use a concise set of pitching metrics, alongside a few simple batting metrics. For batters, we will filter down to a more consice set of batting metrics, while leaving out any pitching metrics. This is because there is effectively no sample of batters who consistiently pitch (outside of a few notable exceptions)\n\n\nBatting Metrics\n\n\n\nMetric\nDescription\n\n\n\n\nb_war\nWins above Replacement (full definition here)\n\n\nb_pa\nPlate Appearances\n\n\nb_h\nHits\n\n\nb_hr\nHome Runs\n\n\nb_sb\nStolen Bases\n\n\nb_bb\nWalks\n\n\nb_so\nStrikeouts\n\n\nb_batting_avg\nBatting Average\n\n\nb_onbase_plus_slugging\nOn Base % plus Slugging % (full definition here)\n\n\nb_onbase_plus_slugging_plus\nOn Base % plus Slugging % indexed to league average value of 100 (full definition here)\n\n\nb_home_run_perc\nHome Run Percent\n\n\nb_strikeout_perc\nStrikeout Percent\n\n\nb_base_on_balls_perc\nWalk Percent\n\n\nb_cwpa_bat\nChampionship win probability added (full definition here)\n\n\nb_baseout_runs\nRun Expectancy Change (full definition here)\n\n\naccomplishments\nAwards won throughout career\n\n\n\n\n\nPitching Metrics\n\n\n\nMetric\nDescription\n\n\n\n\np_war\nWins above Replacement (full definition here)\n\n\np_w\nWins\n\n\np_win_loss_perc\nWin percentage\n\n\np_earned_run_avg\nEarned Run Average (Earned Runs / Innings Pitched)\n\n\np_earned_run_avg_plus\nEarned Run Average indexed to league average value of 100 (full definition here)\n\n\np_g\nGames appeared in\n\n\np_gs\nGames started\n\n\np_sho\nShutouts\n\n\np_sv\nSaves\n\n\np_ip\nInnings Pitched\n\n\np_so\nStrikeouts\n\n\np_whip\nWalks and Hits per Innings Pitched\n\n\np_fip\nFielding Independent Pitching (full definition here)\n\n\np_strikeouts_per_base_on_balls\nRatio of strikeouts to walks\n\n\np_batting_avg\nBatting Average of batters faced\n\n\np_onbase_plus_slugging\nOn Base % + Slugging % of batters faced\n\n\np_home_run_perc\nHome Run %\n\n\np_strikeout_perc\nStrikeout %\n\n\np_cwpa_def\nChampionship win probability added (full definition here)\n\n\np_baseout_runs\nRun Expectancy Change (full definition here)\n\n\nb_war\nWins above Replacement (full definition here)\n\n\nb_batting_avg\nBatting Average (as a batter)\n\n\nb_onbase_plus_slugging_plus\nOn Base % plus Slugging % (as a batter)\n\n\naccomplishments\nAwards won throughout career\n\n\n\n\n# Define the important columns and filter the datasets down\nbatter_columns = [\n    \"b_war\", \"b_pa\", \"b_h\", \"b_hr\", \"b_sb\", \"b_bb\", \"b_so\", \"b_batting_avg\",\n    \"b_onbase_plus_slugging\", \"b_onbase_plus_slugging_plus\", \"b_home_run_perc\",\n    \"b_strikeout_perc\", \"b_base_on_balls_perc\", \"b_cwpa_bat\", \"b_baseout_runs\",\n    \"accomplishments\", 'player_id'\n]\n\npitcher_columns = [\n    \"p_war\", \"p_w\", \"p_win_loss_perc\", \"p_earned_run_avg\", \"p_earned_run_avg_plus\",\n    \"p_g\", \"p_gs\", \"p_sho\", \"p_sv\", \"p_ip\", \"p_so\", \"p_whip\", \"p_fip\",\n    \"p_strikeouts_per_base_on_balls\", \"p_batting_avg\", \"p_onbase_plus_slugging\",\n    \"p_home_run_perc\", \"p_strikeout_perc\", \"p_cwpa_def\", \"p_baseout_runs\",\n    \"accomplishments\", 'b_war','b_batting_avg', 'b_onbase_plus_slugging_plus', 'player_id'\n]\n\nbatter_career_stats = batter_career_stats[batter_columns]\npitcher_career_stats = pitcher_career_stats[pitcher_columns]\n\nLooking through the career stats DataFrames, we still see that there are some missing values present. Specifically, these are for a handfull of metrics, for players who played so long ago that these metrics were not tracked in games. To solve this problem, we impute with the median of each stat. We choose the median becuase the dataset consists of all major league players, so the underlying data is likely skewed with outliers from the few excellent players of each statistic.\n\npitcher_career_stats.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3835 entries, 0 to 3834\nData columns (total 25 columns):\n #   Column                          Non-Null Count  Dtype \n---  ------                          --------------  ----- \n 0   p_war                           3835 non-null   object\n 1   p_w                             3835 non-null   object\n 2   p_win_loss_perc                 3835 non-null   object\n 3   p_earned_run_avg                3835 non-null   object\n 4   p_earned_run_avg_plus           3835 non-null   object\n 5   p_g                             3835 non-null   object\n 6   p_gs                            3835 non-null   object\n 7   p_sho                           3835 non-null   object\n 8   p_sv                            3835 non-null   object\n 9   p_ip                            3835 non-null   object\n 10  p_so                            3835 non-null   object\n 11  p_whip                          3835 non-null   object\n 12  p_fip                           3835 non-null   object\n 13  p_strikeouts_per_base_on_balls  3835 non-null   object\n 14  p_batting_avg                   3271 non-null   object\n 15  p_onbase_plus_slugging          3271 non-null   object\n 16  p_home_run_perc                 3835 non-null   object\n 17  p_strikeout_perc                3835 non-null   object\n 18  p_cwpa_def                      3259 non-null   object\n 19  p_baseout_runs                  3259 non-null   object\n 20  accomplishments                 3835 non-null   object\n 21  b_war                           3750 non-null   object\n 22  b_batting_avg                   3605 non-null   object\n 23  b_onbase_plus_slugging_plus     3605 non-null   object\n 24  player_id                       3835 non-null   object\ndtypes: object(25)\nmemory usage: 749.1+ KB\n\n\n\nbatter_career_stats.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4425 entries, 0 to 4424\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   b_war                        4425 non-null   object\n 1   b_pa                         4425 non-null   object\n 2   b_h                          4425 non-null   object\n 3   b_hr                         4425 non-null   object\n 4   b_sb                         4412 non-null   object\n 5   b_bb                         4425 non-null   object\n 6   b_so                         4267 non-null   object\n 7   b_batting_avg                4425 non-null   object\n 8   b_onbase_plus_slugging       4425 non-null   object\n 9   b_onbase_plus_slugging_plus  4425 non-null   object\n 10  b_home_run_perc              4425 non-null   object\n 11  b_strikeout_perc             4267 non-null   object\n 12  b_base_on_balls_perc         4425 non-null   object\n 13  b_cwpa_bat                   3777 non-null   object\n 14  b_baseout_runs               3777 non-null   object\n 15  accomplishments              4425 non-null   object\n 16  player_id                    4425 non-null   object\ndtypes: object(17)\nmemory usage: 587.8+ KB\n\n\nTo complete the imputing, we must also convert all possible stats to numeric values\n\n# Convert values to numeric\nbatter_career_stats.b_cwpa_bat = batter_career_stats.b_cwpa_bat.apply(lambda x: x.replace('%', '') if type(x) == str else x)\npitcher_career_stats.p_cwpa_def = pitcher_career_stats.p_cwpa_def.apply(lambda x: x.replace('%', '') if type(x) == str else x)\n\nbatter_career_stats = batter_career_stats.apply(lambda x: pd.to_numeric(x, errors='ignore'))\npitcher_career_stats = pitcher_career_stats.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\n# Impute column medians, ignoring the string columns\nimputer = SimpleImputer(strategy='median')\nbatter_career_stats.loc[:, ~batter_career_stats.columns.isin(['accomplishments', 'player_id'])] = imputer.fit_transform(batter_career_stats.loc[:, ~batter_career_stats.columns.isin(['accomplishments', 'player_id'])])\npitcher_career_stats.loc[:, ~pitcher_career_stats.columns.isin(['accomplishments', 'player_id'])] = imputer.fit_transform(pitcher_career_stats.loc[:, ~pitcher_career_stats.columns.isin(['accomplishments', 'player_id'])])\n\n/var/folders/x1/v8sbf2px7bd3mh5s_v4zrxp40000gn/T/ipykernel_53307/1207380148.py:5: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  batter_career_stats = batter_career_stats.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n/var/folders/x1/v8sbf2px7bd3mh5s_v4zrxp40000gn/T/ipykernel_53307/1207380148.py:6: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  pitcher_career_stats = pitcher_career_stats.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\n\n\npitcher_career_stats.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3835 entries, 0 to 3834\nData columns (total 25 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   p_war                           3835 non-null   float64\n 1   p_w                             3835 non-null   int64  \n 2   p_win_loss_perc                 3835 non-null   float64\n 3   p_earned_run_avg                3835 non-null   float64\n 4   p_earned_run_avg_plus           3835 non-null   int64  \n 5   p_g                             3835 non-null   int64  \n 6   p_gs                            3835 non-null   int64  \n 7   p_sho                           3835 non-null   int64  \n 8   p_sv                            3835 non-null   int64  \n 9   p_ip                            3835 non-null   float64\n 10  p_so                            3835 non-null   int64  \n 11  p_whip                          3835 non-null   float64\n 12  p_fip                           3835 non-null   float64\n 13  p_strikeouts_per_base_on_balls  3835 non-null   float64\n 14  p_batting_avg                   3835 non-null   float64\n 15  p_onbase_plus_slugging          3835 non-null   float64\n 16  p_home_run_perc                 3835 non-null   float64\n 17  p_strikeout_perc                3835 non-null   float64\n 18  p_cwpa_def                      3835 non-null   float64\n 19  p_baseout_runs                  3835 non-null   float64\n 20  accomplishments                 3835 non-null   object \n 21  b_war                           3835 non-null   float64\n 22  b_batting_avg                   3835 non-null   float64\n 23  b_onbase_plus_slugging_plus     3835 non-null   float64\n 24  player_id                       3835 non-null   object \ndtypes: float64(16), int64(7), object(2)\nmemory usage: 749.1+ KB\n\n\nNow that the table is free from all missing values, the last step is to account for the awards data, which is currently unstructured text. To do this, we write a function that searches the texts for any of the 5 major awards we want to include, as well as their counts, before adding new columns for the award wins. By looking at awards, we also aim to capture greatness from players who had short stretches of greatness, but potentially didnt play for many years. The major awards are:\n\nMVP - Most Valuable Player (Generally a hitter)\nCy Young - Best Pitcher\nBatting Title - Highest Batting Average\nGold Glove - Best defensive player at a given position\nAll Star - Being named to the ‘All Star’ team in a given year\n\n\ndef scan_awards(text, award):\n    '''Given the text from our award scraping function, and a specific award, return the number of times\n       the award was won'''\n    # Check if the award was won at all\n    win = award.lower() in text.lower()\n\n    # If the award was not won, return 0. Else, return the number of times it was won\n    if not win:\n        return 0\n    else:\n        if ',' in text: # The player won multiple awards, and the formatting is different\n            try:\n                num_wins = int(text.lower().split(award.lower())[0].split(',')[-1].strip().replace('x', ''))\n            except (IndexError, ValueError): # Sometimes when winning the awarad once, the format changes\n                return 1\n        else:\n            try:\n                num_wins = int(text.lower().split('all-star'.lower())[0].strip().replace('x', ''))\n            except (IndexError, ValueError):\n                return 1\n\n        return num_wins\n\n# Insert Pitching Awards  \npitcher_career_stats[\"cy_youngs\"] = pitcher_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"Cy Young\"))\npitcher_career_stats[\"gold_gloves\"] = pitcher_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"Gold Glove\"))\npitcher_career_stats[\"mvps\"] = pitcher_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"MVP\"))\npitcher_career_stats[\"all_stars\"] = pitcher_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"All-Star\"))\n\n# Insert Batting Awards\nbatter_career_stats[\"mvps\"] = batter_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"MVP\"))\nbatter_career_stats[\"gold_gloves\"] = batter_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"Gold Glove\"))\nbatter_career_stats[\"batting_titles\"] = batter_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"Batting Title\"))\nbatter_career_stats[\"all_stars\"] = batter_career_stats.accomplishments.apply(lambda x: scan_awards(x, \"All-Star\"))\n\n# Remove the general accomplishments columns from each DataFrame\nbatter_career_stats = batter_career_stats.drop(columns=['accomplishments'])\npitcher_career_stats = pitcher_career_stats.drop(columns=['accomplishments'])\n\n\n# View the first few rows of the awards data for batters\nbatter_career_stats.head()[['player_id', 'mvps', 'gold_gloves', 'batting_titles', 'all_stars']]\n\n\n\n\n\n\n\n\nplayer_id\nmvps\ngold_gloves\nbatting_titles\nall_stars\n\n\n\n\n0\naaronha01\n1\n3\n2\n25\n\n\n1\naaronto01\n0\n0\n0\n0\n\n\n2\nabbated01\n0\n0\n0\n0\n\n\n3\nabbeych01\n0\n0\n0\n0\n\n\n4\nabbotku01\n0\n0\n0\n0\n\n\n\n\n\n\n\nWith the DataFrame completely clean, and the awards dealt with, we save it back to the processed data folder\n\nbatter_career_stats.to_csv('../../data/processed-data/batter_career_stats.csv', index=False)\npitcher_career_stats.to_csv('../../data/processed-data/pitcher_career_stats.csv', index=False)\n\n\n\nAppearences by Position DataFrame\nFinally, we move onto the appearences by position dataset, which we will use to gain a more granular look at the positions each player played throughout their career. Because this is a list of DataFrames, we start by concatonating everything into a singular one\n\nappearances_df = pd.concat([x for x in all_player_career_appearences], ignore_index=True)\n\nappearances_df.head()\n\n\n\n\n\n\n\n\nlg_ID\nG_all\nGS\nG_batting\nG_defense\nG_p_app\nG_c\nG_1b\nG_2b\nG_3b\nG_ss\nG_lf_app\nG_cf_app\nG_rf_app\nG_of_app\nG_dh\nG_ph\nG_pr\nplayer_id\n\n\n\n\n0\n\n331\n0\n139\n331\n331\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\naardsda01\n\n\n1\n\n3298\n3173\n3298\n2985\n0\n0\n210\n43\n7\n0\n315\n308\n2174\n2760\n201\n122\n1\naaronha01\n\n\n2\n\n437\n206\n437\n346\n0\n0\n232\n7\n10\n0\n135\n1\n2\n137\n0\n102\n35\naaronto01\n\n\n3\n\n448\n91\n81\n448\n448\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\naasedo01\n\n\n4\n\n15\n4\n15\n9\n0\n0\n8\n0\n0\n0\n0\n0\n1\n1\n0\n7\n1\nabadan01\n\n\n\n\n\n\n\nThe dataset looks quite clean already! We see the first 4 columns tally general position types, the final 2 (excluding player_id), tally games as a pinch hitter and pinch runner, while the G_[position] each tally games at a given position. The two steps we take to finish the job are dropping unnecessary columns, and converting game tallies to numeric values and then percentage shares.\nAdditionally, a small subset of players have missing data for their positions. To combat this, we fill the missing values with equal percentages across the player, signaling no relative impact from position.\n\n# Convert values to numeric types\nappearances_df = appearances_df.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\n# Define the important columns to keep and normalize\nposition_cols = ['G_p_app', 'G_c', 'G_1b', 'G_2b', 'G_3b',\n                 'G_ss', 'G_lf_app', 'G_cf_app', 'G_rf_app',\n                 'G_dh']\n\n# Normalize the games by position into percentages\nappearances_df[position_cols] = appearances_df[position_cols].div(appearances_df[position_cols].sum(axis=1), axis=0)\n\n# Filter down to our final DataFrame\nappearances_df = appearances_df[position_cols + ['player_id']]\n\n# Fill missing values\nappearances_df = appearances_df.fillna(1/9)\n\n/var/folders/x1/v8sbf2px7bd3mh5s_v4zrxp40000gn/T/ipykernel_53307/2786824259.py:2: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  appearances_df = appearances_df.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\n\nLooking at the final DataFrame, we see that it is now clean and can be written to the processed data folder.\n\nappearances_df.head()\n\n\n\n\n\n\n\n\nG_p_app\nG_c\nG_1b\nG_2b\nG_3b\nG_ss\nG_lf_app\nG_cf_app\nG_rf_app\nG_dh\nplayer_id\n\n\n\n\n0\n1.0\n0.0\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\naardsda01\n\n\n1\n0.0\n0.0\n0.064457\n0.013198\n0.002149\n0.0\n0.096685\n0.094537\n0.667281\n0.061694\naaronha01\n\n\n2\n0.0\n0.0\n0.599483\n0.018088\n0.025840\n0.0\n0.348837\n0.002584\n0.005168\n0.000000\naaronto01\n\n\n3\n1.0\n0.0\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\naasedo01\n\n\n4\n0.0\n0.0\n0.888889\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.111111\n0.000000\nabadan01\n\n\n\n\n\n\n\n\nappearances_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21228 entries, 0 to 21227\nData columns (total 11 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   G_p_app    21228 non-null  float64\n 1   G_c        21228 non-null  float64\n 2   G_1b       21228 non-null  float64\n 3   G_2b       21228 non-null  float64\n 4   G_3b       21228 non-null  float64\n 5   G_ss       21228 non-null  float64\n 6   G_lf_app   21228 non-null  float64\n 7   G_cf_app   21228 non-null  float64\n 8   G_rf_app   21228 non-null  float64\n 9   G_dh       21228 non-null  float64\n 10  player_id  21228 non-null  object \ndtypes: float64(10), object(1)\nmemory usage: 1.8+ MB\n\n\n\nappearances_df.to_csv('../../data/processed-data/appearances.csv', index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#final-dataset-building",
    "href": "technical-details/data-cleaning/main.html#final-dataset-building",
    "title": "Data Cleaning",
    "section": "Final Dataset Building",
    "text": "Final Dataset Building\nNow that all of the individual DataFrames are cleaned (woohoo!), we move into condensing the data into one singular DataFrame for batters and one for pitchers, that we will use for future exploration and analysis. To accomplish this, we take the BBWAA voting DataFrame as our base, as it already contains only those players who have been voted on in a BBWAA ballot, and then we add extra features like the players stats on from our adjacent DataFrames.\n\n# Read in the core DataFrames\nfinal_dataset = pd.read_csv('../../data/processed-data/all_bbwaa_voting.csv')\n\n# Read in the stats DataFrames\nbatter_stats = pd.read_csv('../../data/processed-data/batter_career_stats.csv')\npitcher_stats = pd.read_csv('../../data/processed-data/pitcher_career_stats.csv')\n\n\ndef sort_row_by_bp(player_id, batter_stats, pitcher_stats):\n    if player_id in pitcher_stats.player_id.values:\n        return 'pitcher'\n    elif player_id in batter_stats.player_id.values:\n        return 'batter'\n    else:\n        return 'missing'\n\n# Attach the batter/pitcher designation for each entry and check the results\nfinal_dataset['position'] = final_dataset.player_id.apply(lambda x: sort_row_by_bp(x, batter_stats, pitcher_stats))\nfinal_dataset.position.value_counts()\n\nposition\nbatter     3144\nmissing    1226\npitcher     530\nName: count, dtype: int64\n\n\nUpon investigation of the players with ‘missing’ values for their position, we can confirm that the vast majority of the players are ‘missing’ because they were among the group to fail to be scraped in the data collection phase due to changes in underlying HTML. We see below the only names of players who did succesfully have data scraped.\n\nfor player in final_dataset[final_dataset.position == 'missing'].player_id:\n    if player == 'mccarjo99': # throws error bc the id is different across datasets\n        continue\n    val = all_player_stats_dict[player]\n    if not isinstance(val, str):\n        print(player)\n\nrickebr01\nrickebr01\nrickebr01\nrickebr01\nensje01\nbeaucji01\n\n\nThere are only 4 players left who did succesfully have data scraped. However, a quick seach reveals that these players were actually primarily manages or executives who played early enough that the ‘BBWAA’ eligibility rules were different such that they were voted on from this pool. Becasue of this, we will drop all players who are ‘missing’ in this dataset.\n\nfinal_dataset = final_dataset[final_dataset.position != 'missing']\nfinal_dataset.head()\n\n\n\n\n\n\n\n\nname\nplayer_id\nvoting_year\nyear_on_ballot\nvotes_pct\nly_votes_pct\noutcome\nposition\n\n\n\n\n0\nTy Cobb\ncobbty01\n1936\n1\n98.2\n12.975208\nelected\nbatter\n\n\n1\nBabe Ruth\nruthba01\n1936\n1\n95.1\n12.975208\nelected\nbatter\n\n\n2\nHonus Wagner\nwagneho01\n1936\n1\n95.1\n12.975208\nelected\nbatter\n\n\n5\nNap Lajoie\nlajoina01\n1936\n1\n64.6\n12.975208\nlimbo\nbatter\n\n\n6\nTris Speaker\nspeaktr01\n1936\n1\n58.8\n12.975208\nlimbo\nbatter\n\n\n\n\n\n\n\nNow that we are sure that all dataset entries have a valid connection to the stats data, we split the data into batters and pitchers, before attaching the career stats\n\npitcher_df = final_dataset[final_dataset.position == 'pitcher']\nbatter_df = final_dataset[final_dataset.position == 'batter']\n\nBefore we merge the stats with the BBWAA voting data, we will also add in the scandal data as set forth in the earlier fangraphs paper. We do this with a manual list of known scandelous players as set forth in that paper. We also filter down to players in our dataset. Reasons for scandals include:\n\nUse of Performance Enhancing Drugs\nGambling\nConduct detrimental to the game and personal image\n\nAfter adding in the presence of a scandal, we remove the players with scandals, as the presense of a scandal essentially interferes with the normal voting process, and critically, is a factor known before the voting process itself. Thus, for future data we would also be able to exclude them from predicions.\n\n# Define the ids with scandals\nscandal_ids = [\n    'bondsba01', 'rodrial01',\n    'pettian01', 'ortizda01',\n    'schilcu01', 'dykstle01',\n    'ramirma02', 'mcgwima01',\n    'rosepe01']\n\n# Create a scandal column in both the pitching and batting DataFrames\nbatter_df['scandal'] = batter_df.player_id.apply(lambda x: 1 if x in scandal_ids else 0)\npitcher_df['scandal'] = pitcher_df.player_id.apply(lambda x: 1 if x in scandal_ids else 0)\n\n# Remove players with scandals\nbatter_df = batter_df[batter_df.scandal == 0]\npitcher_df = pitcher_df[pitcher_df.scandal == 0]\n\n/var/folders/x1/v8sbf2px7bd3mh5s_v4zrxp40000gn/T/ipykernel_53307/2900954371.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  batter_df['scandal'] = batter_df.player_id.apply(lambda x: 1 if x in scandal_ids else 0)\n/var/folders/x1/v8sbf2px7bd3mh5s_v4zrxp40000gn/T/ipykernel_53307/2900954371.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  pitcher_df['scandal'] = pitcher_df.player_id.apply(lambda x: 1 if x in scandal_ids else 0)\n\n\n\nbatter_df = batter_df.merge(batter_stats, on='player_id', how='left')\npitcher_df = pitcher_df.merge(pitcher_stats, on='player_id', how='left')\n\nNext, we attach the appearances by position data with another merge\n\nbatter_df = batter_df.merge(appearances_df, on='player_id', how='left')\npitcher_df = pitcher_df.merge(appearances_df, on='player_id', how='left')\n\nOne other thing we must deal with, is the fact that some advanced metrics like championship win probability added don’t appear in the data until 1912, so players who played before and after 1912 all have depressed values due to its summative property. To counter this, we will override the values for these columns for the first few year of voting, until the league resumed play after WWII in 1946.\n\npitcher_df.head().columns\n\nIndex(['name', 'player_id', 'voting_year', 'year_on_ballot', 'votes_pct',\n       'ly_votes_pct', 'outcome', 'position', 'scandal', 'p_war', 'p_w',\n       'p_win_loss_perc', 'p_earned_run_avg', 'p_earned_run_avg_plus', 'p_g',\n       'p_gs', 'p_sho', 'p_sv', 'p_ip', 'p_so', 'p_whip', 'p_fip',\n       'p_strikeouts_per_base_on_balls', 'p_batting_avg',\n       'p_onbase_plus_slugging', 'p_home_run_perc', 'p_strikeout_perc',\n       'p_cwpa_def', 'p_baseout_runs', 'b_war', 'b_batting_avg',\n       'b_onbase_plus_slugging_plus', 'cy_youngs', 'gold_gloves', 'mvps',\n       'all_stars', 'G_p_app', 'G_c', 'G_1b', 'G_2b', 'G_3b', 'G_ss',\n       'G_lf_app', 'G_cf_app', 'G_rf_app', 'G_dh'],\n      dtype='object')\n\n\n\n# Update some of the advanced stats for very early players\nearly_batters = batter_df[batter_df.voting_year &lt;= 1945]\nearly_cwpa =  early_batters.b_cwpa_bat.mean()\nearly_re24 = early_batters.b_baseout_runs.mean()\n\nbatter_df.b_cwpa_bat = batter_df.apply(lambda x: early_cwpa if x.voting_year &lt;= 1945 else x.b_cwpa_bat, axis=1)\nbatter_df.b_baseout_runs = batter_df.apply(lambda x: early_re24 if x.voting_year &lt;= 1945 else x.b_baseout_runs, axis=1)\n\n# And do that same for the pitchers DataFrame\nearly_pitchers = pitcher_df[pitcher_df.voting_year &lt;= 1945]\nearly_cwpa =  early_pitchers.p_cwpa_def.mean()\nearly_re24 = early_pitchers.p_baseout_runs.mean()\n\npitcher_df.p_cwpa_def = pitcher_df.apply(lambda x: early_cwpa if x.voting_year &lt;= 1945 else x.p_cwpa_def, axis=1)\npitcher_df.p_baseout_runs = pitcher_df.apply(lambda x: early_re24 if x.voting_year &lt;= 1945 else x.p_baseout_runs, axis=1)\n\nFinally, we write out our final datasets to the processed data folder!\n\nbatter_df.to_csv('../../data/processed-data/final_batter_df.csv', index=False)\npitcher_df.to_csv('../../data/processed-data/final_pitcher_df.csv', index=False)\n\nNow that all of our data is cleaned, we move forward to exploring our datasets in depth, and begin analyzing our data! This can be found in the Exploratory Data Analysis tab."
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html",
    "href": "technical-details/supervised-learning/summary.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we explore a number of ‘supervised learning’ techniques in an attempt to predict the outcome of BBWAA votes! For some background, supervised learning, unlike unsupervised learning is the branch of machine learning where the algorithms are given the underlying truth about each point (in our case the voting result - elected, expired, etc.). With the target information present for the machine, it ‘learns’ to connect the the patterns in the data with the specific outcomes, hopefully reaching a place where it can make accurate predictions for new, unseen data.\nWithin the realm of supervised learning, there are two major tasks that we will utilize in this section. They are classification and regression. Classification is used when the target variable is categorical in nature1. For example, prediction the outcome of the BBWAA vote would be a classification task, with categories like elected, expired, in limbo, etc. Within classification, there are two common tasks - binary classification and multi-class classification. The example just described would be a multi-class problem, as there are more than two possible outcomes. On the other hand, if we only work to predict election vs. non-election, it becomes a binary classification problem, as there are only two possible classes1.\nThe second process, regression, is used to make predictions of a numeric or continuous variable2. While the outcomes of the the BBWAA voting are categorical, the underlying percentage of votes recieved can be predicted via regression. The process, at a high level, remains similar, with the regression model ‘learning’ the connections between the underlying data and the target values, but instead the output is now continuous2.\n\n\n\n\n\nK-Nearest neighbors is one the more simpler methods out there, taking direct advantage of the fact, which we saw earlier, that points of similar target classes/values tend to exist close to one another3. With this fact, the K-Neighbors algorithm takes in a value for K, and for any prediction returns the average or majority of the K closest other points to it3. For example, if the algorithm is making a prediction for a point with k=3, and the 3 other data points closest to the predicion point are [elected, elected, eliminated], the prediction returned would be elected due to the majority vote. If instead we were utiling K-Neighbors for regression, and the 3 nearest points were [50%, 60%, 70%] (votes received), the prediction would be 60% via the mean.\n\n\n\nDecision trees, like K-Nearest Neighbors, can be used for both regression and classification tasks. To make predictions, a decision tree splits the data a series of times, grouping based on the underlying value of specific features4. Once the data has been split, the final resulting group becomes the prediction. To ‘learn’, the decision tree determines which features and values to split the data on, such that it results in groupings that accurately reflect the true outcomes4.\nRandom forests utilize a multitude of smaller decision trees, return a prediction of the most common result from the smaller trees, or average of the predictions in the regression case, similar to the majority ruling in K-Neighbors5\n\n\n\nLinear Regression is the most popular form of regression, making predictions based on the line (or hyperplane in more than 3 dimensions) learned during training that minimizes the prediction error of all points in the dataset. There are a few methods to find this optimal line, but they each work to find it such that the sum of the squares of the distance between each datapoint and the line is minimized. It it only used for regression tasks rather than classification tasks, and assumes a linear relationship between each of the features and the target variable6.\n\n\n\nWhile technically titled as a regression, Logistic regression is most often used for classification problems7. This is because logistic regression predictions are bound to 0-1, making them interpretable as percentages of ‘success’. Thus, when utilizing logistic regression for binary classification, say of ‘election’, a model output of 0.76 would become a prediction of elected as the probability of success is &gt;50%. Logistic regression can also be utilized for multi-class predictions7."
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html#what-is-supervised-learning",
    "href": "technical-details/supervised-learning/summary.html#what-is-supervised-learning",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we explore a number of ‘supervised learning’ techniques in an attempt to predict the outcome of BBWAA votes! For some background, supervised learning, unlike unsupervised learning is the branch of machine learning where the algorithms are given the underlying truth about each point (in our case the voting result - elected, expired, etc.). With the target information present for the machine, it ‘learns’ to connect the the patterns in the data with the specific outcomes, hopefully reaching a place where it can make accurate predictions for new, unseen data.\nWithin the realm of supervised learning, there are two major tasks that we will utilize in this section. They are classification and regression. Classification is used when the target variable is categorical in nature1. For example, prediction the outcome of the BBWAA vote would be a classification task, with categories like elected, expired, in limbo, etc. Within classification, there are two common tasks - binary classification and multi-class classification. The example just described would be a multi-class problem, as there are more than two possible outcomes. On the other hand, if we only work to predict election vs. non-election, it becomes a binary classification problem, as there are only two possible classes1.\nThe second process, regression, is used to make predictions of a numeric or continuous variable2. While the outcomes of the the BBWAA voting are categorical, the underlying percentage of votes recieved can be predicted via regression. The process, at a high level, remains similar, with the regression model ‘learning’ the connections between the underlying data and the target values, but instead the output is now continuous2."
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html#supervised-methods-used",
    "href": "technical-details/supervised-learning/summary.html#supervised-methods-used",
    "title": "Introduction",
    "section": "",
    "text": "K-Nearest neighbors is one the more simpler methods out there, taking direct advantage of the fact, which we saw earlier, that points of similar target classes/values tend to exist close to one another3. With this fact, the K-Neighbors algorithm takes in a value for K, and for any prediction returns the average or majority of the K closest other points to it3. For example, if the algorithm is making a prediction for a point with k=3, and the 3 other data points closest to the predicion point are [elected, elected, eliminated], the prediction returned would be elected due to the majority vote. If instead we were utiling K-Neighbors for regression, and the 3 nearest points were [50%, 60%, 70%] (votes received), the prediction would be 60% via the mean.\n\n\n\nDecision trees, like K-Nearest Neighbors, can be used for both regression and classification tasks. To make predictions, a decision tree splits the data a series of times, grouping based on the underlying value of specific features4. Once the data has been split, the final resulting group becomes the prediction. To ‘learn’, the decision tree determines which features and values to split the data on, such that it results in groupings that accurately reflect the true outcomes4.\nRandom forests utilize a multitude of smaller decision trees, return a prediction of the most common result from the smaller trees, or average of the predictions in the regression case, similar to the majority ruling in K-Neighbors5\n\n\n\nLinear Regression is the most popular form of regression, making predictions based on the line (or hyperplane in more than 3 dimensions) learned during training that minimizes the prediction error of all points in the dataset. There are a few methods to find this optimal line, but they each work to find it such that the sum of the squares of the distance between each datapoint and the line is minimized. It it only used for regression tasks rather than classification tasks, and assumes a linear relationship between each of the features and the target variable6.\n\n\n\nWhile technically titled as a regression, Logistic regression is most often used for classification problems7. This is because logistic regression predictions are bound to 0-1, making them interpretable as percentages of ‘success’. Thus, when utilizing logistic regression for binary classification, say of ‘election’, a model output of 0.76 would become a prediction of elected as the probability of success is &gt;50%. Logistic regression can also be utilized for multi-class predictions7."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were not used often during this project, but did have some contributions as small issues arose. Below is the way they were used:"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nGenerated project title ideas"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nFor general information in the data collection process on using BeautifulSoup for scraping commented HTML.\nFor general information on fixing/rewriting git branch after large data file pushed\nDeveloped a function to decode names back into utf-8\nFor writing a line of code to drop/select many specific columns from DataFrames\nFor formatting large markdown tables\nHelped to debug 3d plotly rendering issues\nFor formatting supervised learning results into dataframes"
  },
  {
    "objectID": "technical-details/eda/Citation1.html",
    "href": "technical-details/eda/Citation1.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Here too, we see a few very important takeaways:\n\nThe distribution of voting outcomes changes over the course of the major eras. As we move throughout history, eliminations take a backseat role to limbos, meaning that in the first few attemps on the ballot, players were more likely to receive very low shares of the vote. Additionally, while still a very low total share, we do see the election share slowing rising. Recall however, that the overall number of elections is not increasing on an annual basis as evidence in the above line chart.\nThe total number of people/votes drastically decreases over time. This is likely caused by the creation of the BBWAA selection committee that determines the players on the ballot each year, and limits it to 40, which occured in 19681\n\n\n\n\n\nReferences\n\n1. BBWA. BBWA Voting Rules History."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "To collect the necessary data from Baseball Reference, we utilize Python’s Beautiful Soup package for webscraping. Thankfully, the underlying data is in a nice tabular format, so the majority of the work can be completed by simply parsing the HTML to find the required tables and cells, before ‘rebuilding’ into DataFrames with Pandas.\nTo facilitate the webscraping, we develop 2 classes functions to crawl, collect, and save the data for each of our 2 major data origins, individual players and prior Hall of Fame votes. An outline of the two classes can be seen below:\n\n\n\n\n\n\n\n\nClass Name Name\nPlayerScraper\nHOFScraper\n\n\n\n\nMain Function #1\nscrape_player_info  Collects names and IDs for all MLB players\nscrape_hof_inductees  Collects a list of all players who have been inducted to the Hall of Fame, along with their mothod of induction and final vote total.\n\n\nMain Function #2\nscrape_player_stats  Collects the yearly an career batting+pitching stats for a given set of mlb players, along with their career awards\nscrape_hof_voting  Collects the dataset of all annual BBWA Hall of Fame voting by player and year\n\n\n\n\n\nSee below for the full underlying PlayerScraper class"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#classes",
    "href": "technical-details/data-collection/methods.html#classes",
    "title": "Methods",
    "section": "",
    "text": "See below for the full underlying PlayerScraper class"
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "Three major challengs arose during the data collection process. These are commented HTML, request limiting, and changes in HTML formatting.\n\n\nWhile all tables are visually present on the Baseball Reference webpage for individual player stats, tables beyond the standard stats are coded as comments in the HTML. Becuase of this, BeautifulSoup fails to read them in with a simple find_all call. To account for this, we build the try/except blocks into the advanced stats scraping that fall back onto searching within the commented HTML blocks for the given tables.\n\n\n\nTo project servers, Baseball Reference limits requests to no more than 20/minute. As such, all Baseball Reference scraping code contains a 4 second sleep between requests. While this is not an issue for HOF scraping, there are ~23k baseball players we scrape stats for, resulting in ~25 hours of downtime. With this limitation present, all player data was scraped on a separate mini PC and final data transfered back to the local machine. To optimize collection speed, one could parallelize the collection process across multiple machines, each scraping data for a different set of players, although this is likely not in good faith to the Baseball Reference Terms of Use.\n\n\n\nFor ~9% of individual player stats, the HTML of the Baseball Reference webpage was changes in ways that did not allow our constructed scraping class to retrieve the data. As a result, we do not have data for these players, unless they have been inducted into the Hall of Fame, in which case we have basic batting+pitching stats from their Hall of Fame page. If time permits in a future analysis, we can create different ‘branches’ of our stats scraper to account for these HTML differences on an individual by individual basis and complete the full dataset.\n\n\n\n\nAside from the two challenges, all data collection processes went as expected. Additinally, final data collection aligns with ingoing expectations and prior research standards, as MLB data is static history, with a limited universe of commonly used metrics.\n\n\n\nNow that the all data has been collected, we can move forward and take a deeper dive into learning about the data we have collected, as we clean it and begin exploring trends as they relate to our primary research question! To see these analyses, toggle to the Data Cleaning and Exploratory Data Analyis tabs of the website."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Three major challengs arose during the data collection process. These are commented HTML, request limiting, and changes in HTML formatting.\n\n\nWhile all tables are visually present on the Baseball Reference webpage for individual player stats, tables beyond the standard stats are coded as comments in the HTML. Becuase of this, BeautifulSoup fails to read them in with a simple find_all call. To account for this, we build the try/except blocks into the advanced stats scraping that fall back onto searching within the commented HTML blocks for the given tables.\n\n\n\nTo project servers, Baseball Reference limits requests to no more than 20/minute. As such, all Baseball Reference scraping code contains a 4 second sleep between requests. While this is not an issue for HOF scraping, there are ~23k baseball players we scrape stats for, resulting in ~25 hours of downtime. With this limitation present, all player data was scraped on a separate mini PC and final data transfered back to the local machine. To optimize collection speed, one could parallelize the collection process across multiple machines, each scraping data for a different set of players, although this is likely not in good faith to the Baseball Reference Terms of Use.\n\n\n\nFor ~9% of individual player stats, the HTML of the Baseball Reference webpage was changes in ways that did not allow our constructed scraping class to retrieve the data. As a result, we do not have data for these players, unless they have been inducted into the Hall of Fame, in which case we have basic batting+pitching stats from their Hall of Fame page. If time permits in a future analysis, we can create different ‘branches’ of our stats scraper to account for these HTML differences on an individual by individual basis and complete the full dataset."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Aside from the two challenges, all data collection processes went as expected. Additinally, final data collection aligns with ingoing expectations and prior research standards, as MLB data is static history, with a limited universe of commonly used metrics."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Now that the all data has been collected, we can move forward and take a deeper dive into learning about the data we have collected, as we clean it and begin exploring trends as they relate to our primary research question! To see these analyses, toggle to the Data Cleaning and Exploratory Data Analyis tabs of the website."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Final Report\nWe embarked on this project to explore the state and history of Baseball’s Hall of Fame. For so many children and adults alike, baseball is an incredibly comforting pastime. For those lucky enough to play it, the goal is one day suiting up to take the field for an MLB game. While only slightly over 20,000 men have had the golden opportunity to accomplish this feat, only ~1% achieve the ultimate goal: Baseball Immortality and a spot in the Hall of Fame. Because so many people are in love with the game, and because the peak of accomplishment is so difficult to achieve, it seems only fitting to analyze what goes into a Hall of Fame career.\nThere were three major objectives of this research. First, we aim to understand what the landscape of the Baseball Hall of Fame looks like. Second, we wanted to investigate what it takes to make the Hall of Fame. What stats are important? How has this changed over time? Finally, our major priority was to predict the outcome of BBWAA HOF votes. Can we determine who will make the Hall of Fame on a specific ballot? Who will be eliminated from future contention? By doing this exercise, we hope to offer a path to making predictions on future ballots, so fans can understand what chances their favorite players have to achieving imortality.\nOur research was very succesful, as we have important key findings from each of our three major objectives. First, when looking at the Hall of Fame, we confirm that it is truly a difficult accomplishment, as only ~1% of any player who has ever stepped foot on the field end up in the Hall of Fame. Additionally, even if you are lucky enough to end up on a HOF ballot, only ~13% of all votes end in elections, and the majority of players throughout history have been eliminated from any future contention in just their first year of eligibility.\nWhen it comes to the qualifications needed to make the Hall of Fame, we determined that the position played make a noticable impact. Some positions like first base and third base make up a plurality of succesful elections, while positions such as catcher and second base are much less likely. As a result, if your favorite player is a catcher or second baseman, like mine, they may have a higher bar to make the Hall. . When looking at the statistics that aid in a succesful election, it became clear that many of the commonly cited basic stats do a good job of predicting whether or not a player will make the Hall. These included Hits, Walks, and MVP awards won. Additionally, advanced stats like WAR, which is commonly thought of as the ultimate measure of ‘goodness’ in baseball, is the strongest predictor of an election. This is a good finding, as WAR was not a referenced stat before the recent rise of sabermeterics. On top of these findings, we also saw that the important stats in determining whether a player will make the Hall have suprisingly not changes significantly over time, meaning that even before todays advanced metrics were known, fans and voters had some level of ‘innate understand’ of their impact on the game! \nFinally, when we looked to predict the outcomes of BBWAA ballots, we learned that it is certainly a reasonable task! Whether it was predicting binary outcomes for whether a player will be elected, non-binary outcomes including classes for elimination, expiration, and limbo, or predicting the actual vote percent for the player, we uncover methods that offer accuracy increases over baseline random guessers. These results can be seen in the below 3 tables. We also learned an important lesson about imbalanced datasets with regards to interpreting results. While our classification accuracies were ~98%, around 97% of all targets were non-elections. Thus, we determined model performance by considering secondary metrics like F1, which balances the tasks of correctly predicting elections while also succesfully finding every election.\n\n\n\nBinary Classification Results by Model\n\n\n\n\n\nMulti-Class Classification Results by Model\n\n\n\n\n\nRegression Results by Model\n\n\nThe goal for this project is to be used to better understand how Hall of Fame voting operates, so fans of all teams can gauge how their favorite playes may fare in the process one day, but also to analyize and appreciate the immense skill of players across the league! I hope that other’s may use this data and potentially prediction models to look forward at current and future players working towards the Hall and predict their chances of election, instantly becoming the most informed at the inevitable HOF arguements that pop up among groups of fans.\nWhile we covered many different topics during this research, and collected many key takeaways and succesful prediction models, there too still exists future work to be completed on the topic. First and foremost, I would like to be able to include pitchers alongside batters for the full analysis. This involves going back to the scraping stage, and determining in what specific instances pitcher’s HTML differs from the majority template. I would like to further segment predictions by MLB era. In the exploratory analysis, we saw in some specific ways how the ballot has changed over time. While the differences did not seem massive, there is potential that building a model on more recent data will improve results.\nAdditionally, I would like to create an in-depth analysis of players who have not made a Hall of Fame ballot before, to determine which players if any should have either been voted on, or potentially even elected. This may result in a series of players who have been ‘robbed’ by the BBWAA. If this list exists, we can also cross reference with the other HOF committees to see if they do a good job of righting past wrongs.\nFinally, I think there is opportunity to make the predictive models more publicly accesable. By letting users access the models and plug in players they are interested in, we would create a way for readers and fans to engage more directly with the research!"
  },
  {
    "objectID": "about-me.html",
    "href": "about-me.html",
    "title": "Who I Am",
    "section": "",
    "text": "Who I Am\n\n\n\n\n\n\n\nJared Zirkes\n\n\n\n\n\n\nJared Zirkes is a first year graduate student in the Georgetown DSAN program, working towards a Master of Data Science. He completed his undergrad at Washington University in St. Louis, studing Economics and Finance, before beginning his career in market research at NielsenIQ. In his free time, Jared enjoys working on baseball analytics projects, creating and testing new recipes, and keeping track of all his favorite mediocre Minnesota based sports teams.\n\n\n\n\n\nProfessional Experience\n\nNielsenIQ (2021-2024)\n\nAs an analyst on the NielsenIQ BASES Forecasting team, I forecasted first year sales for products our clients were looking to bring to market, and offered optimized launch plans based on forecast results!\n\n\n\n\nTop Ten Liquors (2020)\n\nAt Top Ten Liquors, I developed a number of internal tools, utilizing 4 years of internal sales data. These included a sales forecaster for weekly store revenues and a daily simulation of in-store customers to optimize store guidelines.\n\n\n\n\nEducation\n\nGeorgetown University Master of Data Science and Analytics (2024-2026)\nWashington University in St. Louis Bachelor of Science in Business Administration (2017-2021)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "The Plaque Gallery at the Baseball Hall of Fame\n\n\n\n\nEvery summer, during a weekend in the middle of July, on a simple multi-purpose field in upstate New York, something magical happens as a small handful of baseball’s legends are officially inducted into the National Baseball Hall of Fame (HOF)1. In front of a crowd often reaching into the tens of thousands, a few players receive the highest honor in baseball: a lifetime in the Hall of Fame. The ceremony may only last a few hours, and the event a few days, but the honor is anything but temporary. These people have done something that only ~1% of all Major League players will accomplish: They’ve achieved baseball immortality.\nBut what does it take to achieve this greatness? Certainly one must be a superstar, but to what end? This project aims to answer that question, and offer a guide to fans and enthusiasts looking to understand the work it takes to put together a ‘Hall of Fame’ career on the field.\nBefore we can begin analyzing the Hall of Fame’s election history, we must first ensure familiarity with the Hall of Fame, and it’s election process. Established as a physical museum to honor and preserve the history of baseball, the Hall offers two paths for election. The first is via the Baseball Writers’ Association of America (BBWAA) Committe, which is the primary method of election, and used for recently retired players2. The second is the Era Committees Election, which is comprised of Hall of Fame players, plus executives and media members, and elects players no longer eligible under BBWA rules, as well as former managers, umpires, and baseball executives3. This analysis focuses solely on election via the BBWA process, as it best represents MLB players who achieved ‘great’ on the field careers.\nThe Baseball Hall of Fame also outlines eligibility and voting guidelines for election under the BBWAA system. For a simple explanation of the process, a highlight of major requirements are outlined below:\n\nEligibility Guidelines2\n\nTo appear on the BBWAA HOF Election ballot, players must:\n\nHave played in parts of 10 Major League seasons\nHave stopped playing at least 5 years ago, and no more than 15 years ago\nBe selected by a screening committee to appear on the full BBWAA voting ballot for the first time\n\n\nVoting Guidelines2\n\nOnce eligible and appearing on ballot, a player remains on the ballot until:\n\nThe player is no longer eligible (generally after 10 ballots)\nThe player revieves &lt;5% of the BBWAA vote in a given year\nThe player recieved &gt;75% of the BBWA vote in a given year (and is elected)\n\n\n\n\n\n\nGiven the extreme interest in baseball, its history, and its future Hall of Fame elections, this project will dive into the world of BBWAA HOF elections, and answer a number of questions of interest. Topics of focus include:\n\nWhat does it take to make the Hall of Fame?\nHas the benchmark for Hall of Fame election changed over time alongside the game itself?\nCan the election and/or exclusion from the Hall of Fame for a player up for vote be predicted?\nCan the actual vote % received during a BBWAA vote be predicted?\n\nThroughout the following website, we offer answers to these questions in the form of Exploratory Analysis and Visualization, in addition to data-driven prediction models, in both ‘Supervised’ and ‘Unsupervised’ methods.\n\n\n\nBaseball is a very heavily researched sport, both in terms of on-field performance, as well as history. This universe has also expanded rapidly in the last few decades, with advancements into advanced analytics and Sabermetric research4. As a result, our project has the benefit of building off a number of prior works.\nThe history of predicting a player’s likelihood of making the Hall of Fame dates back years, and encompasses methods that range in complexity. On the simpler side, there are a number of commonly cited baseline metrics, including:\n\n\n\nBill James’s Hall of Fame Monitor\n\nInvented by one of the fathers of advanced baseball research, Bill James4, the Hall of Fame monitor assigns ‘points’ for in-career accomplishments, with the expectation that 100 points gives a player a good chance to be elected, and 130 points is a ‘virtual clinch’. An abbreviated selection of points can be seen below, with the full system linked here.\n\n8 points for winning and MVP award\n10 points for each season hitting 50 home runs\n50 points for 3,500 career hits\n20 points for 4,000 career pitching strikeouts\n\n\n\n\n\nJaffe WAR Score System (JAWS)\n\nThe JAWS method offers a much simplier estimation of Hall of Fame ‘worthy-ness’, by utilizing the singular baseball metric Wins Above Replacement (WAR). WAR measures the number of wins an individual player creates for his team, indexed over the expected number of wins ‘the next man up’ might provide5. Today, WAR is one of the most commonly cited stats to quantify how good a player is overall.\nJAWS is calculated by averaging the career WAR for a player with the sum of his 7 highest years of WAR, offering a look at both his overall strength and peak strength, before indexing over other players in from the same defensive position6. This creates a simple relative understanding of how the player performed against other possible Hall of Fame candidates.\n\n\n\n\n\nBeyond simple metrics, there too is previous research on the predictibility of Hall of Fame Elections. In his paper titled, ‘A Predictive Model of Whether a Major League Baseball Player Will be Inducted into the Hall of Fame’, Aaron Springer fits different Machine Learning models on a smattering of baseball stats across ~200 former MLB players to classify Hall of Famers and non-Hall of Famers with strong accuracy7. Additionally, in a paper published on FanGraphs, a site for advanced baseball research, one user trains an xgboost model on WAR based metrics, while also including external player information, such as the presence of scandals like doping and gambling8.\n\n\n\n\nOur projects aims to build upon prior research in a number of ways. Firstly, while prior works create a solid fountation, some do not account for the drastic changes the game of baseball has gone through in its long history. Additionally, much of the prior work aims to predict the presence of a Hall of Fame induction given a player, but we look to predict elections at the time of the ballot, which leverages a single-step markov assumption that the prior year’s (and only the prior year’s) voting history can help predict the current year’s results. Finally, we also significantly expand upon the player pool, by using our models to answer if any players throughout MLB history have been ‘wrongfully’ left out of the Hall of Fame.\n\n\n\nTo continue learning about this project, you can continue to the data collection tab, where the underlying data sources powering this project are discussed."
  },
  {
    "objectID": "index.html#the-national-baseball-hall-of-fame",
    "href": "index.html#the-national-baseball-hall-of-fame",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "Every summer, during a weekend in the middle of July, on a simple multi-purpose field in upstate New York, something magical happens as a small handful of baseball’s legends are officially inducted into the National Baseball Hall of Fame (HOF)1. In front of a crowd often reaching into the tens of thousands, a few players receive the highest honor in baseball: a lifetime in the Hall of Fame. The ceremony may only last a few hours, and the event a few days, but the honor is anything but temporary. These people have done something that only ~1% of all Major League players will accomplish: They’ve achieved baseball immortality.\nBut what does it take to achieve this greatness? Certainly one must be a superstar, but to what end? This project aims to answer that question, and offer a guide to fans and enthusiasts looking to understand the work it takes to put together a ‘Hall of Fame’ career on the field.\nBefore we can begin analyzing the Hall of Fame’s election history, we must first ensure familiarity with the Hall of Fame, and it’s election process. Established as a physical museum to honor and preserve the history of baseball, the Hall offers two paths for election. The first is via the Baseball Writers’ Association of America (BBWAA) Committe, which is the primary method of election, and used for recently retired players2. The second is the Era Committees Election, which is comprised of Hall of Fame players, plus executives and media members, and elects players no longer eligible under BBWA rules, as well as former managers, umpires, and baseball executives3. This analysis focuses solely on election via the BBWA process, as it best represents MLB players who achieved ‘great’ on the field careers.\nThe Baseball Hall of Fame also outlines eligibility and voting guidelines for election under the BBWAA system. For a simple explanation of the process, a highlight of major requirements are outlined below:\n\nEligibility Guidelines2\n\nTo appear on the BBWAA HOF Election ballot, players must:\n\nHave played in parts of 10 Major League seasons\nHave stopped playing at least 5 years ago, and no more than 15 years ago\nBe selected by a screening committee to appear on the full BBWAA voting ballot for the first time\n\n\nVoting Guidelines2\n\nOnce eligible and appearing on ballot, a player remains on the ballot until:\n\nThe player is no longer eligible (generally after 10 ballots)\nThe player revieves &lt;5% of the BBWAA vote in a given year\nThe player recieved &gt;75% of the BBWA vote in a given year (and is elected)"
  },
  {
    "objectID": "index.html#project-goals",
    "href": "index.html#project-goals",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "Given the extreme interest in baseball, its history, and its future Hall of Fame elections, this project will dive into the world of BBWAA HOF elections, and answer a number of questions of interest. Topics of focus include:\n\nWhat does it take to make the Hall of Fame?\nHas the benchmark for Hall of Fame election changed over time alongside the game itself?\nCan the election and/or exclusion from the Hall of Fame for a player up for vote be predicted?\nCan the actual vote % received during a BBWAA vote be predicted?\n\nThroughout the following website, we offer answers to these questions in the form of Exploratory Analysis and Visualization, in addition to data-driven prediction models, in both ‘Supervised’ and ‘Unsupervised’ methods."
  },
  {
    "objectID": "index.html#prior-work",
    "href": "index.html#prior-work",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "Baseball is a very heavily researched sport, both in terms of on-field performance, as well as history. This universe has also expanded rapidly in the last few decades, with advancements into advanced analytics and Sabermetric research4. As a result, our project has the benefit of building off a number of prior works.\nThe history of predicting a player’s likelihood of making the Hall of Fame dates back years, and encompasses methods that range in complexity. On the simpler side, there are a number of commonly cited baseline metrics, including:\n\n\n\nBill James’s Hall of Fame Monitor\n\nInvented by one of the fathers of advanced baseball research, Bill James4, the Hall of Fame monitor assigns ‘points’ for in-career accomplishments, with the expectation that 100 points gives a player a good chance to be elected, and 130 points is a ‘virtual clinch’. An abbreviated selection of points can be seen below, with the full system linked here.\n\n8 points for winning and MVP award\n10 points for each season hitting 50 home runs\n50 points for 3,500 career hits\n20 points for 4,000 career pitching strikeouts\n\n\n\n\n\nJaffe WAR Score System (JAWS)\n\nThe JAWS method offers a much simplier estimation of Hall of Fame ‘worthy-ness’, by utilizing the singular baseball metric Wins Above Replacement (WAR). WAR measures the number of wins an individual player creates for his team, indexed over the expected number of wins ‘the next man up’ might provide5. Today, WAR is one of the most commonly cited stats to quantify how good a player is overall.\nJAWS is calculated by averaging the career WAR for a player with the sum of his 7 highest years of WAR, offering a look at both his overall strength and peak strength, before indexing over other players in from the same defensive position6. This creates a simple relative understanding of how the player performed against other possible Hall of Fame candidates.\n\n\n\n\n\nBeyond simple metrics, there too is previous research on the predictibility of Hall of Fame Elections. In his paper titled, ‘A Predictive Model of Whether a Major League Baseball Player Will be Inducted into the Hall of Fame’, Aaron Springer fits different Machine Learning models on a smattering of baseball stats across ~200 former MLB players to classify Hall of Famers and non-Hall of Famers with strong accuracy7. Additionally, in a paper published on FanGraphs, a site for advanced baseball research, one user trains an xgboost model on WAR based metrics, while also including external player information, such as the presence of scandals like doping and gambling8."
  },
  {
    "objectID": "index.html#moving-forward",
    "href": "index.html#moving-forward",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "Our projects aims to build upon prior research in a number of ways. Firstly, while prior works create a solid fountation, some do not account for the drastic changes the game of baseball has gone through in its long history. Additionally, much of the prior work aims to predict the presence of a Hall of Fame induction given a player, but we look to predict elections at the time of the ballot, which leverages a single-step markov assumption that the prior year’s (and only the prior year’s) voting history can help predict the current year’s results. Finally, we also significantly expand upon the player pool, by using our models to answer if any players throughout MLB history have been ‘wrongfully’ left out of the Hall of Fame."
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Predicting Immortality: An Analysis of Baseball’s Hall of Fame ",
    "section": "",
    "text": "To continue learning about this project, you can continue to the data collection tab, where the underlying data sources powering this project are discussed."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nLike any project, cleaning our raw data collected in the prior step is critical to ensuring we have a dataset that can be explored and modeled to extract insights. In this section, we read in the raw scraped data, and process in the following steps:\n\nReading in all the scraped data\nLooking carefully through each dataset and:\n\nEnsuring data is read in properly\nChecking for and potentially removing missing values\nDropping unnecessary columns that do not relate to the research questions\nCreating new features for for analysis\nMerging datasets together into a singular final dataset each for batters and pitchers"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "In order to conduct this Hall of Fame analysis, we require a variety of baseball data. We need data on annual Hall of Fame voting to know who has been elected into the Hall as well as annual vote totals for all players. We also require the career and annual stats for these players to understand what stats it takes to make the Hall, but also for every other player who has played in the MLB to conduct further analyses on HOF voting trends.\nThankfully, Baseball Reference has all of the data we need all on their website. Baseball Reference is a baseball encyclopedia for ‘everything’ baseball, including individual player and game stats dating back to 1871, as well as information and data on managers, teams, leagues, The Hall of Fame, in addition to many other baseball topics. With this site available, we collect our data across their webpages, as outlined below:\nList of Baseball Reference Webpages for Data Collection:\n\nHall of Fame Ballot History → A central hub for Hall of Fame voting records. This page provides links to sub-pages for the voting data in each specific year. These sub-pages contain voting data from across the voting methings, such as BBWA and the CBE.\n\nHOF Voting can be accesed here\n\nIndividual Player Statistics → Baseball Reference provides webpages on an individual player basis for recording both annual and career stats. This central player hub provides all the links to individual player pages.\n\nPage can be accesed here"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#hall-of-fame-voting-data",
    "href": "technical-details/data-collection/overview.html#hall-of-fame-voting-data",
    "title": "Overview",
    "section": "Hall of Fame Voting Data",
    "text": "Hall of Fame Voting Data\nAs outlined above, the Hall of Fame ballot history hub provides central access to each of the annual voting records. Upon reaching any of these webpages, we have full access to the BBWA voting table of interest for the given year:\n\n\n\n2024 Hall of Fame Voting Dataset\n\n\nThe dataset is tabular, with each row representing a player up for BBWA voting in the given year. Players are either highlighted yellow representing succesful induction, marked with an ‘x’, representing the end of their BBWA eligibility and unsuccesful induction, or neither, representing an unsuccesful induction, but continued eligibility.\nFeatures include 3 metrics on the player’s vote (year on ballot, number of votes, and vote%), a basic overview of their career stats, both pitching and hitting, and a handful of the previously discussed HOF metrics."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#individual-player-statistics",
    "href": "technical-details/data-collection/overview.html#individual-player-statistics",
    "title": "Overview",
    "section": "Individual Player Statistics",
    "text": "Individual Player Statistics\nSimilarly to the Hall of Fame voting data, we gain access to individual player stats pages via the central player page. Once a page is accesed, the following data of interest is accesable:\nBoth standard and advanced metrics are available for batters…\n \n…as well as for pitchers\n \nOnce again, the dataset is tabular, with each row representing a specific season for the player (or subset if they changed teams mid-season). However, at the bottom of each dataset, the aggregate stats for the players career are also presented.\nAwards\nFinally, in addition to batting and pitching stats, the player pages also offer data on awards/accomplishments from the player’s career. This data can be seen below:\n\n\n\nJohan Santana Career Awards"
  },
  {
    "objectID": "technical-details/eda/Citation2.html",
    "href": "technical-details/eda/Citation2.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "We very quickly see that electio to the Hall of Fame is quite difficult, even if you are a player worthy enough of being selected to the ballot! For both pitchers and hitters, the most common occurence is elimination, quickly followed by limbo. For each position, elected players only represent &lt;10% of the total voted body.\nWe wonder however, if this distribution, and general likelihoods have changed overtime and the game itself and potentially perspectevs on the Hall of Fame have changed. To do this, we split the history of baseball into 3 distinct categories there is often much discussion as to how the history of baseball should be split, with some views containing many more smaller eras, but sor simplicity, we group the timeline into three larger ones.\n\nThe Golden Age (1936-1968)\n\nA contemporary era of baseball, filled with a number of all-time legends. During this era baseball explodes in popularity. The ‘balance of power’ shifts heavily towards pitchers near the end, resulting in a change to pitching mound dimensions to artifically increase offense league wide1.\n\nThe Expansion Era (1969-1994)\n\nBeginning with the lowering of the mound, offense gradually increases throughout this era. Additionally, the leage expands on multiple occasions as the sport continues to increase in populatity2\n\nThe Modern Era\n\nThe Modern Era begins in 1995 when Major League Baseball breaks its strike, and popularity is fueled again by the home run races and doping scandals of the 1990s. The era continues into the modern day, where hitting for power, and throwing with maximum speed are emphasized, alongside a revolution of advanced scouting, training, and analysis3!\n\n\nWhile many players have played across two eras, we will filter player/votes into the era belonging to the year of the vote, which represents the mindset of the BBWAA at the time of the vote.\nWe begin by plotting the number of elections over time, segmenting by era\n\n\n\n\nReferences\n\n1. Contributers, W. Golden Age of Baseball. (2024).\n\n\n2. Bullpen, B. Expansion.\n\n\n3. Contributers, W. History of Baseball in the United States. (2024)."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "M: 11-04-2024\n\nBegun data collection by writing code to scrape a table of all MLB Hall of Fame (HOF) members from https://www.baseball-reference.com/awards/hof.shtml\n\nM: 11-18-2024\n\nContinued data collection, scraping annual HOF voting records from each year based on the following webpage: https://www.baseball-reference.com/awards/hall-of-fame-ballot-history.shtml\nBuilt landing page/about me page\n\nSat: 12-7-2024\n\nFinished writing data collection classes and begun scraping individual player stats.\n\nSun: 12-8-2024\n\nWrote out data collection slide.\n\nWed: 12-11-2024\n\nResearched study background in more depth and wrote out project landing page\nFinalized Data Collection tab\nStarted working through Data Cleaning tab\nWrote a line of code to filter the many metrics of interest in data cleaning\nFormatted the large markdown table of pitching and batting metrics\n\nThu: 12-12-2024\n\nFinished Data Cleaning Tab\nStarted work on EDA\n\nFriday: 12-13-2024\n\nFinished EDA\n\nSat: 12-14-2024\n\nWorked through Unsupervised learning tab\n\nSun: 12-15-2024\n\nFinished unsupervised learning tab\n\nThu: 12-16-2024\n\nFinished project"
  },
  {
    "objectID": "technical-details/progress-log.html#process-log",
    "href": "technical-details/progress-log.html#process-log",
    "title": "Progress log",
    "section": "",
    "text": "M: 11-04-2024\n\nBegun data collection by writing code to scrape a table of all MLB Hall of Fame (HOF) members from https://www.baseball-reference.com/awards/hof.shtml\n\nM: 11-18-2024\n\nContinued data collection, scraping annual HOF voting records from each year based on the following webpage: https://www.baseball-reference.com/awards/hall-of-fame-ballot-history.shtml\nBuilt landing page/about me page\n\nSat: 12-7-2024\n\nFinished writing data collection classes and begun scraping individual player stats.\n\nSun: 12-8-2024\n\nWrote out data collection slide.\n\nWed: 12-11-2024\n\nResearched study background in more depth and wrote out project landing page\nFinalized Data Collection tab\nStarted working through Data Cleaning tab\nWrote a line of code to filter the many metrics of interest in data cleaning\nFormatted the large markdown table of pitching and batting metrics\n\nThu: 12-12-2024\n\nFinished Data Cleaning Tab\nStarted work on EDA\n\nFriday: 12-13-2024\n\nFinished EDA\n\nSat: 12-14-2024\n\nWorked through Unsupervised learning tab\n\nSun: 12-15-2024\n\nFinished unsupervised learning tab\n\nThu: 12-16-2024\n\nFinished project"
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html",
    "href": "technical-details/unsupervised-learning/summary.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we explore a number of ‘unsupervized learning’ techniques in order to both again build upon our understanding of the data at hand, but also to try and uncover new interesting or hidden relationships within the BBWAA voting data. For some background, unsupervised learning is a branch of machine learning where the algorithms we employ are not given the underlying truth about each point (in our case the voting result - elected, expired, etc.). Because the machine does not have the ground truths, it works to find general underlying trends or patterns in the data1. It is these general patterns and trends that are quite helpful to us as baseball fans, in learning about how BBWWAA voting functions.\nWithin the realm of unsupervised learning, there are two major tasks that we will utilize in this section. They are clustering and dimensionality reduction. Clustering is the process of grouping data points into distinct groupings based on the underlying features of each point2. By converting the data from a singular dataset into subsets of ‘similar’ groupings, again, based on underlying similarities of each datapoint, we can learn about the relationships between different datapoints, different groupings within our data, and most importantly what drives these similarities and differences. We remember that during this process our computer is not given the truthful group labels for each datapoint, so it relys on the features of the each datapoint for its grouping process.2.\nThe second process, dimensionality reduction, is used to reduce the number of dimensions our data exists in. While this sounds complicated, if we remember that each column, or feature represents a dimension of the data, this process boils down to reducing the number of columns for the data3. Ideally, we do this in ways that preserve the ‘essense’ of the original data, so that we don’t lose significant useful information that can be used later during the process. Dimensionality reduction offers a few important benefits. First, it makes data visualization of datasets with greater than 3 dimensions possible, by creating the ability to convert the data back into a 2-d or 3-d space. Beyond this however, it can also help make prediction models more accurate on unseen data by reducing correlation between features (multicolinearity) among other benefits3.\n\n\n\nIn the following Unsupervised Learning section, we will utilize a number of both clustering and dimensionality reduction techniques. We outline the chosen methods below, alongside a brief explanation of how they work:\n\n\n\n\nKMeans is one of the more popular ‘centroid based’ clustering techniques, meaning the model defines a number of ‘center points’ or centroids to act as the centers of each grouping, and assigns each datapoint to the group belonging to the centroid which it is closest to. As a result, this classifies similar points to the same centroid/group, as they tend to already exist close to one another2. The KMeans clustering algorithm works in the following way.\n\nA user defined number (k) of centroids are chosen, and the machine randomly places them in the feature space\nEach datapoint in the dataset is classified to the closest centroid, resulting in k groups of datapoints\nThe ‘center’ of each group is calculated, and the centroids are moved to these centers\n\nTo calculate the centers, the mean of each group is taken, hence the name KMeans\n\nSteps 1-3 are repeated until the centers no longer change\n\nThe biggest decision when using KMeans is the number of centroids to choose, as this drastically changes the final result of the algorithm. If we know the truthful number of groupings, we can use this number, otherwise it is common practice to run the algorithm with a multitude of different cluster numbers, and choose the final result that results in the most ‘distinct’ clusters. Distinct-ness can be calculated with a few metrics such as the inertia, but in essence we are looking for clusters that are tightly packed to one another, while remaining far away from other clusters4.\n\n\n\nDBSCAN (Density Based Spatial Clustering of Applications with Noise), unlike a centroid based method, is a Density based method meaning it created clusters of the data based on the density of the data at specific points. This is calculated by looking at the number of points which are ‘close’ to any given other point, with closeness being a distance again defined by the user5.\nThere are generally two important benefits of DBSCAN over K-Means. First, the user does not have to determine the number of clusters. DBSCAN is automatically create its ‘optimal’ number of clusters based on the relative densities of the dataset. The second benefit is that DBSCAN is much less sensetive to outliers. In K-Means, an outlier can drag the cluster out toward it during the means calculations, but DBSCAN is able to classify outliers as ‘noise’, and not assign them to specific clusters5.\n\n\n\n\n\n\nPrincipal Component Analysis, or PCA, is one of the most popular dimensionality reduction techniques, reducing the dimensions of a dataset by projecting the data onto the ‘principal components’ of the data. These principal components are created by using the eigenvectors of the eigenvalues of the data’s covariance matrix3. The user can chose the number of principal components to project onto, which directly determines the number of dimensions the resulting data will exist in after the projection.\nThe added benefit of PCA is that by projecting onto the principal components, we are minimizing information loss during the reduction. Thus, our resulting data is as informationally similar to our original data as possible, but also much more free from multicolinearity and noise3!\n\n\n\nt-distributed Stochastic Neighbor Embedding, or t-SNE, is another method of dimensionality reduction that differs from PCA in its effectiveness with non-linear data. Rather than preserving the maximum variance of the dataset, its primary goal is to map data onto a lower dimension in such a way that points that are close to one another in the original dimensions stay close to one another in the resulting lower dimensions6. Because of this, it is often a great visualization technique for reduced data. The DataCamp page on how t-SNE works offers a great explanation of the major steps within t-SNE, which I have included below:\n\nt-SNE models a point being selected as a neighbor of another point in both higher and lower dimensions. It starts by calculating a pairwise similarity between all data points in the high-dimensional space using a Gaussian kernel. The points far apart have a lower probability of being picked than the points close together6.\nThe algorithm then tries to map higher-dimensional data points onto lower-dimensional space while preserving the pairwise similarities6.\n\n\n\n\n\n\nNow that we are familiar with the underpinnings of supervised learning, including the major techniques and applications of Clustering and Dimensionality Reduction, we can now move forward by applying these techniques to our BBWWAA voting dataset to visualize and explore the underlying relationships of the data!"
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html#what-is-unsupervised-learning",
    "href": "technical-details/unsupervised-learning/summary.html#what-is-unsupervised-learning",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we explore a number of ‘unsupervized learning’ techniques in order to both again build upon our understanding of the data at hand, but also to try and uncover new interesting or hidden relationships within the BBWAA voting data. For some background, unsupervised learning is a branch of machine learning where the algorithms we employ are not given the underlying truth about each point (in our case the voting result - elected, expired, etc.). Because the machine does not have the ground truths, it works to find general underlying trends or patterns in the data1. It is these general patterns and trends that are quite helpful to us as baseball fans, in learning about how BBWWAA voting functions.\nWithin the realm of unsupervised learning, there are two major tasks that we will utilize in this section. They are clustering and dimensionality reduction. Clustering is the process of grouping data points into distinct groupings based on the underlying features of each point2. By converting the data from a singular dataset into subsets of ‘similar’ groupings, again, based on underlying similarities of each datapoint, we can learn about the relationships between different datapoints, different groupings within our data, and most importantly what drives these similarities and differences. We remember that during this process our computer is not given the truthful group labels for each datapoint, so it relys on the features of the each datapoint for its grouping process.2.\nThe second process, dimensionality reduction, is used to reduce the number of dimensions our data exists in. While this sounds complicated, if we remember that each column, or feature represents a dimension of the data, this process boils down to reducing the number of columns for the data3. Ideally, we do this in ways that preserve the ‘essense’ of the original data, so that we don’t lose significant useful information that can be used later during the process. Dimensionality reduction offers a few important benefits. First, it makes data visualization of datasets with greater than 3 dimensions possible, by creating the ability to convert the data back into a 2-d or 3-d space. Beyond this however, it can also help make prediction models more accurate on unseen data by reducing correlation between features (multicolinearity) among other benefits3."
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html#unsupervised-methods-used",
    "href": "technical-details/unsupervised-learning/summary.html#unsupervised-methods-used",
    "title": "Introduction",
    "section": "",
    "text": "In the following Unsupervised Learning section, we will utilize a number of both clustering and dimensionality reduction techniques. We outline the chosen methods below, alongside a brief explanation of how they work:\n\n\n\n\nKMeans is one of the more popular ‘centroid based’ clustering techniques, meaning the model defines a number of ‘center points’ or centroids to act as the centers of each grouping, and assigns each datapoint to the group belonging to the centroid which it is closest to. As a result, this classifies similar points to the same centroid/group, as they tend to already exist close to one another2. The KMeans clustering algorithm works in the following way.\n\nA user defined number (k) of centroids are chosen, and the machine randomly places them in the feature space\nEach datapoint in the dataset is classified to the closest centroid, resulting in k groups of datapoints\nThe ‘center’ of each group is calculated, and the centroids are moved to these centers\n\nTo calculate the centers, the mean of each group is taken, hence the name KMeans\n\nSteps 1-3 are repeated until the centers no longer change\n\nThe biggest decision when using KMeans is the number of centroids to choose, as this drastically changes the final result of the algorithm. If we know the truthful number of groupings, we can use this number, otherwise it is common practice to run the algorithm with a multitude of different cluster numbers, and choose the final result that results in the most ‘distinct’ clusters. Distinct-ness can be calculated with a few metrics such as the inertia, but in essence we are looking for clusters that are tightly packed to one another, while remaining far away from other clusters4.\n\n\n\nDBSCAN (Density Based Spatial Clustering of Applications with Noise), unlike a centroid based method, is a Density based method meaning it created clusters of the data based on the density of the data at specific points. This is calculated by looking at the number of points which are ‘close’ to any given other point, with closeness being a distance again defined by the user5.\nThere are generally two important benefits of DBSCAN over K-Means. First, the user does not have to determine the number of clusters. DBSCAN is automatically create its ‘optimal’ number of clusters based on the relative densities of the dataset. The second benefit is that DBSCAN is much less sensetive to outliers. In K-Means, an outlier can drag the cluster out toward it during the means calculations, but DBSCAN is able to classify outliers as ‘noise’, and not assign them to specific clusters5.\n\n\n\n\n\n\nPrincipal Component Analysis, or PCA, is one of the most popular dimensionality reduction techniques, reducing the dimensions of a dataset by projecting the data onto the ‘principal components’ of the data. These principal components are created by using the eigenvectors of the eigenvalues of the data’s covariance matrix3. The user can chose the number of principal components to project onto, which directly determines the number of dimensions the resulting data will exist in after the projection.\nThe added benefit of PCA is that by projecting onto the principal components, we are minimizing information loss during the reduction. Thus, our resulting data is as informationally similar to our original data as possible, but also much more free from multicolinearity and noise3!\n\n\n\nt-distributed Stochastic Neighbor Embedding, or t-SNE, is another method of dimensionality reduction that differs from PCA in its effectiveness with non-linear data. Rather than preserving the maximum variance of the dataset, its primary goal is to map data onto a lower dimension in such a way that points that are close to one another in the original dimensions stay close to one another in the resulting lower dimensions6. Because of this, it is often a great visualization technique for reduced data. The DataCamp page on how t-SNE works offers a great explanation of the major steps within t-SNE, which I have included below:\n\nt-SNE models a point being selected as a neighbor of another point in both higher and lower dimensions. It starts by calculating a pairwise similarity between all data points in the high-dimensional space using a Gaussian kernel. The points far apart have a lower probability of being picked than the points close together6.\nThe algorithm then tries to map higher-dimensional data points onto lower-dimensional space while preserving the pairwise similarities6."
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html#summary",
    "href": "technical-details/unsupervised-learning/summary.html#summary",
    "title": "Introduction",
    "section": "",
    "text": "Now that we are familiar with the underpinnings of supervised learning, including the major techniques and applications of Clustering and Dimensionality Reduction, we can now move forward by applying these techniques to our BBWWAA voting dataset to visualize and explore the underlying relationships of the data!"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "In order to conduct this Hall of Fame analysis, we require a variety of baseball data. We need data on annual Hall of Fame voting to know who has been elected into the Hall as well as annual vote totals for all players. We also require the career and annual stats for these players to understand what stats it takes to make the Hall, but also for every other player who has played in the MLB to conduct further analyses on HOF voting trends.\nThankfully, Baseball Reference has all of the data we need all on their website. Baseball Reference is a baseball encyclopedia for ‘everything’ baseball, including individual player and game stats dating back to 1871, as well as information and data on managers, teams, leagues, The Hall of Fame, in addition to many other baseball topics. With this site available, we collect our data across their webpages, as outlined below:\nList of Baseball Reference Webpages for Data Collection:\n\nHall of Fame Ballot History → A central hub for Hall of Fame voting records. This page provides links to sub-pages for the voting data in each specific year. These sub-pages contain voting data from across the voting methings, such as BBWA and the CBE.\n\nHOF Voting can be accesed here\n\nIndividual Player Statistics → Baseball Reference provides webpages on an individual player basis for recording both annual and career stats. This central player hub provides all the links to individual player pages.\n\nPage can be accesed here"
  },
  {
    "objectID": "technical-details/data-collection/main.html#hall-of-fame-voting-data",
    "href": "technical-details/data-collection/main.html#hall-of-fame-voting-data",
    "title": "Data Collection",
    "section": "Hall of Fame Voting Data",
    "text": "Hall of Fame Voting Data\nAs outlined above, the Hall of Fame ballot history hub provides central access to each of the annual voting records. Upon reaching any of these webpages, we have full access to the BBWA voting table of interest for the given year:\n\n\n\n2024 Hall of Fame Voting Dataset\n\n\nThe dataset is tabular, with each row representing a player up for BBWA voting in the given year. Players are either highlighted yellow representing succesful induction, marked with an ‘x’, representing the end of their BBWA eligibility and unsuccesful induction, or neither, representing an unsuccesful induction, but continued eligibility.\nFeatures include 3 metrics on the player’s vote (year on ballot, number of votes, and vote%), a basic overview of their career stats, both pitching and hitting, and a handful of the previously discussed HOF metrics."
  },
  {
    "objectID": "technical-details/data-collection/main.html#individual-player-statistics",
    "href": "technical-details/data-collection/main.html#individual-player-statistics",
    "title": "Data Collection",
    "section": "Individual Player Statistics",
    "text": "Individual Player Statistics\nSimilarly to the Hall of Fame voting data, we gain access to individual player stats pages via the central player page. Once a page is accesed, the following data of interest is accesable:\nBoth standard and advanced metrics are available for batters…\n \n…as well as for pitchers\n \nOnce again, the dataset is tabular, with each row representing a specific season for the player (or subset if they changed teams mid-season). However, at the bottom of each dataset, the aggregate stats for the players career are also presented.\nAwards\nFinally, in addition to batting and pitching stats, the player pages also offer data on awards/accomplishments from the player’s career. This data can be seen below:\n\n\n\nJohan Santana Career Awards"
  },
  {
    "objectID": "technical-details/data-collection/main.html#classes",
    "href": "technical-details/data-collection/main.html#classes",
    "title": "Data Collection",
    "section": "Classes",
    "text": "Classes\nSee below for the full underlying PlayerScraper class\n\n\nCode\nimport requests\nimport time\nimport re\nimport pickle as pkl\n\nimport pandas as pd\n\nfrom bs4 import BeautifulSoup as bs\nfrom bs4 import Comment\nfrom IPython.display import clear_output\n\nclass PlayerScraper():\n    def __init__(self, save_csv=True):\n        self.save_csv = save_csv\n\n    def scrape_player_info(self, leading_letters = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n                                         'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n                                         'U', 'V', 'W', 'X', 'Y', 'Z')):\n\n        # Create storage\n        names = []\n        suffixes = []\n        ids = []\n\n        for letter in leading_letters:\n            # Sleep to remain under rate limits\n            time.sleep(4)\n\n            # Create the url with the data for all players with a last name starting with the letter\n            letter_url = f'https://www.baseball-reference.com/players/{letter.lower()}/'\n            \n            # Request the url that contains the data for all players belonging to the letter\n            req = requests.get(letter_url)\n            soup = bs(req.text, 'html.parser')\n\n            player_soup = soup.find_all('div', {'id':'div_players_'})[0]\n\n            # Grab all names\n            player_names = [name.a.text for name in player_soup.find_all('p')]\n            names = names + player_names\n\n            # Grab all URL suffixes\n            url_suffixes = [name.a['href'] for name in player_soup.find_all('p')]\n            suffixes = suffixes + url_suffixes\n\n            # Grab IDs\n            player_ids = [name.a['href'].split('/')[-1].split('.sh')[0] for name in player_soup.find_all('p')]\n            ids = ids + player_ids\n\n        # Combine everything into a dataframe\n        player_df = pd.DataFrame({'name':names, 'id':ids, 'url_suffix':suffixes})\n\n        if self.save_csv:\n            with open('../../data/raw-data/all_player_info.csv', 'w') as file:\n                player_df.to_csv(file, index=False)\n\n    def _scrape_career_batting_or_pitching_stats_from_soup(self, position: str, soup: bs):\n        if position.lower() not in ['pitching', 'batting', 'fielding']:\n            raise ValueError(f\"Position {position} is invalid: Must be one of 'batting' or 'pitching' or 'fielding'\")\n\n        is_position = True if soup.find_all('div', {'id':f'all_players_standard_{position}'}) else False\n\n        if is_position:\n            try: # Pull the career standard stats\n                standard_footer = soup.find_all('div', {'id':f'all_players_standard_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_standard_{position}.')})\n            except (IndexError, AttributeError) as e: # If the table is commented out\n                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n                for comment in comments:\n                    if f'players_standard_{position}' in comment:\n                        table_html = bs(comment, 'html.parser')\n                        standard_footer = table_html.find_all('table', {'id':f'players_standard_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_standard_{position}.')})\n                        break\n            stat_names = [cell['data-stat'] for cell in standard_footer.find_all('td')]\n            stat_values = [cell.text for cell in standard_footer.find_all('td')]\n            career_standard_stats = pd.DataFrame([stat_values], columns=stat_names)\n\n            if position.lower() != 'fielding':\n                try: # Pull the career advanced stats\n                    advanced_footer = soup.find_all('div', {'id':f'all_players_advanced_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_advanced_{position}.')})\n                except (IndexError, AttributeError) as e: # If the table is commented out\n                    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n                    for comment in comments:\n                        if f'players_advanced_{position}' in comment:\n                            table_html = bs(comment, 'html.parser')\n                            advanced_footer = table_html.find_all('table', {'id':f'players_advanced_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_advanced_{position}.')})\n                            break\n                \n                stat_names = [cell['data-stat'] for cell in advanced_footer.find_all('td')]\n                stat_values = [cell.text for cell in advanced_footer.find_all('td')]\n                career_advanced_stats = pd.DataFrame([stat_values], columns=stat_names)\n\n                total_career_stats = pd.concat([career_standard_stats, career_advanced_stats], axis=1)\n\n                return total_career_stats\n            \n            else:\n                return career_standard_stats\n        \n        else: # If no data for the position\n            return pd.DataFrame()\n    \n    def _scrape_annual_batting_or_pitching_stats_from_soup(self, position, level, soup):\n\n        if position.lower() not in ['pitching', 'batting', 'fielding']:\n            raise ValueError(f\"Position {position} is invalid: Must be one of 'batting' or 'pitching' or 'fielding\")\n        \n        is_position = True if soup.find_all('div', {'id':f'all_players_standard_{position}'}) else False\n\n        if is_position:\n            try:\n                rows = soup.find_all('tr', {'id':re.compile(f'players_{level}_{position}.')})\n                existince_checker = rows[0]\n            except (IndexError, AttributeError) as e: # If the table is commented out\n                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n                for comment in comments:\n                    if f'players_{level}_{position}' in comment:\n                        table_html = bs(comment, 'html.parser')\n                        rows = table_html.find_all('table', {'id':f'players_{level}_{position}'})[0].find('tbody').find_all('tr', {'id':re.compile(f'players_{level}_{position}.')})\n                        break\n\n            headers = [cell['data-stat'] for cell in rows[0].find_all('td')]\n\n            stats_list = []\n            for row in rows:\n                stats = [cell.text for cell in row.find_all('td')]\n                stats_list.append(stats)\n            stats = pd.DataFrame(stats_list, columns=headers)\n            stats = stats[stats[stats.columns[-2]] != None]\n\n            return stats\n\n    def _scrape_acomplishments_from_soup(self, soup):\n        try:\n            accomplishments_soup = soup.find_all('ul',{'id':'bling'})[0]\n            accomplishments = [accomplishment.text for accomplishment in accomplishments_soup.find_all('li')]\n            accomplishments = ', '.join(accomplishments)\n            return accomplishments\n        except (AttributeError, IndexError) as e:\n            return ''\n\n    def _scrape_position_appearances_from_soup(self, soup):\n        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n        for comment in comments:\n            if f'div_appearances' in comment:\n                table_html = bs(comment, 'html.parser')\n                footer_row = table_html.find('tfoot')\n                headers = [cell['data-stat'] for cell in footer_row.find_all('td')]\n                stats = [cell.text for cell in footer_row.find_all('td')]\n                break\n        app_df = pd.DataFrame([stats], columns=headers)\n        return app_df\n\n    def scrape_player_stats(self, player_suffixs = (), cache_path = ''):\n        try:\n            with open(cache_path, 'rb') as fpath:\n                storage_dict = pkl.load(fpath)\n        except FileNotFoundError:\n            storage_dict = {}\n\n        save_counter = 0\n        for suffix in player_suffixs:\n            insert = suffix.split('/')[-1].split('.')[0]\n            if insert in storage_dict:\n                continue\n            \n            # Wrap everything in a try/except to catch anything unforseen, while still appending to storage dict, so we can go back later if needed\n            try:\n                # Create the url with the data for all players with a last name starting with the letter\n                player_url = f'https://www.baseball-reference.com{suffix}'\n                \n                # Request the url for the players Baseball Reference page\n                req = requests.get(player_url)\n                soup = bs(req.text, 'html.parser')\n\n                # Scrape the career batting pitching, and fielding stats\n                career_batting_stats = self._scrape_career_batting_or_pitching_stats_from_soup('batting', soup)\n                career_pitching_stats = self._scrape_career_batting_or_pitching_stats_from_soup('pitching', soup)\n                #career_fielding_stats = self._scrape_career_batting_or_pitching_stats_from_soup('fielding', soup)\n                \n                career_stats = pd.concat([career_batting_stats, career_pitching_stats], axis=1)\n\n                # Add awards/accomplishments to the career stats\n                accomplishment_list = self._scrape_acomplishments_from_soup(soup)\n                career_stats['accomplishments'] = accomplishment_list\n\n                ##### ANNUAL STATS #####\n\n                # Scrape the annual batting, pitching, and fielding stats, combining the standard and advanced for each\n                annual_standard_batting_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('batting', 'standard', soup)\n                annual_advanced_batting_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('batting', 'advanced', soup)\n                \n                # Check if the batting stats existed, and if so, merge them into one df\n                if isinstance(annual_standard_batting_stats, pd.DataFrame):\n                    annual_batting_stats = pd.concat([annual_standard_batting_stats, annual_advanced_batting_stats], axis=1)\n                    annual_batting_stats = annual_batting_stats.loc[:, ~annual_batting_stats.columns.duplicated()]\n                else:\n                    annual_batting_stats = pd.DataFrame()\n                # Get the index of the career column, and get rid of it and anything below (postseason)\n                i = annual_batting_stats[annual_batting_stats.age.str.contains(f'\\.') == True].index[0] if isinstance(annual_batting_stats, pd.DataFrame) and not annual_batting_stats.empty and len(annual_batting_stats[annual_batting_stats.age.str.contains(f'\\.') == True]) &gt; 0 else len(annual_batting_stats.index)\n                annual_batting_stats = annual_batting_stats.iloc[:i]\n\n                annual_standard_pitching_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('pitching', 'standard', soup)\n                annual_advanced_pitching_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('pitching', 'advanced', soup)\n\n                # Check if the pitching stats existed, and if so, merge them into one df\n                if isinstance(annual_standard_pitching_stats, pd.DataFrame):\n                    annual_pitching_stats = pd.concat([annual_standard_pitching_stats, annual_advanced_pitching_stats], axis=1)\n                    annual_pitching_stats = annual_pitching_stats.loc[:, ~annual_pitching_stats.columns.duplicated()]\n                else:\n                    annual_pitching_stats = pd.DataFrame()\n                # Get the index of the career column, and get rid of it and anything below (postseason)\n                i = annual_pitching_stats[annual_pitching_stats.age.str.contains(f'\\.') == True].index[0] if isinstance(annual_pitching_stats, pd.DataFrame) and not annual_pitching_stats.empty and len(annual_pitching_stats[annual_pitching_stats.age.str.contains(f'\\.') == True]) &gt; 0 else len(annual_pitching_stats)\n                annual_pitching_stats = annual_pitching_stats.iloc[:i]\n\n                # Merge any annual DataFrames that actually exist. Then drop any duplicated columns\n                real_dfs = [df for df in [annual_batting_stats, annual_pitching_stats] if not df.empty]\n                annual_stats = real_dfs[0]\n                for df in real_dfs[1:]:\n                    annual_stats = pd.merge(annual_stats, df, on=['age', 'team_name_abbr'], how='outer')\n                annual_stats = annual_stats.loc[:, ~annual_stats.columns.duplicated()]\n\n                # Drop rows that don't belong on known conditions\n                annual_stats = annual_stats.dropna(subset=['age'])\n\n                # Scrape appearences\n                appearances = self._scrape_position_appearances_from_soup(soup)\n                \n                # Save everything to our storage\n                storage_dict[insert] = {'career_stats':career_stats, 'annual_stats':annual_stats, 'appearances':appearances}\n\n            except:\n                storage_dict[insert] = 'FAILED TO PULL DATA'\n\n            save_counter += 1\n            if save_counter % 5 == 0:\n                print(insert)\n                with open('../../data/raw-data/all_player_stats.pkl', 'wb') as fpath:\n                    pkl.dump(storage_dict, fpath)\n\n            time.sleep(4)\n        \n\n\n\nAnd the HOFScraper Class\n\n\nCode\nclass HOFScraper():\n\n    def __init__(self, save_csv=True):\n        self.save_csv = save_csv\n\n    def scrape_hof_inductees(self):\n        '''Function to scrape baseball references Hall of Fame webpage, and collect information on every inducted member into the HOF.\n           Optionally, saves the data as a csv.'''\n\n        # Pull table for all players inducted to the HOF\n        hof_url = 'https://www.baseball-reference.com/awards/hof.shtml'\n        req = requests.get(hof_url)\n        soup = bs(req.text, 'html.parser')\n\n        ### Build out the dataframe, column by column ###\n\n        # Year\n        all_years = soup.find_all('th', {'data-stat':'year_ID'})\n        years = [int(all_years[n].a.text) for n in range(1, len(all_years))]\n\n        # Player Name\n        all_players = soup.find_all('td', {'data-stat':'player'})\n        players = [all_players[n].a.text for n in range(len(all_players))]\n\n        # Living Status\n        all_status = soup.find_all('td', {'data-stat':'lifespan'})\n        status = [all_status[n].text for n in range(len(all_status))]\n\n        # Entrance Method\n        all_entrance_methods = soup.find_all('td', {'data-stat':'votedBy'})\n        entrance_methods = [all_entrance_methods[n].text for n in range(len(all_entrance_methods))]\n\n        # Induction Identity\n        all_identities = soup.find_all('td', {'data-stat':'category_hof'})\n        identities = [all_identities[n].text for n in range(len(all_identities))]\n\n        # Total Votes For, including the if statement in the list comprehension for players induction via 0-vote processes\n        all_raw_votes = soup.find_all('td', {'data-stat':'votes'})\n        raw_votes = [all_raw_votes[n].text if pd.isna(all_raw_votes[n]) == False else None for n in range(len(all_raw_votes))]\n\n        # Vote Percentage, including the if statement in the list comprehension for players induction via 0-vote processes\n        all_vote_percentages = soup.find_all('td', {'data-stat':'votes_pct'})\n        vote_percentages = [all_vote_percentages[n].text if pd.isna(all_vote_percentages[n]) == False else None for n in range(len(all_vote_percentages))]\n\n        # Put all of our data into a dictionary for easy conversion to a Pandas DataFrame\n        conversion_dict = {'voting_year':years, 'player':players,\n                        'living_status':status, \n                        'voting_body':entrance_methods,\n                        'inducted_as':identities, 'votes':raw_votes,\n                        'vote_percentage':vote_percentages}\n\n        # And finally make the DataFrame\n        hof_df = pd.DataFrame(conversion_dict)\n\n        # Save as csv\n        if self.save_csv:\n            with open('../../data/raw-data/all_hof_inductees_table.csv', 'w') as file:\n                hof_df.to_csv(file, index=False)\n\n        \n    def scrape_hof_voting(self, years=None):\n            \n            # Build the unique urls for each year's webpage\n            try:\n                page_urls = [f'https://www.baseball-reference.com/awards/hof_{year}.shtml' for year in years]\n            except TypeError:\n                raise TypeError(\"Must set the 'years' input of scrape_hof_voting to an iterable object of length at least one\")\n\n            # Create storage for all of our yearly HOF voting tables\n            voting_tables = []\n\n            # Iterate over each page and scrape the table\n            for url in page_urls:\n                # Sleep as to abide by website scraping rules\n                time.sleep(4)\n\n                # Note the given year\n                year = years[page_urls.index(url)]\n                print(year)\n\n                # Gather the soup and filter down \n                req = requests.get(url)\n                soup = bs(req.text, 'html.parser')\n\n                ########## SCRAPE THE BBWA TABLE ##########\n                try: # Early on, there was not voting every year, so we need to skip these 'incorrect' URLs\n                    bbwa_soup = soup.find_all('div', {'id':'div_hof_BBWAA'})[0]\n                except IndexError:\n                    try: # Table named differently in 1946\n                        bbwa_soup = soup.find_all('div', {'id':'div_hof_Nominating_Vote'})[0]\n                    except IndexError:\n                        print(f'No Data for {year}')\n                        pass\n                \n                bbwa_table = {}\n                \n                # Pull each column in the BBWA table and format into a list for later DF creation\n                rank_boxes = bbwa_soup.find_all('th', {\"data-stat\":'ranker'})\n                ranks = [box.text for box in rank_boxes[1:]]\n                bbwa_table['rank'] = ranks\n\n                name_boxes = bbwa_soup.find_all('td', {\"data-stat\":'player'})\n                names = [box.a.text for box in name_boxes]\n                player_page_urls = ['https://www.baseball-reference.com' + box.a['href'] for box in name_boxes]\n                bbwa_table['name'] = names\n                bbwa_table['player_page_url'] = player_page_urls\n\n                # After the first two columns, everything is laid our similarly, so we can scrape in a loop\n                data_stats = ['year_on_ballot', 'votes', 'votes_pct', 'hof_monitor', 'hof_standard', 'experience', 'WAR_career',\n                            'WAR_peak7', 'JAWS', 'JAWS_pos', 'G', \"AB\", \"R\", 'H', 'HR', 'RBI', 'SB', 'BB', 'batting_avg',\n                            'onbase_perc', 'slugging_perc', 'onbase_plus_slugging', 'onbase_plus_slugging_plus', 'W', 'L',\n                            'earned_run_avg', 'earned_run_avg_plus', 'whip', 'G_p', 'GS', 'SV', 'IP', 'IP', 'H_p', 'HR_p',\n                            'BB_p', 'SO_p', \"pos_summary\"]\n                \n                for stat in data_stats:\n                    stat_boxes = bbwa_soup.find_all('td', {\"data-stat\":stat})\n                    stats = [box.text for box in stat_boxes]\n\n                    bbwa_table[stat] = stats\n\n                # Convert the data from the bbwa table into a pandas df, and add a column for the voting year\n                bbwa_df = pd.DataFrame(bbwa_table)\n                bbwa_df['voting_year'] = year\n\n                # Append the table to the voting tables dictionary\n                voting_tables.append(bbwa_df)\n\n            # Combine all yearly voting tables into one dataframe\n            hof_voting_df = pd.concat([df for df in voting_tables])\n\n            # Optionally, save the df to the data folder\n            if self.save_csv:\n                with open('../../data/raw-data/yearly_hof_voting_data.csv', 'w') as file:\n                    hof_voting_df.to_csv(file, index=False)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nThree major challengs arose during the data collection process. These are commented HTML, request limiting, and changes in HTML formatting.\n\nCommented HTML\nWhile all tables are visually present on the Baseball Reference webpage for individual player stats, tables beyond the standard stats are coded as comments in the HTML. Becuase of this, BeautifulSoup fails to read them in with a simple find_all call. To account for this, we build the try/except blocks into the advanced stats scraping that fall back onto searching within the commented HTML blocks for the given tables.\n\n\nRequest limiting\nTo project servers, Baseball Reference limits requests to no more than 20/minute. As such, all Baseball Reference scraping code contains a 4 second sleep between requests. While this is not an issue for HOF scraping, there are ~23k baseball players we scrape stats for, resulting in ~25 hours of downtime. With this limitation present, all player data was scraped on a separate mini PC and final data transfered back to the local machine. To optimize collection speed, one could parallelize the collection process across multiple machines, each scraping data for a different set of players, although this is likely not in good faith to the Baseball Reference Terms of Use.\n\n\nChanges in HTML formatting\nFor ~9% of individual player stats, the HTML of the Baseball Reference webpage was changes in ways that did not allow our constructed scraping class to retrieve the data. As a result, we do not have data for these players, unless they have been inducted into the Hall of Fame, in which case we have basic batting+pitching stats from their Hall of Fame page. If time permits in a future analysis, we can create different ‘branches’ of our stats scraper to account for these HTML differences on an individual by individual basis and complete the full dataset."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nAside from the two challenges, all data collection processes went as expected. Additinally, final data collection aligns with ingoing expectations and prior research standards, as MLB data is static history, with a limited universe of commonly used metrics."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nNow that the all data has been collected, we can move forward and take a deeper dive into learning about the data we have collected, as we clean it and begin exploring trends as they relate to our primary research question! To see these analyses, toggle to the Data Cleaning and Exploratory Data Analyis tabs of the website."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this section, we explore a number of ‘supervised learning’ techniques in an attempt to predict the outcome of BBWAA votes! For some background, supervised learning, unlike unsupervised learning is the branch of machine learning where the algorithms are given the underlying truth about each point (in our case the voting result - elected, expired, etc.). With the target information present for the machine, it ‘learns’ to connect the the patterns in the data with the specific outcomes, hopefully reaching a place where it can make accurate predictions for new, unseen data.\nWithin the realm of supervised learning, there are two major tasks that we will utilize in this section. They are classification and regression. Classification is used when the target variable is categorical in nature1. For example, prediction the outcome of the BBWAA vote would be a classification task, with categories like elected, expired, in limbo, etc. Within classification, there are two common tasks - binary classification and multi-class classification. The example just described would be a multi-class problem, as there are more than two possible outcomes. On the other hand, if we only work to predict election vs. non-election, it becomes a binary classification problem, as there are only two possible classes1.\nThe second process, regression, is used to make predictions of a numeric or continuous variable2. While the outcomes of the the BBWAA voting are categorical, the underlying percentage of votes recieved can be predicted via regression. The process, at a high level, remains similar, with the regression model ‘learning’ the connections between the underlying data and the target values, but instead the output is now continuous2.\n\n\n\n\n\nK-Nearest neighbors is one the more simpler methods out there, taking direct advantage of the fact, which we saw earlier, that points of similar target classes/values tend to exist close to one another3. With this fact, the K-Neighbors algorithm takes in a value for K, and for any prediction returns the average or majority of the K closest other points to it3. For example, if the algorithm is making a prediction for a point with k=3, and the 3 other data points closest to the predicion point are [elected, elected, eliminated], the prediction returned would be elected due to the majority vote. If instead we were utiling K-Neighbors for regression, and the 3 nearest points were [50%, 60%, 70%] (votes received), the prediction would be 60% via the mean.\n\n\n\nDecision trees, like K-Nearest Neighbors, can be used for both regression and classification tasks. To make predictions, a decision tree splits the data a series of times, grouping based on the underlying value of specific features4. Once the data has been split, the final resulting group becomes the prediction. To ‘learn’, the decision tree determines which features and values to split the data on, such that it results in groupings that accurately reflect the true outcomes4.\nRandom forests utilize a multitude of smaller decision trees, return a prediction of the most common result from the smaller trees, or average of the predictions in the regression case, similar to the majority ruling in K-Neighbors5\n\n\n\nLinear Regression is the most popular form of regression, making predictions based on the line (or hyperplane in more than 3 dimensions) learned during training that minimizes the prediction error of all points in the dataset. There are a few methods to find this optimal line, but they each work to find it such that the sum of the squares of the distance between each datapoint and the line is minimized. It it only used for regression tasks rather than classification tasks, and assumes a linear relationship between each of the features and the target variable6.\n\n\n\nWhile technically titled as a regression, Logistic regression is most often used for classification problems7. This is because logistic regression predictions are bound to 0-1, making them interpretable as percentages of ‘success’. Thus, when utilizing logistic regression for binary classification, say of ‘election’, a model output of 0.76 would become a prediction of elected as the probability of success is &gt;50%. Logistic regression can also be utilized for multi-class predictions7."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-is-supervised-learning",
    "href": "technical-details/supervised-learning/main.html#what-is-supervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this section, we explore a number of ‘supervised learning’ techniques in an attempt to predict the outcome of BBWAA votes! For some background, supervised learning, unlike unsupervised learning is the branch of machine learning where the algorithms are given the underlying truth about each point (in our case the voting result - elected, expired, etc.). With the target information present for the machine, it ‘learns’ to connect the the patterns in the data with the specific outcomes, hopefully reaching a place where it can make accurate predictions for new, unseen data.\nWithin the realm of supervised learning, there are two major tasks that we will utilize in this section. They are classification and regression. Classification is used when the target variable is categorical in nature1. For example, prediction the outcome of the BBWAA vote would be a classification task, with categories like elected, expired, in limbo, etc. Within classification, there are two common tasks - binary classification and multi-class classification. The example just described would be a multi-class problem, as there are more than two possible outcomes. On the other hand, if we only work to predict election vs. non-election, it becomes a binary classification problem, as there are only two possible classes1.\nThe second process, regression, is used to make predictions of a numeric or continuous variable2. While the outcomes of the the BBWAA voting are categorical, the underlying percentage of votes recieved can be predicted via regression. The process, at a high level, remains similar, with the regression model ‘learning’ the connections between the underlying data and the target values, but instead the output is now continuous2."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#supervised-methods-used",
    "href": "technical-details/supervised-learning/main.html#supervised-methods-used",
    "title": "Supervised Learning",
    "section": "",
    "text": "K-Nearest neighbors is one the more simpler methods out there, taking direct advantage of the fact, which we saw earlier, that points of similar target classes/values tend to exist close to one another3. With this fact, the K-Neighbors algorithm takes in a value for K, and for any prediction returns the average or majority of the K closest other points to it3. For example, if the algorithm is making a prediction for a point with k=3, and the 3 other data points closest to the predicion point are [elected, elected, eliminated], the prediction returned would be elected due to the majority vote. If instead we were utiling K-Neighbors for regression, and the 3 nearest points were [50%, 60%, 70%] (votes received), the prediction would be 60% via the mean.\n\n\n\nDecision trees, like K-Nearest Neighbors, can be used for both regression and classification tasks. To make predictions, a decision tree splits the data a series of times, grouping based on the underlying value of specific features4. Once the data has been split, the final resulting group becomes the prediction. To ‘learn’, the decision tree determines which features and values to split the data on, such that it results in groupings that accurately reflect the true outcomes4.\nRandom forests utilize a multitude of smaller decision trees, return a prediction of the most common result from the smaller trees, or average of the predictions in the regression case, similar to the majority ruling in K-Neighbors5\n\n\n\nLinear Regression is the most popular form of regression, making predictions based on the line (or hyperplane in more than 3 dimensions) learned during training that minimizes the prediction error of all points in the dataset. There are a few methods to find this optimal line, but they each work to find it such that the sum of the squares of the distance between each datapoint and the line is minimized. It it only used for regression tasks rather than classification tasks, and assumes a linear relationship between each of the features and the target variable6.\n\n\n\nWhile technically titled as a regression, Logistic regression is most often used for classification problems7. This is because logistic regression predictions are bound to 0-1, making them interpretable as percentages of ‘success’. Thus, when utilizing logistic regression for binary classification, say of ‘election’, a model output of 0.76 would become a prediction of elected as the probability of success is &gt;50%. Logistic regression can also be utilized for multi-class predictions7."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification",
    "href": "technical-details/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "Binary Classification",
    "text": "Binary Classification\n\n# Split data into training and testing datasets for binary classification\n# We reserve 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(decomposed_data, binary_targets, test_size=0.2, random_state=5000, stratify=binary_targets)\n\n\nLogistic Regression\n\n# Initialize Logistic Regression Model and fit to the training data\nlogistic_model = LogisticRegression()\nlogistic_model.fit(x_train, y_train)\n\n# Utilize cross validation training to train the logistic model\n# for a baseline score\nbaseline_accuracy = cross_val_score(logistic_model, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Logistic Regression Model is {round(baseline_accuracy, 2)}%')\n\nAccuracy for the baseline Logistic Regression Model is 97.63%\n\n\nThis is a great result! When attempting to predict whether a player will be elected to the Hall of Fame via cross validation, we are correct &gt; 97% of the time!. We will confirm this result later however, on the unseen test data.\n\n\nK-Nearest Neighbors\n\n# Initialize the KNeighbor model and fit to the training data\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the KNeighbors model\n# for a baseline score\nbaseline_accuracy = cross_val_score(knn_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy, 2)}%')\n\nAccuracy for the baseline K-Neighbors Classifier Model is 97.02%\n\n\nWe can potentially build upon this result for K-Neighbors by altering the number of neighbors used in classification. We do this by running the cross validation procedure on a number of different values for neighbor, and use the best one for final training. This process is known as hyperparameter tuning.\n\nknn_classifier = KNeighborsClassifier()\n\n# Define the neighbor values to search\n# We test each value from 5 to 50 in increments of 5\nknn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n\n# Set up the grid search\nknn_grid = GridSearchCV(knn_classifier, knn_grid)\n\n# Fit the grid search, finding the optimal value\nknn_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\nprint(f'Accuracy for the optimized K-Neighbors Classifier Model is {round(knn_grid.best_score_*100, 2)}%')\n\nThe best K-Neighbors model has 5 Neighbors\nAccuracy for the optimized K-Neighbors Classifier Model is 97.02%\n\n\n\n\nDecision Tree\n\n# Initialize the tree and fit to the training data\ntree_classifier = DecisionTreeClassifier()\ntree_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the model for a baseline score\nbaseline_accuracy = cross_val_score(tree_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Decision Tree Classifier Model is {round(baseline_accuracy, 2)}%')\n\nAccuracy for the baseline Decision Tree Classifier Model is 97.02%\n\n\nWe again use a grid search in an attempt to optimize the tree classifier. The hyperparameters include the maximimun number of splits the tree makes, as well as the minimum number of data points that can be included in an individual split.\n\ntree_classifier = DecisionTreeClassifier()\n\n# Define the hyperparameter search grid\ntree_grid ={'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n           'min_samples_split':[2, 3, 4, 5, 6]}\n\n# Set up the grid search\ntree_grid = GridSearchCV(tree_classifier, tree_grid)\n\n# Fit the grid search, finding the optimal value\ntree_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\nprint(f'Accuracy for the optimized Decision Tree Classifier Model is {round(tree_grid.best_score_*100, 2)}%')\n\nThe best Decision Tree model has a max depth of 5 and a minimum of 3 points per split\nAccuracy for the optimized Decision Tree Classifier Model is 97.75%\n\n\n\nRandom Forest\n\n# Initialize the tree and fit to the training data\nforest_classifier = RandomForestClassifier()\nforest_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the model for a baseline score\nbaseline_accuracy = cross_val_score(forest_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Random Forest Classifier Model is {round(baseline_accuracy, 2)}%')\n\nAccuracy for the baseline Random Forest Classifier Model is 97.83%\n\n\nWe next optimize the random forest classifier with the same hyperparameters as the Decision Tree, plus an extra that defines the number of Decision Trees the forest uses.\n\nforest_classifier = RandomForestClassifier()\n\n# Define the hyperparameter search grid\nforest_grid ={'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n           'min_samples_split':[2, 3, 4, 5, 6],\n           'n_estimators':[50, 100, 150]}\n\n# Set up the grid search\nforest_grid = GridSearchCV(forest_classifier, forest_grid)\n\n# Fit the grid search, finding the optimal value\nforest_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\nprint(f'Accuracy for the optimized Decision Tree Classifier Model is {round(forest_grid.best_score_*100, 2)}%')\n\nThe best Random Forest model has a max depth of 15, a minimum of 5 points per split, and 150 trees\nAccuracy for the optimized Decision Tree Classifier Model is 98.07%\n\n\nBecause the optimized Random Forest did not improve results over the baseline, we consider the baseline the best version.\nWith these strong scores on the training set, we move next into making predictions on the test set to see how well our model generalizes to unseen data. Beyond considering models for their accuracy during this stage, we can also rely on two other metrics for classification tasks. These are Precision and Recall. Precision tells us what our accuracy is among positive predictions (if we say elected how often are we right?), while recall tells us among all of our positive predicions, how many of the true positives did we find (what percent of the actual HOFers did we find?)\nFinally, because precision and recall always move in opposite directions as one another, we also introduce the F1 metric. This metric balances both precision and recall, offering a more general view on how the model is performing.\n\n# Test logistic model\nlogistic_preds = logistic_model.predict(x_test)\nlogistic_accuracy = accuracy_score(y_test, logistic_preds)\nlogistic_precision = precision_score(y_test, logistic_preds)\nlogistic_recall = recall_score(y_test, logistic_preds)\nlogistic_f1 = f1_score(y_test, logistic_preds)\n\n# Test KNN model\nknn_test_preds = knn_grid.best_estimator_.predict(x_test)\nknn_accuracy = accuracy_score(y_test, knn_test_preds)\nknn_precision = precision_score(y_test, knn_test_preds)\nknn_recall = recall_score(y_test, knn_test_preds)\nknn_f1 = f1_score(y_test, knn_test_preds)\n\n# Test Decision Tree model\ntree_test_preds = tree_grid.best_estimator_.predict(x_test)\ntree_accuracy = accuracy_score(y_test, tree_test_preds)\ntree_precision = precision_score(y_test, tree_test_preds)\ntree_recall = recall_score(y_test, tree_test_preds)\ntree_f1 = f1_score(y_test, tree_test_preds)\n\n# Test Random Forest model\nforest_test_preds = forest_grid.best_estimator_.predict(x_test)\nforest_accuracy = accuracy_score(y_test, forest_test_preds)\nforest_precision = precision_score(y_test, forest_test_preds)\nforest_recall = recall_score(y_test, forest_test_preds)\nforest_f1 = f1_score(y_test, forest_test_preds)\n\n# Convert scoring to a DataFrame\nresults = {\n    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n    \"Accuracy\": [logistic_accuracy, knn_accuracy, tree_accuracy, forest_accuracy],\n    \"Precision\": [logistic_precision, knn_precision, tree_precision, forest_precision],\n    \"Recall\": [logistic_recall, knn_recall, tree_recall, forest_recall],\n    \"F1 Score\": [logistic_f1, knn_f1, tree_f1, forest_f1]\n}\n\n# Convert the dictionary into a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\nprint('Binary Classification Results by Model\\n')\n\n# Display the DataFrame\nprint(results_df)\n\nBinary Classification Results by Model\n\n                 Model  Accuracy  Precision    Recall  F1 Score\n0  Logistic Regression  0.979100   0.875000  0.368421  0.518519\n1                  KNN  0.971061   0.666667  0.105263  0.181818\n2        Decision Tree  0.975884   1.000000  0.210526  0.347826\n3        Random Forest  0.975884   0.750000  0.315789  0.444444\n\n\nHere an interesting trend emerges. While the accuracy of each model is quite high, the secondary metrics are not as rosy. This is occuring because of the class imbalance in our dataset. With almost 97% of the datapoints being non-elections, we could achieve a 97% accuracy by randomly guessing, or just guessing non-election for every single point. Thus, the precision and recall tell a better story as to the fact that when we guess a succesfull election, we are fairly accurate, but we often fail to predict all of the elections.\nThat said, we do still see significant improvement over a random guesser, which tells us that our model is working to predict elections succesfully. With a random baseline just under 97%, we improve incorrect predictions by ~30% with our logistic model."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multi-class-classification",
    "href": "technical-details/supervised-learning/main.html#multi-class-classification",
    "title": "Supervised Learning",
    "section": "Multi-Class Classification",
    "text": "Multi-Class Classification\nWe do the same exercise as above, this time using the full set of classes [elected, eliminated, expired, limbo]\n\n# Split data into training and testing datasets for binary classification\n# We reserve 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(decomposed_data, multi_targets, test_size=0.2, random_state=5000, stratify=multi_targets)\n\n# Initialize Logistic Regression Model and fit to the training data\nlogistic_model = LogisticRegression()\nlogistic_model.fit(x_train, y_train)\n\n# Utilize cross validation training to train the logistic model\n# for a baseline score\nbaseline_accuracy = cross_val_score(logistic_model, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Logistic Regression Model is {round(baseline_accuracy, 2)}% \\n')\n\n# Initialize the KNeighbor model and fit to the training data\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the KNeighbors model\n# for a baseline score\nbaseline_accuracy = cross_val_score(knn_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy, 2)}%')\n\nknn_classifier = KNeighborsClassifier()\n\n# Define the grid values to search\n# We test each value from 5 to 50 in increments of 5\nknn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n\n# Set up the grid search\nknn_grid = GridSearchCV(knn_classifier, knn_grid)\n\n# Fit the grid search, finding the optimal value\nknn_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\nprint(f'Accuracy for the optimized K-Neighbors Classifier Model is {round(knn_grid.best_score_, 2)}% \\n')\n\n\n# Initialize the tree and fit to the training data\ntree_classifier = DecisionTreeClassifier()\ntree_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the model for a baseline score\nbaseline_accuracy = cross_val_score(tree_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Decision Tree Classifier Model is {round(baseline_accuracy, 2)}%')\n\ntree_classifier = DecisionTreeClassifier()\n\n# Define the hyperparameter search grid\ntree_grid ={'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n           'min_samples_split':[2, 3, 4, 5, 6]}\n\n# Set up the grid search\ntree_grid = GridSearchCV(tree_classifier, tree_grid)\n\n# Fit the grid search, finding the optimal value\ntree_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\nprint(f'Accuracy for the optimized Decision Tree Classifier Model is {round(tree_grid.best_score_, 2)}% \\n')\n\n\n# Initialize the tree and fit to the training data\nforest_classifier = RandomForestClassifier()\nforest_classifier.fit(x_train, y_train)\n\n# Utilize cross validation training to train the model for a baseline score\nbaseline_accuracy = cross_val_score(forest_classifier, x_train, y_train).mean() * 100\n\nprint(f'Accuracy for the baseline Random Forest Classifier Model is {round(baseline_accuracy, 2)}%')\n\nforest_classifier = RandomForestClassifier()\n\n# Define the hyperparameter search grid\nforest_grid ={'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n           'min_samples_split':[2, 3, 4, 5, 6],\n           'n_estimators':[50, 100, 150]}\n\n# Set up the grid search\nforest_grid = GridSearchCV(forest_classifier, forest_grid)\n\n# Fit the grid search, finding the optimal value\nforest_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\nprint(f'Accuracy for the optimized Decision Tree Classifier Model is {round(forest_grid.best_score_, 2)}%')\n\nAccuracy for the baseline Logistic Regression Model is 81.66% \n\nAccuracy for the baseline K-Neighbors Classifier Model is 82.5%\nThe best K-Neighbors model has 5 Neighbors\nAccuracy for the optimized K-Neighbors Classifier Model is 0.83% \n\nAccuracy for the baseline Decision Tree Classifier Model is 82.1%\nThe best Decision Tree model has a max depth of 15 and a minimum of 2 points per split\nAccuracy for the optimized Decision Tree Classifier Model is 0.83% \n\nAccuracy for the baseline Random Forest Classifier Model is 87.05%\nThe best Random Forest model has a max depth of 20, a minimum of 3 points per split, and 50 trees\nAccuracy for the optimized Decision Tree Classifier Model is 0.88%\n\n\n\n# Test logistic model\nlogistic_preds = logistic_model.predict(x_test)\nlogistic_accuracy = accuracy_score(y_test, logistic_preds)\nlogistic_precision = precision_score(y_test, logistic_preds, average='weighted')\nlogistic_recall = recall_score(y_test, logistic_preds, average='weighted')\nlogistic_f1 = f1_score(y_test, logistic_preds, average='weighted')\n\n# Test KNN model\nknn_test_preds = knn_grid.best_estimator_.predict(x_test)\nknn_accuracy = accuracy_score(y_test, knn_test_preds)\nknn_precision = precision_score(y_test, knn_test_preds, average='weighted')\nknn_recall = recall_score(y_test, knn_test_preds, average='weighted')\nknn_f1 = f1_score(y_test, knn_test_preds, average='weighted')\n\n# Test Decision Tree model\ntree_test_preds = tree_grid.best_estimator_.predict(x_test)\ntree_accuracy = accuracy_score(y_test, tree_test_preds)\ntree_precision = precision_score(y_test, tree_test_preds, average='weighted')\ntree_recall = recall_score(y_test, tree_test_preds, average='weighted')\ntree_f1 = f1_score(y_test, tree_test_preds, average='weighted')\n\n# Test Random Forest model\nforest_test_preds = forest_grid.best_estimator_.predict(x_test)\nforest_accuracy = accuracy_score(y_test, forest_test_preds)\nforest_precision = precision_score(y_test, forest_test_preds, average='weighted')\nforest_recall = recall_score(y_test, forest_test_preds, average='weighted')\nforest_f1 = f1_score(y_test, forest_test_preds, average='weighted')\n\n# Convert scoring to a DataFrame\nresults = {\n    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n    \"Accuracy\": [logistic_accuracy, knn_accuracy, tree_accuracy, forest_accuracy],\n    \"Precision\": [logistic_precision, knn_precision, tree_precision, forest_precision],\n    \"Recall\": [logistic_recall, knn_recall, tree_recall, forest_recall],\n    \"F1 Score\": [logistic_f1, knn_f1, tree_f1, forest_f1]\n}\n\n# Convert the dictionary into a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\nprint('Test results for Multi-Class Classification by Model\\n')\n# Display the DataFrame\nprint(results_df)\n\nTest results for Multi-Class Classification by Model\n\n                 Model  Accuracy  Precision    Recall  F1 Score\n0  Logistic Regression  0.795820   0.793197  0.795820  0.794228\n1                  KNN  0.834405   0.819622  0.834405  0.815207\n2        Decision Tree  0.827974   0.819027  0.827974  0.822636\n3        Random Forest  0.860129   0.844698  0.860129  0.847190"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression",
    "href": "technical-details/supervised-learning/main.html#regression",
    "title": "Supervised Learning",
    "section": "Regression",
    "text": "Regression\nFor our last task, we utilize the regression methods to predict the total vote percentage received by the player. For our metric of success, we use the ‘mean squared error’, which is the average of the squared errors for each individual prediction.\n\n# Split data into training and testing datasets for regression\n# We reserve 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(decomposed_data, reg_targets, test_size=0.2, random_state=5000, stratify=binary_targets)\n\n\nLinear Regression\n\n# Initialize Logistic Regression Model and fit to the training data\nlinear_model = LinearRegression()\nlinear_model.fit(x_train, y_train)\n\n# Utilize cross validation training to train the logistic model\n# for a baseline score\nbaseline_accuracy = cross_val_score(linear_model, x_train, y_train, scoring='neg_mean_squared_error').mean()\n\nprint(f'MSE for the baseline Linear Regression Model is {round(baseline_accuracy*-1, 2)}')\n\nMSE for the baseline Linear Regression Model is 165.66\n\n\n\n\nK-Neighbors Regression\n\n# Initialize the KNeighbor model and fit to the training data\nknn_regressor = KNeighborsRegressor()\n\n\n# Utilize cross validation training to train the KNeighbors model\n# for a baseline score\nbaseline_accuracy = cross_val_score(knn_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n\nprint(f'MSE for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy*-1, 2)}\\n')\n\nknn_regressor = KNeighborsRegressor()\nknn_regressor.fit(x_train, y_train)\n\n# Define the neighbor values to search\n# We test each value from 5 to 50 in increments of 5\nknn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n\n# Set up the grid search\nknn_grid = GridSearchCV(knn_regressor, knn_grid, scoring='neg_mean_squared_error')\n\n# Fit the grid search, finding the optimal value\nknn_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\nprint(f'MSE for the optimized K-Neighbors Regressor Model is {round(knn_grid.best_score_*-1, 2)}')\n\nMSE for the baseline K-Neighbors Classifier Model is 153.64\n\nThe best K-Neighbors model has 5 Neighbors\nMSE for the optimized K-Neighbors Regressor Model is 153.64\n\n\n\n# Initialize the model and fit to the training data\ntree_regressor = DecisionTreeRegressor()\n\n# Utilize cross validation training to train the model\n# for a baseline score\nbaseline_accuracy = cross_val_score(tree_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n\nprint(f'MSE for the baseline Decision Tree Regressor Model is {round(baseline_accuracy*-1, 2)}\\n')\n\ntree_regressor = DecisionTreeRegressor()\ntree_regressor.fit(x_train, y_train)\n\n# Define the grid values to search\n# We test each value from 5 to 50 in increments of 5\ntree_grid = {'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n            'min_samples_split':[2, 3, 4, 5, 6]}\n\n# Set up the grid search\ntree_grid = GridSearchCV(tree_regressor, tree_grid, scoring='neg_mean_squared_error')\n\n# Fit the grid search, finding the optimal value\ntree_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The optimized Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\nprint(f'MSE for the optimized Decision Tree Regressor Model is {round(tree_grid.best_score_*-1, 2)}')\n\nMSE for the baseline Decision Tree Regressor Model is 168.81\n\nThe optimized Decision Tree model has a max depth of None and a minimum of 5 points per split\nMSE for the optimized Decision Tree Regressor Model is 162.21\n\n\n\n\nRandom Forest Regressor\n\n# Initialize the model and fit to the training data\nforest_regressor = RandomForestRegressor()\nforest_regressor.fit(x_train, y_train)\n\n# Utilize cross validation training to train the model\n# for a baseline score\nbaseline_accuracy = cross_val_score(forest_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n\nprint(f'MSE for the baseline Random Forest Regressor Model is {round(baseline_accuracy*-1, 2)}\\n')\n\ntree_regressor = RandomForestRegressor()\n\n# Define the grid values to search\nforest_grid = {'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n               'min_samples_split':[4,5,6], \n               'n_estimators':[100, 150]}\n\n# Set up the grid search\nforest_grid = GridSearchCV(forest_regressor, forest_grid, scoring='neg_mean_squared_error')\n\n# Fit the grid search, finding the optimal value\nforest_grid.fit(x_train, y_train)\n\n# Print the best model and its score\nprint(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\nprint(f'MSE for the optimized Decision Tree Regressor Model is {round(forest_grid.best_score_*-1, 2)}')\n\nMSE for the baseline Random Forest Regressor Model is 98.09\n\nThe best Random Forest model has a max depth of None, a minimum of 5 points per split, and 150 trees\nMSE for the optimized Decision Tree Regressor Model is 95.97\n\n\n\n# Test linear model\nlinear_preds = linear_model.predict(x_test)\nlinear_mse = mean_squared_error(y_test, linear_preds)\n\n# Test Nearest Neighbors model\nknn_preds = knn_regressor.predict(x_test)\nknn_mse = mean_squared_error(y_test, knn_preds)\n\n# Test linear model\ntree_preds = tree_grid.best_estimator_.predict(x_test)\ntree_mse = mean_squared_error(y_test, tree_preds)\n\n# Test linear model\nforest_preds = forest_grid.best_estimator_.predict(x_test)\nforest_mse = mean_squared_error(y_test, forest_preds)\n\n\n# Convert scoring to a DataFrame\nresults = {\n    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n    \"MSE\": [linear_mse, knn_mse, tree_mse, forest_mse],\n}\n\n# Convert the dictionary into a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\nprint('Test results for Regression by Model\\n')\n# Display the DataFrame\nprint(results_df)\n\nTest results for Regression by Model\n\n                 Model         MSE\n0  Logistic Regression  137.182293\n1                  KNN   94.947084\n2        Decision Tree   87.421412\n3        Random Forest   65.643072\n\n\nOnce again, we see pretty strong results! With our random forest model we are able to predict voting outcomes with a MSE of ~65. Although we do see this is quite a large jump in accuracy from the training data, and may be due to randomness in the data. Even with this caveat however, other models also offer and MSE in the range of 85-95. We should however remember that the mean voting percentage is ~13%, given the multitude of players who do not make the HOF even once on the ballot. That said, if we use the MSE for just an average guesser that predicts ~13% each time, the MSE would be &gt;400, so we do certainly see an improvement over this value!\nThis concludes the section on supervised learning, where we saw how it is possible to increase prediction accuracy of the BBWAA HOF ballot outcomes and voting percentage by utilizing an array of both classification and regression methods. For a more detailed report of the project as a whole, make sure to check out the report section!"
  }
]