{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlayerScraper():\n",
    "    def __init__(self, save_csv=True):\n",
    "        self.save_csv = save_csv\n",
    "\n",
    "    def scrape_player_info(self, leading_letters = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "                                         'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                                         'U', 'V', 'W', 'X', 'Y', 'Z')):\n",
    "\n",
    "        # Create storage\n",
    "        names = []\n",
    "        suffixes = []\n",
    "        ids = []\n",
    "\n",
    "        for letter in leading_letters:\n",
    "            # Sleep to remain under rate limits\n",
    "            time.sleep(4)\n",
    "\n",
    "            # Create the url with the data for all players with a last name starting with the letter\n",
    "            letter_url = f'https://www.baseball-reference.com/players/{letter.lower()}/'\n",
    "            \n",
    "            # Request the url that contains the data for all players belonging to the letter\n",
    "            req = requests.get(letter_url)\n",
    "            soup = bs(req.text, 'html.parser')\n",
    "\n",
    "            player_soup = soup.find_all('div', {'id':'div_players_'})[0]\n",
    "\n",
    "            # Grab all names\n",
    "            player_names = [name.a.text for name in player_soup.find_all('p')]\n",
    "            names = names + player_names\n",
    "\n",
    "            # Grab all URL suffixes\n",
    "            url_suffixes = [name.a['href'] for name in player_soup.find_all('p')]\n",
    "            suffixes = suffixes + url_suffixes\n",
    "\n",
    "            # Grab IDs\n",
    "            player_ids = [name.a['href'].split('/')[-1].split('.sh')[0] for name in player_soup.find_all('p')]\n",
    "            ids = ids + player_ids\n",
    "\n",
    "        # Combine everything into a dataframe\n",
    "        player_df = pd.DataFrame({'name':names, 'id':ids, 'url_suffix':suffixes})\n",
    "\n",
    "        if self.save_csv:\n",
    "            with open('../../data/raw-data/all_player_info.csv', 'w') as file:\n",
    "                player_df.to_csv(file, index=False)\n",
    "\n",
    "    def _scrape_career_batting_or_pitching_stats_from_soup(self, position: str, soup: bs):\n",
    "        if position.lower() not in ['pitching', 'batting', 'fielding']:\n",
    "            raise ValueError(f\"Position {position} is invalid: Must be one of 'batting' or 'pitching' or 'fielding'\")\n",
    "\n",
    "        is_position = True if soup.find_all('div', {'id':f'all_players_standard_{position}'}) else False\n",
    "\n",
    "        if is_position:\n",
    "            try: # Pull the career standard stats\n",
    "                standard_footer = soup.find_all('div', {'id':f'all_players_standard_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_standard_{position}.')})\n",
    "            except (IndexError, AttributeError) as e: # If the table is commented out\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                for comment in comments:\n",
    "                    if f'players_standard_{position}' in comment:\n",
    "                        table_html = bs(comment, 'html.parser')\n",
    "                        standard_footer = table_html.find_all('table', {'id':f'players_standard_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_standard_{position}.')})\n",
    "                        break\n",
    "            stat_names = [cell['data-stat'] for cell in standard_footer.find_all('td')]\n",
    "            stat_values = [cell.text for cell in standard_footer.find_all('td')]\n",
    "            career_standard_stats = pd.DataFrame([stat_values], columns=stat_names)\n",
    "\n",
    "            if position.lower() != 'fielding':\n",
    "                try: # Pull the career advanced stats\n",
    "                    advanced_footer = soup.find_all('div', {'id':f'all_players_advanced_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_advanced_{position}.')})\n",
    "                except (IndexError, AttributeError) as e: # If the table is commented out\n",
    "                    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                    for comment in comments:\n",
    "                        if f'players_advanced_{position}' in comment:\n",
    "                            table_html = bs(comment, 'html.parser')\n",
    "                            advanced_footer = table_html.find_all('table', {'id':f'players_advanced_{position}'})[0].find('tfoot').find('tr', {'id':re.compile(f'players_advanced_{position}.')})\n",
    "                            break\n",
    "                \n",
    "                stat_names = [cell['data-stat'] for cell in advanced_footer.find_all('td')]\n",
    "                stat_values = [cell.text for cell in advanced_footer.find_all('td')]\n",
    "                career_advanced_stats = pd.DataFrame([stat_values], columns=stat_names)\n",
    "\n",
    "                total_career_stats = pd.concat([career_standard_stats, career_advanced_stats], axis=1)\n",
    "\n",
    "                return total_career_stats\n",
    "            \n",
    "            else:\n",
    "                return career_standard_stats\n",
    "        \n",
    "        else: # If no data for the position\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _scrape_annual_batting_or_pitching_stats_from_soup(self, position, level, soup):\n",
    "\n",
    "        if position.lower() not in ['pitching', 'batting', 'fielding']:\n",
    "            raise ValueError(f\"Position {position} is invalid: Must be one of 'batting' or 'pitching' or 'fielding\")\n",
    "        \n",
    "        is_position = True if soup.find_all('div', {'id':f'all_players_standard_{position}'}) else False\n",
    "\n",
    "        if is_position:\n",
    "            try:\n",
    "                rows = soup.find_all('tr', {'id':re.compile(f'players_{level}_{position}.')})\n",
    "                existince_checker = rows[0]\n",
    "            except (IndexError, AttributeError) as e: # If the table is commented out\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                for comment in comments:\n",
    "                    if f'players_{level}_{position}' in comment:\n",
    "                        table_html = bs(comment, 'html.parser')\n",
    "                        rows = table_html.find_all('table', {'id':f'players_{level}_{position}'})[0].find('tbody').find_all('tr', {'id':re.compile(f'players_{level}_{position}.')})\n",
    "                        break\n",
    "\n",
    "            headers = [cell['data-stat'] for cell in rows[0].find_all('td')]\n",
    "\n",
    "            stats_list = []\n",
    "            for row in rows:\n",
    "                stats = [cell.text for cell in row.find_all('td')]\n",
    "                stats_list.append(stats)\n",
    "            stats = pd.DataFrame(stats_list, columns=headers)\n",
    "            stats = stats[stats[stats.columns[-2]] != None]\n",
    "\n",
    "            return stats\n",
    "\n",
    "    def _scrape_acomplishments_from_soup(self, soup):\n",
    "        try:\n",
    "            accomplishments_soup = soup.find_all('ul',{'id':'bling'})[0]\n",
    "            accomplishments = [accomplishment.text for accomplishment in accomplishments_soup.find_all('li')]\n",
    "            accomplishments = ', '.join(accomplishments)\n",
    "            return accomplishments\n",
    "        except (AttributeError, IndexError) as e:\n",
    "            return ''\n",
    "\n",
    "    def _scrape_position_appearances_from_soup(self, soup):\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        for comment in comments:\n",
    "            if f'div_appearances' in comment:\n",
    "                table_html = bs(comment, 'html.parser')\n",
    "                footer_row = table_html.find('tfoot')\n",
    "                headers = [cell['data-stat'] for cell in footer_row.find_all('td')]\n",
    "                stats = [cell.text for cell in footer_row.find_all('td')]\n",
    "                break\n",
    "        app_df = pd.DataFrame([stats], columns=headers)\n",
    "        return app_df\n",
    "\n",
    "    def scrape_player_stats(self, player_suffixs = (), cache_path = ''):\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as fpath:\n",
    "                storage_dict = pkl.load(fpath)\n",
    "        except FileNotFoundError:\n",
    "            storage_dict = {}\n",
    "\n",
    "        save_counter = 0\n",
    "        for suffix in player_suffixs:\n",
    "            insert = suffix.split('/')[-1].split('.')[0]\n",
    "            if insert in storage_dict:\n",
    "                continue\n",
    "            \n",
    "            # Wrap everything in a try/except to catch anything unforseen, while still appending to storage dict, so we can go back later if needed\n",
    "            try:\n",
    "                # Create the url with the data for all players with a last name starting with the letter\n",
    "                player_url = f'https://www.baseball-reference.com{suffix}'\n",
    "                \n",
    "                # Request the url for the players Baseball Reference page\n",
    "                req = requests.get(player_url)\n",
    "                soup = bs(req.text, 'html.parser')\n",
    "\n",
    "                # Scrape the career batting pitching, and fielding stats\n",
    "                career_batting_stats = self._scrape_career_batting_or_pitching_stats_from_soup('batting', soup)\n",
    "                career_pitching_stats = self._scrape_career_batting_or_pitching_stats_from_soup('pitching', soup)\n",
    "                #career_fielding_stats = self._scrape_career_batting_or_pitching_stats_from_soup('fielding', soup)\n",
    "                \n",
    "                career_stats = pd.concat([career_batting_stats, career_pitching_stats], axis=1)\n",
    "\n",
    "                # Add awards/accomplishments to the career stats\n",
    "                accomplishment_list = self._scrape_acomplishments_from_soup(soup)\n",
    "                career_stats['accomplishments'] = accomplishment_list\n",
    "\n",
    "                ##### ANNUAL STATS #####\n",
    "\n",
    "                # Scrape the annual batting, pitching, and fielding stats, combining the standard and advanced for each\n",
    "                annual_standard_batting_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('batting', 'standard', soup)\n",
    "                annual_advanced_batting_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('batting', 'advanced', soup)\n",
    "                \n",
    "                # Check if the batting stats existed, and if so, merge them into one df\n",
    "                if isinstance(annual_standard_batting_stats, pd.DataFrame):\n",
    "                    annual_batting_stats = pd.concat([annual_standard_batting_stats, annual_advanced_batting_stats], axis=1)\n",
    "                    annual_batting_stats = annual_batting_stats.loc[:, ~annual_batting_stats.columns.duplicated()]\n",
    "                else:\n",
    "                    annual_batting_stats = pd.DataFrame()\n",
    "                # Get the index of the career column, and get rid of it and anything below (postseason)\n",
    "                i = annual_batting_stats[annual_batting_stats.age.str.contains(f'\\.') == True].index[0] if isinstance(annual_batting_stats, pd.DataFrame) and not annual_batting_stats.empty and len(annual_batting_stats[annual_batting_stats.age.str.contains(f'\\.') == True]) > 0 else len(annual_batting_stats.index)\n",
    "                annual_batting_stats = annual_batting_stats.iloc[:i]\n",
    "\n",
    "                annual_standard_pitching_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('pitching', 'standard', soup)\n",
    "                annual_advanced_pitching_stats = self._scrape_annual_batting_or_pitching_stats_from_soup('pitching', 'advanced', soup)\n",
    "\n",
    "                # Check if the pitching stats existed, and if so, merge them into one df\n",
    "                if isinstance(annual_standard_pitching_stats, pd.DataFrame):\n",
    "                    annual_pitching_stats = pd.concat([annual_standard_pitching_stats, annual_advanced_pitching_stats], axis=1)\n",
    "                    annual_pitching_stats = annual_pitching_stats.loc[:, ~annual_pitching_stats.columns.duplicated()]\n",
    "                else:\n",
    "                    annual_pitching_stats = pd.DataFrame()\n",
    "                # Get the index of the career column, and get rid of it and anything below (postseason)\n",
    "                i = annual_pitching_stats[annual_pitching_stats.age.str.contains(f'\\.') == True].index[0] if isinstance(annual_pitching_stats, pd.DataFrame) and not annual_pitching_stats.empty and len(annual_pitching_stats[annual_pitching_stats.age.str.contains(f'\\.') == True]) > 0 else len(annual_pitching_stats)\n",
    "                annual_pitching_stats = annual_pitching_stats.iloc[:i]\n",
    "\n",
    "                # Merge any annual DataFrames that actually exist. Then drop any duplicated columns\n",
    "                real_dfs = [df for df in [annual_batting_stats, annual_pitching_stats] if not df.empty]\n",
    "                annual_stats = real_dfs[0]\n",
    "                for df in real_dfs[1:]:\n",
    "                    annual_stats = pd.merge(annual_stats, df, on=['age', 'team_name_abbr'], how='outer')\n",
    "                annual_stats = annual_stats.loc[:, ~annual_stats.columns.duplicated()]\n",
    "\n",
    "                # Drop rows that don't belong on known conditions\n",
    "                annual_stats = annual_stats.dropna(subset=['age'])\n",
    "\n",
    "                # Scrape appearences\n",
    "                appearances = self._scrape_position_appearances_from_soup(soup)\n",
    "                \n",
    "                # Save everything to our storage\n",
    "                storage_dict[insert] = {'career_stats':career_stats, 'annual_stats':annual_stats, 'appearances':appearances}\n",
    "\n",
    "            except:\n",
    "                storage_dict[insert] = 'FAILED TO PULL DATA'\n",
    "\n",
    "            save_counter += 1\n",
    "            if save_counter % 5 == 0:\n",
    "                print(insert)\n",
    "                with open('../../data/raw-data/all_player_stats.pkl', 'wb') as fpath:\n",
    "                    pkl.dump(storage_dict, fpath)\n",
    "\n",
    "            time.sleep(4)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And the HOFScraper Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "class HOFScraper():\n",
    "\n",
    "    def __init__(self, save_csv=True):\n",
    "        self.save_csv = save_csv\n",
    "\n",
    "    def scrape_hof_inductees(self):\n",
    "        '''Function to scrape baseball references Hall of Fame webpage, and collect information on every inducted member into the HOF.\n",
    "           Optionally, saves the data as a csv.'''\n",
    "\n",
    "        # Pull table for all players inducted to the HOF\n",
    "        hof_url = 'https://www.baseball-reference.com/awards/hof.shtml'\n",
    "        req = requests.get(hof_url)\n",
    "        soup = bs(req.text, 'html.parser')\n",
    "\n",
    "        ### Build out the dataframe, column by column ###\n",
    "\n",
    "        # Year\n",
    "        all_years = soup.find_all('th', {'data-stat':'year_ID'})\n",
    "        years = [int(all_years[n].a.text) for n in range(1, len(all_years))]\n",
    "\n",
    "        # Player Name\n",
    "        all_players = soup.find_all('td', {'data-stat':'player'})\n",
    "        players = [all_players[n].a.text for n in range(len(all_players))]\n",
    "\n",
    "        # Living Status\n",
    "        all_status = soup.find_all('td', {'data-stat':'lifespan'})\n",
    "        status = [all_status[n].text for n in range(len(all_status))]\n",
    "\n",
    "        # Entrance Method\n",
    "        all_entrance_methods = soup.find_all('td', {'data-stat':'votedBy'})\n",
    "        entrance_methods = [all_entrance_methods[n].text for n in range(len(all_entrance_methods))]\n",
    "\n",
    "        # Induction Identity\n",
    "        all_identities = soup.find_all('td', {'data-stat':'category_hof'})\n",
    "        identities = [all_identities[n].text for n in range(len(all_identities))]\n",
    "\n",
    "        # Total Votes For, including the if statement in the list comprehension for players induction via 0-vote processes\n",
    "        all_raw_votes = soup.find_all('td', {'data-stat':'votes'})\n",
    "        raw_votes = [all_raw_votes[n].text if pd.isna(all_raw_votes[n]) == False else None for n in range(len(all_raw_votes))]\n",
    "\n",
    "        # Vote Percentage, including the if statement in the list comprehension for players induction via 0-vote processes\n",
    "        all_vote_percentages = soup.find_all('td', {'data-stat':'votes_pct'})\n",
    "        vote_percentages = [all_vote_percentages[n].text if pd.isna(all_vote_percentages[n]) == False else None for n in range(len(all_vote_percentages))]\n",
    "\n",
    "        # Put all of our data into a dictionary for easy conversion to a Pandas DataFrame\n",
    "        conversion_dict = {'voting_year':years, 'player':players,\n",
    "                        'living_status':status, \n",
    "                        'voting_body':entrance_methods,\n",
    "                        'inducted_as':identities, 'votes':raw_votes,\n",
    "                        'vote_percentage':vote_percentages}\n",
    "\n",
    "        # And finally make the DataFrame\n",
    "        hof_df = pd.DataFrame(conversion_dict)\n",
    "\n",
    "        # Save as csv\n",
    "        if self.save_csv:\n",
    "            with open('../../data/raw-data/all_hof_inductees_table.csv', 'w') as file:\n",
    "                hof_df.to_csv(file, index=False)\n",
    "\n",
    "        \n",
    "    def scrape_hof_voting(self, years=None):\n",
    "            \n",
    "            # Build the unique urls for each year's webpage\n",
    "            try:\n",
    "                page_urls = [f'https://www.baseball-reference.com/awards/hof_{year}.shtml' for year in years]\n",
    "            except TypeError:\n",
    "                raise TypeError(\"Must set the 'years' input of scrape_hof_voting to an iterable object of length at least one\")\n",
    "\n",
    "            # Create storage for all of our yearly HOF voting tables\n",
    "            voting_tables = []\n",
    "\n",
    "            # Iterate over each page and scrape the table\n",
    "            for url in page_urls:\n",
    "                # Sleep as to abide by website scraping rules\n",
    "                time.sleep(4)\n",
    "\n",
    "                # Note the given year\n",
    "                year = years[page_urls.index(url)]\n",
    "                print(year)\n",
    "\n",
    "                # Gather the soup and filter down \n",
    "                req = requests.get(url)\n",
    "                soup = bs(req.text, 'html.parser')\n",
    "\n",
    "                ########## SCRAPE THE BBWA TABLE ##########\n",
    "                try: # Early on, there was not voting every year, so we need to skip these 'incorrect' URLs\n",
    "                    bbwa_soup = soup.find_all('div', {'id':'div_hof_BBWAA'})[0]\n",
    "                except IndexError:\n",
    "                    try: # Table named differently in 1946\n",
    "                        bbwa_soup = soup.find_all('div', {'id':'div_hof_Nominating_Vote'})[0]\n",
    "                    except IndexError:\n",
    "                        print(f'No Data for {year}')\n",
    "                        pass\n",
    "                \n",
    "                bbwa_table = {}\n",
    "                \n",
    "                # Pull each column in the BBWA table and format into a list for later DF creation\n",
    "                rank_boxes = bbwa_soup.find_all('th', {\"data-stat\":'ranker'})\n",
    "                ranks = [box.text for box in rank_boxes[1:]]\n",
    "                bbwa_table['rank'] = ranks\n",
    "\n",
    "                name_boxes = bbwa_soup.find_all('td', {\"data-stat\":'player'})\n",
    "                names = [box.a.text for box in name_boxes]\n",
    "                player_page_urls = ['https://www.baseball-reference.com' + box.a['href'] for box in name_boxes]\n",
    "                bbwa_table['name'] = names\n",
    "                bbwa_table['player_page_url'] = player_page_urls\n",
    "\n",
    "                # After the first two columns, everything is laid our similarly, so we can scrape in a loop\n",
    "                data_stats = ['year_on_ballot', 'votes', 'votes_pct', 'hof_monitor', 'hof_standard', 'experience', 'WAR_career',\n",
    "                            'WAR_peak7', 'JAWS', 'JAWS_pos', 'G', \"AB\", \"R\", 'H', 'HR', 'RBI', 'SB', 'BB', 'batting_avg',\n",
    "                            'onbase_perc', 'slugging_perc', 'onbase_plus_slugging', 'onbase_plus_slugging_plus', 'W', 'L',\n",
    "                            'earned_run_avg', 'earned_run_avg_plus', 'whip', 'G_p', 'GS', 'SV', 'IP', 'IP', 'H_p', 'HR_p',\n",
    "                            'BB_p', 'SO_p', \"pos_summary\"]\n",
    "                \n",
    "                for stat in data_stats:\n",
    "                    stat_boxes = bbwa_soup.find_all('td', {\"data-stat\":stat})\n",
    "                    stats = [box.text for box in stat_boxes]\n",
    "\n",
    "                    bbwa_table[stat] = stats\n",
    "\n",
    "                # Convert the data from the bbwa table into a pandas df, and add a column for the voting year\n",
    "                bbwa_df = pd.DataFrame(bbwa_table)\n",
    "                bbwa_df['voting_year'] = year\n",
    "\n",
    "                # Append the table to the voting tables dictionary\n",
    "                voting_tables.append(bbwa_df)\n",
    "\n",
    "            # Combine all yearly voting tables into one dataframe\n",
    "            hof_voting_df = pd.concat([df for df in voting_tables])\n",
    "\n",
    "            # Optionally, save the df to the data folder\n",
    "            if self.save_csv:\n",
    "                with open('../../data/raw-data/yearly_hof_voting_data.csv', 'w') as file:\n",
    "                    hof_voting_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import re\n",
    "\n",
    "# Create an instance of our HOFScraper class\n",
    "hof_scraper = HOFScraper()\n",
    "\n",
    "# Use the HOFScraper instance to scrape and save all players inducted into the Hall of Fame\n",
    "hof_scraper.scrape_hof_inductees()\n",
    "\n",
    "# Next use it to scrape and save all Hall of Fame voting data for all available years\n",
    "hof_scraper.scrape_hof_voting(years=[year for year in range(1936, 2025)])\n",
    "\n",
    "# Create an instance of our PlayerScraper class\n",
    "player_scraper = PlayerScraper()\n",
    "\n",
    "# Use the PlayerScraper instance to scrape background information and ID on every MLB player\n",
    "player_scraper.scrape_player_info()\n",
    "\n",
    "# And finally, use the instance to scrape annual and career stats, alongside career awards for every MLB player previously scraped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
