# Introduction

## What is Supervised Learning?

In this section, we explore a number of 'supervised learning' techniques in an attempt to predict the outcome of BBWAA votes! For some background, supervised learning, unlike unsupervised learning is the branch of machine learning where the algorithms *are*   given the underlying truth about each point (in our case the voting result - elected, expired, etc.). With the target information present for the machine, it 'learns' to connect the the patterns in the data with the specific outcomes, hopefully reaching a place where it can make accurate predictions for new, unseen data.

Within the realm of supervised learning, there are two major tasks that we will utilize in this section. They are **classification** and **regression**. Classification is used when the target variable is categorical in nature[@ibm_classification_nodate]. For example, prediction the outcome of the BBWAA vote would be a classification task, with categories like elected, expired, in limbo, etc. Within classification, there are two common tasks - binary classification and multi-class classification. The example just described would be a multi-class problem, as there are more than two possible outcomes. On the other hand, if we only work to predict election vs. non-election, it becomes a binary classification problem, as there are only two possible classes[@ibm_classification_nodate].

The second process, regression, is used to make predictions of a numeric or continuous variable[@geeksforgeeks_regression_nodate]. While the outcomes of the the BBWAA voting are categorical, the underlying percentage of votes recieved can be predicted via regression. The process, at a high level, remains similar, with the regression model 'learning' the connections between the underlying data and the target values, but instead the output is now continuous[@geeksforgeeks_regression_nodate].


## Supervised Methods Used
### K-Nearest Neighbors

K-Nearest neighbors is one the more simpler methods out there, taking direct advantage of the fact, which we saw earlier, that points of similar target classes/values tend to exist close to one another[@ibm_knn_nodate]. With this fact, the K-Neighbors algorithm takes in a value for K, and for any prediction **returns the average or majority of the K closest other points to it**[@ibm_knn_nodate]. For example, if the algorithm is making a prediction for a point with k=3, and the 3 *other* data points closest to the predicion point are [elected, elected, eliminated], the prediction returned would be elected due to the majority vote. If instead we were utiling K-Neighbors for regression, and the 3 nearest points were [50%, 60%, 70%] (votes received), the prediction would be 60% via the mean.

### Decision Tree/Random Forest
Decision trees, like K-Nearest Neighbors, can be used for both regression and classification tasks. To make predictions, a decision tree splits the data a series of times, grouping based on the underlying value of specific features[@ibm_decision_nodate]. Once the data has been split, the final resulting group becomes the prediction. To 'learn', the decision tree determines which features and values to split the data on, such that it results in groupings that accurately reflect the true outcomes[@ibm_decision_nodate].

Random forests utilize a multitude of smaller decision trees, return a prediction of the most common result from the smaller trees, or average of the predictions in the regression case, similar to the majority ruling in K-Neighbors [@ibm_random_nodate]


### Linear Regression

Linear Regression is the most popular form of regression, making predictions based on the line (or hyperplane in more than 3 dimensions) learned during training that minimizes the prediction error of all points in the dataset. There are a few methods to find this optimal line, but they each work to find it such that the sum of the squares of the distance between each datapoint and the line is minimized. It it only used for regression tasks rather than classification tasks, and assumes a linear relationship between each of the features and the target variable[@ibm_linear_nodate].

### Logistic Regression

While technically titled as a regression, Logistic regression is most often used for classification problems[@ibm_logistic_nodate]. This is because logistic regression predictions are bound to 0-1, making them interpretable as percentages of 'success'. Thus, when utilizing logistic regression for binary classification, say of 'election', a model output of 0.76 would become a prediction of elected as the probability of success is >50%. Logistic regression can also be utilized for multi-class predictions[@ibm_logistic_nodate].
