{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Supervised Learning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include summary.qmd >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BBWAA voting data for batters\n",
    "batter_df = pd.read_csv('../../data/processed-data/batter_df_for_prediction.csv')\n",
    "\n",
    "# Create the targets for multi-class classification, binary classification, and regression\n",
    "multi_targets = batter_df.outcome\n",
    "binary_targets = multi_targets == 'elected'\n",
    "reg_targets = batter_df.votes_pct\n",
    "\n",
    "# Drop the informational columns that we dont use to predict (like name)\n",
    "# Plus the targets columns and the scandal column (which is all 0s)\n",
    "batter_df = batter_df.drop(columns=['name', 'player_id', 'votes_pct', 'outcome', 'position', 'scandal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, we are making predictions on the follow columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['voting_year', 'year_on_ballot', 'ly_votes_pct', 'b_war', 'b_h', 'b_hr',\n",
       "       'b_sb', 'b_bb', 'b_so', 'b_batting_avg', 'b_onbase_plus_slugging_plus',\n",
       "       'b_home_run_perc', 'b_strikeout_perc', 'b_base_on_balls_perc',\n",
       "       'b_cwpa_bat', 'b_baseout_runs', 'mvps', 'gold_gloves', 'batting_titles',\n",
       "       'all_stars', 'G_p_app', 'G_c', 'G_1b', 'G_2b', 'G_3b', 'G_ss',\n",
       "       'G_lf_app', 'G_cf_app', 'G_rf_app', 'G_dh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make predictions, we process our data once more with the methods described in the unsupervised learning section. This includes reducing dimensionality with PCA, and scaling all data with a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The processed dataframe now had the shape (3108, 20)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data with PCA and a standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(batter_df)\n",
    "\n",
    "# We keep 95% of the variance in our PCA decomposition\n",
    "pca = PCA(n_components=.95)\n",
    "decomposed_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(f'The processed dataframe now had the shape {decomposed_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step before training models and making predictions, we split our data into two categories: training data and testing data. We do this so that we can ensure our models adapt well to unseen data in the future. After splitting our data into these two groups, we will train the models on the training data, before feeding it the 'unseen' testing data to make predictions on. These blind test results will give us the best understanding of how our model adapts to data in the future. While splitting the data, we also ensure that the proportion of succesfull elections is similar across the two splits, as the majority of our dataset is non-elections.\n",
    "\n",
    "Within this training data, we also undertake one further step that allows us to tweak our model in ways that optimize it. When training each model, we utilize a technique called cross-validation. This process splits the training data into a predefined number (k) sets, before training the model on all but each set k times, and validating the score on each 'unseen' final set. This allowed to train each model with different inputs (like the number of neighbors), without breaking into our final testing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing datasets for binary classification\n",
    "# We reserve 20% of the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(decomposed_data, binary_targets, test_size=0.2, random_state=5000, stratify=binary_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the baseline Logistic Regression Model is 97.63%\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression Model and fit to the training data\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the logistic model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(logistic_model, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Logistic Regression Model is {round(baseline_accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a great result! When attempting to predict whether a player will be elected to the Hall of Fame via cross validation, we are correct > 97% of the time!**. We will confirm this result later however, on the unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the baseline K-Neighbors Classifier Model is 97.02%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNeighbor model and fit to the training data\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the KNeighbors model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(knn_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy, 2)}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can potentially build upon this result for K-Neighbors by altering the number of neighbors used in classification. We do this by running the cross validation procedure on a number of different values for neighbor, and use the best one for final training. This process is known as hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K-Neighbors model has 5 Neighbors\n",
      "Accuracy for the optimized K-Neighbors Classifier Model is 97.02%\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Define the neighbor values to search\n",
    "# We test each value from 5 to 50 in increments of 5\n",
    "knn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n",
    "\n",
    "# Set up the grid search\n",
    "knn_grid = GridSearchCV(knn_classifier, knn_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "knn_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\n",
    "print(f'Accuracy for the optimized K-Neighbors Classifier Model is {round(knn_grid.best_score_*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the baseline Decision Tree Classifier Model is 97.02%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tree and fit to the training data\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the model for a baseline score\n",
    "baseline_accuracy = cross_val_score(tree_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Decision Tree Classifier Model is {round(baseline_accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use a grid search in an attempt to optimize the tree classifier. The hyperparameters include the maximimun number of splits the tree makes, as well as the minimum number of data points that can be included in an individual split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Decision Tree model has a max depth of 5 and a minimum of 3 points per split\n",
      "Accuracy for the optimized Decision Tree Classifier Model is 97.75%\n"
     ]
    }
   ],
   "source": [
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter search grid\n",
    "tree_grid ={'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n",
    "           'min_samples_split':[2, 3, 4, 5, 6]}\n",
    "\n",
    "# Set up the grid search\n",
    "tree_grid = GridSearchCV(tree_classifier, tree_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "tree_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\n",
    "print(f'Accuracy for the optimized Decision Tree Classifier Model is {round(tree_grid.best_score_*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the baseline Random Forest Classifier Model is 97.83%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tree and fit to the training data\n",
    "forest_classifier = RandomForestClassifier()\n",
    "forest_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the model for a baseline score\n",
    "baseline_accuracy = cross_val_score(forest_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Random Forest Classifier Model is {round(baseline_accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next optimize the random forest classifier with the same hyperparameters as the Decision Tree, plus an extra that defines the number of Decision Trees the forest uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Random Forest model has a max depth of 15, a minimum of 5 points per split, and 150 trees\n",
      "Accuracy for the optimized Decision Tree Classifier Model is 98.07%\n"
     ]
    }
   ],
   "source": [
    "forest_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter search grid\n",
    "forest_grid ={'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n",
    "           'min_samples_split':[2, 3, 4, 5, 6],\n",
    "           'n_estimators':[50, 100, 150]}\n",
    "\n",
    "# Set up the grid search\n",
    "forest_grid = GridSearchCV(forest_classifier, forest_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "forest_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\n",
    "print(f'Accuracy for the optimized Decision Tree Classifier Model is {round(forest_grid.best_score_*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the optimized Random Forest did not improve results over the baseline, we consider the baseline the best version.\n",
    "\n",
    "With these strong scores on the training set, we move next into making predictions on the test set to see how well our model generalizes to unseen data. Beyond considering models for their accuracy during this stage, we can also rely on two other metrics for classification tasks. These are **Precision** and **Recall**. Precision tells us what our accuracy is among positive predictions (if we say elected how often are we right?), while recall tells us among all of our positive predicions, how many of the true positives did we find (what percent of the actual HOFers did we find?)\n",
    "\n",
    "Finally, because precision and recall always move in opposite directions as one another, we also introduce the **F1** metric. This metric balances both precision and recall, offering a more general view on how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Results by Model\n",
      "\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.979100   0.875000  0.368421  0.518519\n",
      "1                  KNN  0.971061   0.666667  0.105263  0.181818\n",
      "2        Decision Tree  0.975884   1.000000  0.210526  0.347826\n",
      "3        Random Forest  0.975884   0.750000  0.315789  0.444444\n"
     ]
    }
   ],
   "source": [
    "# Test logistic model\n",
    "logistic_preds = logistic_model.predict(x_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_preds)\n",
    "logistic_precision = precision_score(y_test, logistic_preds)\n",
    "logistic_recall = recall_score(y_test, logistic_preds)\n",
    "logistic_f1 = f1_score(y_test, logistic_preds)\n",
    "\n",
    "# Test KNN model\n",
    "knn_test_preds = knn_grid.best_estimator_.predict(x_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_test_preds)\n",
    "knn_precision = precision_score(y_test, knn_test_preds)\n",
    "knn_recall = recall_score(y_test, knn_test_preds)\n",
    "knn_f1 = f1_score(y_test, knn_test_preds)\n",
    "\n",
    "# Test Decision Tree model\n",
    "tree_test_preds = tree_grid.best_estimator_.predict(x_test)\n",
    "tree_accuracy = accuracy_score(y_test, tree_test_preds)\n",
    "tree_precision = precision_score(y_test, tree_test_preds)\n",
    "tree_recall = recall_score(y_test, tree_test_preds)\n",
    "tree_f1 = f1_score(y_test, tree_test_preds)\n",
    "\n",
    "# Test Random Forest model\n",
    "forest_test_preds = forest_grid.best_estimator_.predict(x_test)\n",
    "forest_accuracy = accuracy_score(y_test, forest_test_preds)\n",
    "forest_precision = precision_score(y_test, forest_test_preds)\n",
    "forest_recall = recall_score(y_test, forest_test_preds)\n",
    "forest_f1 = f1_score(y_test, forest_test_preds)\n",
    "\n",
    "# Convert scoring to a DataFrame\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n",
    "    \"Accuracy\": [logistic_accuracy, knn_accuracy, tree_accuracy, forest_accuracy],\n",
    "    \"Precision\": [logistic_precision, knn_precision, tree_precision, forest_precision],\n",
    "    \"Recall\": [logistic_recall, knn_recall, tree_recall, forest_recall],\n",
    "    \"F1 Score\": [logistic_f1, knn_f1, tree_f1, forest_f1]\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('Binary Classification Results by Model\\n')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an interesting trend emerges. While the accuracy of each model is quite high, the secondary metrics are not as rosy. This is occuring because of the class imbalance in our dataset. With almost 97% of the datapoints being non-elections, we could achieve a 97% accuracy by randomly guessing, or just guessing non-election for every single point. Thus, the precision and recall tell a better story as to the fact that when we guess a succesfull election, we are fairly accurate, but we often fail to predict *all* of the elections. \n",
    "\n",
    "That said, we do still see significant improvement over a random guesser, which tells us that our model is working to predict elections succesfully. With a random baseline just under 97%, we improve incorrect predictions by ~30% with our logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "We do the same exercise as above, this time using the full set of classes [elected, eliminated, expired, limbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the baseline Logistic Regression Model is 81.66% \n",
      "\n",
      "Accuracy for the baseline K-Neighbors Classifier Model is 82.5%\n",
      "The best K-Neighbors model has 5 Neighbors\n",
      "Accuracy for the optimized K-Neighbors Classifier Model is 0.83% \n",
      "\n",
      "Accuracy for the baseline Decision Tree Classifier Model is 82.1%\n",
      "The best Decision Tree model has a max depth of 15 and a minimum of 2 points per split\n",
      "Accuracy for the optimized Decision Tree Classifier Model is 0.83% \n",
      "\n",
      "Accuracy for the baseline Random Forest Classifier Model is 87.05%\n",
      "The best Random Forest model has a max depth of 20, a minimum of 3 points per split, and 50 trees\n",
      "Accuracy for the optimized Decision Tree Classifier Model is 0.88%\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing datasets for binary classification\n",
    "# We reserve 20% of the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(decomposed_data, multi_targets, test_size=0.2, random_state=5000, stratify=multi_targets)\n",
    "\n",
    "# Initialize Logistic Regression Model and fit to the training data\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the logistic model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(logistic_model, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Logistic Regression Model is {round(baseline_accuracy, 2)}% \\n')\n",
    "\n",
    "# Initialize the KNeighbor model and fit to the training data\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the KNeighbors model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(knn_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy, 2)}%')\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Define the grid values to search\n",
    "# We test each value from 5 to 50 in increments of 5\n",
    "knn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n",
    "\n",
    "# Set up the grid search\n",
    "knn_grid = GridSearchCV(knn_classifier, knn_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "knn_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\n",
    "print(f'Accuracy for the optimized K-Neighbors Classifier Model is {round(knn_grid.best_score_, 2)}% \\n')\n",
    "\n",
    "\n",
    "# Initialize the tree and fit to the training data\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the model for a baseline score\n",
    "baseline_accuracy = cross_val_score(tree_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Decision Tree Classifier Model is {round(baseline_accuracy, 2)}%')\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter search grid\n",
    "tree_grid ={'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n",
    "           'min_samples_split':[2, 3, 4, 5, 6]}\n",
    "\n",
    "# Set up the grid search\n",
    "tree_grid = GridSearchCV(tree_classifier, tree_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "tree_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\n",
    "print(f'Accuracy for the optimized Decision Tree Classifier Model is {round(tree_grid.best_score_, 2)}% \\n')\n",
    "\n",
    "\n",
    "# Initialize the tree and fit to the training data\n",
    "forest_classifier = RandomForestClassifier()\n",
    "forest_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the model for a baseline score\n",
    "baseline_accuracy = cross_val_score(forest_classifier, x_train, y_train).mean() * 100\n",
    "\n",
    "print(f'Accuracy for the baseline Random Forest Classifier Model is {round(baseline_accuracy, 2)}%')\n",
    "\n",
    "forest_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter search grid\n",
    "forest_grid ={'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n",
    "           'min_samples_split':[2, 3, 4, 5, 6],\n",
    "           'n_estimators':[50, 100, 150]}\n",
    "\n",
    "# Set up the grid search\n",
    "forest_grid = GridSearchCV(forest_classifier, forest_grid)\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "forest_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\n",
    "print(f'Accuracy for the optimized Decision Tree Classifier Model is {round(forest_grid.best_score_, 2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for Multi-Class Classification by Model\n",
      "\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.795820   0.793197  0.795820  0.794228\n",
      "1                  KNN  0.834405   0.819622  0.834405  0.815207\n",
      "2        Decision Tree  0.827974   0.819027  0.827974  0.822636\n",
      "3        Random Forest  0.860129   0.844698  0.860129  0.847190\n"
     ]
    }
   ],
   "source": [
    "# Test logistic model\n",
    "logistic_preds = logistic_model.predict(x_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_preds)\n",
    "logistic_precision = precision_score(y_test, logistic_preds, average='weighted')\n",
    "logistic_recall = recall_score(y_test, logistic_preds, average='weighted')\n",
    "logistic_f1 = f1_score(y_test, logistic_preds, average='weighted')\n",
    "\n",
    "# Test KNN model\n",
    "knn_test_preds = knn_grid.best_estimator_.predict(x_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_test_preds)\n",
    "knn_precision = precision_score(y_test, knn_test_preds, average='weighted')\n",
    "knn_recall = recall_score(y_test, knn_test_preds, average='weighted')\n",
    "knn_f1 = f1_score(y_test, knn_test_preds, average='weighted')\n",
    "\n",
    "# Test Decision Tree model\n",
    "tree_test_preds = tree_grid.best_estimator_.predict(x_test)\n",
    "tree_accuracy = accuracy_score(y_test, tree_test_preds)\n",
    "tree_precision = precision_score(y_test, tree_test_preds, average='weighted')\n",
    "tree_recall = recall_score(y_test, tree_test_preds, average='weighted')\n",
    "tree_f1 = f1_score(y_test, tree_test_preds, average='weighted')\n",
    "\n",
    "# Test Random Forest model\n",
    "forest_test_preds = forest_grid.best_estimator_.predict(x_test)\n",
    "forest_accuracy = accuracy_score(y_test, forest_test_preds)\n",
    "forest_precision = precision_score(y_test, forest_test_preds, average='weighted')\n",
    "forest_recall = recall_score(y_test, forest_test_preds, average='weighted')\n",
    "forest_f1 = f1_score(y_test, forest_test_preds, average='weighted')\n",
    "\n",
    "# Convert scoring to a DataFrame\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n",
    "    \"Accuracy\": [logistic_accuracy, knn_accuracy, tree_accuracy, forest_accuracy],\n",
    "    \"Precision\": [logistic_precision, knn_precision, tree_precision, forest_precision],\n",
    "    \"Recall\": [logistic_recall, knn_recall, tree_recall, forest_recall],\n",
    "    \"F1 Score\": [logistic_f1, knn_f1, tree_f1, forest_f1]\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('Test results for Multi-Class Classification by Model\\n')\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "For our last task, we utilize the regression methods to predict the total vote percentage received by the player. For our metric of success, we use the 'mean squared error', which is the average of the squared errors for each individual prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing datasets for regression\n",
    "# We reserve 20% of the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(decomposed_data, reg_targets, test_size=0.2, random_state=5000, stratify=binary_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for the baseline Linear Regression Model is 165.66\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression Model and fit to the training data\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the logistic model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(linear_model, x_train, y_train, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "print(f'MSE for the baseline Linear Regression Model is {round(baseline_accuracy*-1, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for the baseline K-Neighbors Classifier Model is 153.64\n",
      "\n",
      "The best K-Neighbors model has 5 Neighbors\n",
      "MSE for the optimized K-Neighbors Regressor Model is 153.64\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNeighbor model and fit to the training data\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "\n",
    "# Utilize cross validation training to train the KNeighbors model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(knn_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "print(f'MSE for the baseline K-Neighbors Classifier Model is {round(baseline_accuracy*-1, 2)}\\n')\n",
    "\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "knn_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Define the neighbor values to search\n",
    "# We test each value from 5 to 50 in increments of 5\n",
    "knn_grid ={'n_neighbors': [int(n) for n in np.linspace(5, 50, 10)]}\n",
    "\n",
    "# Set up the grid search\n",
    "knn_grid = GridSearchCV(knn_regressor, knn_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "knn_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best K-Neighbors model has {knn_grid.best_params_[\"n_neighbors\"]} Neighbors')\n",
    "print(f'MSE for the optimized K-Neighbors Regressor Model is {round(knn_grid.best_score_*-1, 2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for the baseline Decision Tree Regressor Model is 168.81\n",
      "\n",
      "The optimized Decision Tree model has a max depth of None and a minimum of 5 points per split\n",
      "MSE for the optimized Decision Tree Regressor Model is 162.21\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and fit to the training data\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Utilize cross validation training to train the model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(tree_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "print(f'MSE for the baseline Decision Tree Regressor Model is {round(baseline_accuracy*-1, 2)}\\n')\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "tree_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Define the grid values to search\n",
    "# We test each value from 5 to 50 in increments of 5\n",
    "tree_grid = {'max_depth':[int(n) for n in np.linspace(5, 50, 10)] + [None],\n",
    "            'min_samples_split':[2, 3, 4, 5, 6]}\n",
    "\n",
    "# Set up the grid search\n",
    "tree_grid = GridSearchCV(tree_regressor, tree_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "tree_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The optimized Decision Tree model has a max depth of {tree_grid.best_params_[\"max_depth\"]} and a minimum of {tree_grid.best_params_[\"min_samples_split\"]} points per split')\n",
    "print(f'MSE for the optimized Decision Tree Regressor Model is {round(tree_grid.best_score_*-1, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for the baseline Random Forest Regressor Model is 98.09\n",
      "\n",
      "The best Random Forest model has a max depth of None, a minimum of 5 points per split, and 150 trees\n",
      "MSE for the optimized Decision Tree Regressor Model is 95.97\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and fit to the training data\n",
    "forest_regressor = RandomForestRegressor()\n",
    "forest_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Utilize cross validation training to train the model\n",
    "# for a baseline score\n",
    "baseline_accuracy = cross_val_score(forest_regressor, x_train, y_train, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "print(f'MSE for the baseline Random Forest Regressor Model is {round(baseline_accuracy*-1, 2)}\\n')\n",
    "\n",
    "tree_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define the grid values to search\n",
    "forest_grid = {'max_depth':[int(n) for n in np.linspace(5, 20, 4)] + [None],\n",
    "               'min_samples_split':[4,5,6], \n",
    "               'n_estimators':[100, 150]}\n",
    "\n",
    "# Set up the grid search\n",
    "forest_grid = GridSearchCV(forest_regressor, forest_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search, finding the optimal value\n",
    "forest_grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best model and its score\n",
    "print(f'The best Random Forest model has a max depth of {forest_grid.best_params_[\"max_depth\"]}, a minimum of {forest_grid.best_params_[\"min_samples_split\"]} points per split, and {forest_grid.best_params_[\"n_estimators\"]} trees')\n",
    "print(f'MSE for the optimized Decision Tree Regressor Model is {round(forest_grid.best_score_*-1, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for Regression by Model\n",
      "\n",
      "                 Model         MSE\n",
      "0  Logistic Regression  137.182293\n",
      "1                  KNN   94.947084\n",
      "2        Decision Tree   87.421412\n",
      "3        Random Forest   65.643072\n"
     ]
    }
   ],
   "source": [
    "# Test linear model\n",
    "linear_preds = linear_model.predict(x_test)\n",
    "linear_mse = mean_squared_error(y_test, linear_preds)\n",
    "\n",
    "# Test Nearest Neighbors model\n",
    "knn_preds = knn_regressor.predict(x_test)\n",
    "knn_mse = mean_squared_error(y_test, knn_preds)\n",
    "\n",
    "# Test linear model\n",
    "tree_preds = tree_grid.best_estimator_.predict(x_test)\n",
    "tree_mse = mean_squared_error(y_test, tree_preds)\n",
    "\n",
    "# Test linear model\n",
    "forest_preds = forest_grid.best_estimator_.predict(x_test)\n",
    "forest_mse = mean_squared_error(y_test, forest_preds)\n",
    "\n",
    "\n",
    "# Convert scoring to a DataFrame\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\"],\n",
    "    \"MSE\": [linear_mse, knn_mse, tree_mse, forest_mse],\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('Test results for Regression by Model\\n')\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see pretty strong results! With our random forest model we are able to predict voting outcomes with a MSE of ~65. Although we do see this is quite a large jump in accuracy from the training data, and may be due to randomness in the data. Even with this caveat however, other models also offer and MSE in the range of 85-95. We should however remember that the mean voting percentage is ~13%, given the multitude of players who do not make the HOF even once on the ballot. That said, if we use the MSE for just an average guesser that predicts ~13% each time, the MSE would be >400, so we do certainly see an improvement over this value!\n",
    "\n",
    "This concludes the section on supervised learning, where we saw how it is possible to increase prediction accuracy of the BBWAA HOF ballot outcomes and voting percentage by utilizing an array of both classification and regression methods. For a more detailed report of the project as a whole, make sure to check out the **report section**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
